[["index.html", "Customer Analytics setup", " Customer Analytics setup "],["customer-lifetime-value.html", "1 Customer Lifetime Value 1.1 Non-contractual: The matrix formulation (Migration model) 1.2 Contractual: Valuing customer base + residual lifetime value 1.3 Non-Contractual Setting: Probabilitic Models", " 1 Customer Lifetime Value Contractual vs. non contractual: Contractual setting: We see that a customer pays a constant amount each month. Notice that one may also buy extra stuff. Non contractual setting: We see that a customer can buy at any given time, e.g., a supermarket. Discrete purchase: That is when a person makes purchase for a given time period, that is for instance subscriptions Continuous purchase: That is a person is able to make a purchase at any given time. In the following we see an example of placing orders either in the beginning or at the end of the period. The difference between case 1 and 2, is that in case 2 it is not a new customer, as we do not have a purchase in time period 0. Discrete purchases See section CLV continuous time accounting for when you get the money in the CLV templates. 1.1 Non-contractual: The matrix formulation (Migration model) This is for a non contractual setting. This comes in relation to having different states, where the last state is an absorbing state, which indicates a lost customer. It can be shown with a Markov Chain. Different states We see that there are different probabilities of starting in the different states. Then for that given state you can either move one down or move up to state one. Where state 1 = most recent purchase. Notice that this illustration has four states, state one equal the most recent purchasers while state 5 is the absorbing state. In this state there is 0 probability of moving up to state 1, hence the customer is lost. In the following example we have a scenario of the cost being 4 and a purchase giving a margin og 40 For an infinite period #Facts margin &lt;- 40 costs &lt;- 4 discount_factor &lt;- 1.2 states &lt;- 5 #Transition probabilities p1 &lt;- 0.3 #30% of starting here p2 &lt;- 0.2 #20% of starting here p3 &lt;- 0.15 #15% of starting here p4 &lt;- 0.05 p5 &lt;- 0 #Transition probability matrix P &lt;- matrix(nrow = states,byrow = TRUE ,c(p1,1-p1,0,0,0, p2,0,1-p2,0,0, p3,0,0,1-p3,0, p4,0,0,0,1-p4, p5,0,0,0,1-p5 )# NOTE, ADD STATES BELOW, AND EXTEND THE MATRIX ) # Profit vector (G for profit :) ) G &lt;- matrix(nrow = states ,c(margin - costs, #Purchase only occurs in state 1 -costs, -costs, -costs, 0)) #We spend no money on the last recency state, because we dont mail them # NOTE THE AMOUNT OF LINES MUST = STATES #Identify matrix I &lt;- diag(states) #Customer lifetime value CLV &lt;- solve(I-1/(discount_factor)*P) %*% G CLV ## [,1] ## [1,] 52.319609 ## [2,] 5.553784 ## [3,] 1.250773 ## [4,] -1.820016 ## [5,] 0.000000 We see that for customers starting in state 1, we have a profit of 52, if the customer is starting in state 2, then the profit is 5.6, if starting in state three then 1.25 and fourth they are not positive. Notice that the approach above show the infinite lifetime of the customers. We may be interested in finding out what the lifetime of a customer is, given a certain period. That we look into in the following Infinite modified CLV It is modified as the infinite solution, does not tell what they earn after a certain period, the following does, i guess that is why it is called modified We can also calculate the infinite modified solution, where we apply the transition probabilities and assess how the lifetime value of a customer starting in a specific state is. Notice, we are already having the customer. #Note, it is prerequisite to run the code above first # Infinite modified solution * # Transition probabilities p1=0.3 p2=0.2 p3=0.15 p4= #Notice, 0that we discovered above that recency state 4 where negative, so we will not invest in these. Then we also assume, that they will not buy anything. One may also have kept p = 0.05 p5=0 # Transition probability matrix Pmod=matrix(nrow=states,byrow=T, c(p1,1-p1,0,0,0, p2,0,1-p2,0,0, p3,0,0,1-p3,0, p4,0,0,0,1-p4, p5,0,0,0,1-p5 ) ) Gmod=matrix(c(margin-costs, #G for profit :) -costs, -costs, 0, #Notice, that we remove the cost, that is also why we know that p4 = 0 0) ,nrow=states) Gmod ## [,1] ## [1,] 36 ## [2,] -4 ## [3,] -4 ## [4,] 0 ## [5,] 0 # Customer lifetime value CLV = solve(I-1/(discount_factor)*Pmod) %*% Gmod CLV ## [,1] ## [1,] 53.149425 ## [2,] 6.620690 ## [3,] 2.643678 ## [4,] 0.000000 ## [5,] 0.000000 # Now we see that we do not loose money on cohort 4 Finite period Notice that the following is just for a finite period, where the calculations above are infinite. We see that the result is not really different, just as we saw in the example in excel. # 4 periods ** p2 &lt;- P %*% P #P = the first period p3 &lt;- p2 %*% P #By matrix multiplication can we find the proportions in consecutive periods p4 &lt;- p3 %*% P P ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.30 0.7 0.0 0.00 0.00 ## [2,] 0.20 0.0 0.8 0.00 0.00 ## [3,] 0.15 0.0 0.0 0.85 0.00 ## [4,] 0.05 0.0 0.0 0.00 0.95 ## [5,] 0.00 0.0 0.0 0.00 1.00 p4 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.139700 0.136500 0.1288 0.1428 0.4522 ## [2,] 0.076800 0.081200 0.1008 0.0952 0.6460 ## [3,] 0.039025 0.033075 0.0490 0.0714 0.8075 ## [4,] 0.009750 0.008050 0.0084 0.0238 0.9500 ## [5,] 0.000000 0.000000 0.0000 0.0000 1.0000 What do we see from this. We see that in period 1 recency 1, we have 30% stayers and 70% that moves to recency 2, notice that the stayers, will be those that actually buy something. Then by matrix multiplication we can see that 13.97% of those originally in state 1 is still in state 1. On the other hand can we see that 45% of the customers are lost. That is because in this scenario, there is always a greater chance of loosing the customer rather than them staying. Thus if we had more than 50% of staying, then naturally we would have more people staying in state one (repetitive buys) compared to completely lost customers (those in the absorbing state.) In the following we can replicate the lifetime value through matrix multiplication as well. res1 &lt;- G + 1/discount_factor * P %*% G + 1/discount_factor^2 * p2 %*% G + 1/discount_factor^3 * p3 %*% G + 1/discount_factor^4 * p4 %*% G res1 ## [,1] ## [1,] 50.1149691 ## [2,] 4.2199074 ## [3,] 0.5921103 ## [4,] -1.9801312 ## [5,] 0.0000000 Notice that the results are a bit closdr to 0, that is because we go for a fixed period and not an infinate period. 1.2 Contractual: Valuing customer base + residual lifetime value We see that the customer base can be estimated with the sBG approach. In this scenario we have different cohorts, for instance all those that that signed up in a given period. Hence we are working in a contratual setting. We see that we are able to measure how many customers that are actually left in the different cohorts. Customer base Then we can also show the retention rates for the given cohorts. Customer base retention rates We see that the retention rate is actually increasing. Two explanations could be: Getting more loyal customers Filtering out in the heterogeneous customer base - this also implies that we assume heterogeneity Then you may ask, how do we get the retention rates and hence the survival rates? This is estimated with the sBG (Shifted-Beta-Geometric model), with a beta distribution you are able to reflect many different shapes, hence we can also replicate the retention rates. Examples of different outcomes can be seen in the following picture. Why is it called ‘shifted’? That is because a customer cannot leave in time period 0, as they have paid for that period. Shifted-Beta-Geometric model (sBG) Application of this is made in the excel sheet and in the following. The following also simulate two different scenarios. alpha1 &lt;- 0.676 beta1 &lt;- 3.86 alpha2 &lt;- 0.7009 beta2 &lt;- 1.1712 theta &lt;- (1:99)/100 p.beta_high = dbeta(theta,alpha1,beta1) p.beta.reg = dbeta(theta,alpha2,beta2) matplot(theta,cbind(p.beta_high,p.beta.reg),xlab = &quot;Churn probability&quot; ,ylab = &quot;Density&quot;,type = &quot;l&quot;,col = c(&quot;blue&quot;,&quot;red&quot;) ,main = &quot;Composition of customers with respect to churn&quot;) legend(0.7,7,legend = c(&quot;High_end&quot;,&quot;Regular&quot;),pch = c(16,1) ,col = c(&quot;blue&quot;,&quot;red&quot;),cex = 0.7) We can also calculate the amount of people that are expected to churn in period one, for this we just need the alpha and the beta values (or at least the estimated values). That is done with \\(P = \\frac{\\alpha}{\\alpha + \\beta}\\) options(digits = 3) alpha1/(alpha1 + beta1) #0.149 alpha2/(alpha2 + beta2) #0.374 ## [1] 0.149 ## [1] 0.374 Hence we see that 37% of the regular customers will churn, while only 15% of the high end customers will churn (leave). That is as expected as we see the greatest density of the of regular churners is higher than the high end, where we see that most lie in the lower region. 1.2.1 sBG (Shifted Beta Geometric model) This is basically the same as the example we have seen in excel, just applied in R instead. The example is based on the Berry-Linoff data about some high-end customers. library(tidyverse) library(foreign) ### LOADING THE DATA ### berrydata = read.spss(&quot;Data/sBG/berry_linoff_ext.sav&quot;, to.data.frame=TRUE) #First seven years chosen berry.sub &lt;- berrydata %&gt;% filter(year&lt;8) head(berry.sub) year surv_reg surv_high survh_lag survr_lag ret_high ret_reg filter_. 1 63.1 86.9 NA NA 0.8690000 0.6310000 Selected 2 46.8 74.3 86.9 63.1 0.8550058 0.7416799 Selected 3 38.2 65.3 74.3 46.8 0.8788694 0.8162393 Selected 4 32.6 59.3 65.3 38.2 0.9081164 0.8534031 Selected 5 28.9 55.1 59.3 32.6 0.9291737 0.8865031 Selected 6 26.2 51.7 55.1 28.9 0.9382940 0.9065744 Selected We see that we have data on the year, survival on regular and high end customers and retention on both as well. The sBG Model First we must find the parameters #High end customers nls(ret_high~(beta+year-1)/(alpha+beta+year-1) ,start=list(alpha=1,beta=1) ,data=berry.sub) ## Nonlinear regression model ## model: ret_high ~ (beta + year - 1)/(alpha + beta + year - 1) ## data: berry.sub ## alpha beta ## 0.6755 3.8618 ## residual sum-of-squares: 0.001514 ## ## Number of iterations to convergence: 7 ## Achieved convergence tolerance: 2.443e-06 alpha &lt;- 0.675 beta &lt;- 3.862 # Estimated retention rates added to data file berry.subII &lt;- berrydata %&gt;% mutate(ret_est_high_sbg = (beta+year-1)/(alpha+beta+year-1)) #Plot of observed and estimated retention rates ggplot(berry.subII, aes(year, y = value, color = variable)) + geom_point(aes(y = ret_high, col = &quot;ret_high&quot;)) + geom_point(aes(y = ret_est_high_sbg, col = &quot;ret_est_high_sbg&quot;)) #Regular customers nls(ret_reg~(beta+year-1)/(alpha+beta+year-1),start=list(alpha=1,beta=1),data=berry.sub) ## Nonlinear regression model ## model: ret_reg ~ (beta + year - 1)/(alpha + beta + year - 1) ## data: berry.sub ## alpha beta ## 0.7009 1.1712 ## residual sum-of-squares: 0.0004348 ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 9.129e-07 alpha &lt;- 0.701 beta &lt;- 1.171 berry.subII &lt;- berrydata %&gt;% mutate(ret_est_reg_sbg = (beta+year-1)/(alpha+beta+year-1)) ggplot(berry.subII, aes(year, y = value, color = variable)) + geom_line(aes(y = ret_reg, col = &quot;ret_reg&quot;)) + geom_line(aes(y = ret_est_reg_sbg, col = &quot;ret_est_reg_sbg&quot;)) Here we see that the retention rates are increasing, but not reaching as high levels as the high earners. 1.3 Non-Contractual Setting: Probabilitic Models library(BTYD) #Buy till you die. Now we are going to move into non-contractual models estimate the lifetime value of such a customer base, we are going to apply probabilistic models to estimate the churn rate but also the rate of purchase (transaction rate). We are working with a non-contratual setting. Also these models are specifically applicable when the company is not able to point out the exact time, that the customer will drop out, so the opposite of when we have a contratual setting, where we know exactly if the customer does not renew his contract. I have put notes to to BG/NBD, hence one may go directly to that section to see some details. It is put as a note to describe the other approaches. Summary of the models: We see the following summary: Summary of probability models Notice that we are not going to look at the BB distribution!! Individual level = drop out of a given customer Heterogeneity = difference in customers, hence difference in when customers tend to drop out, e.g., some drops out quickly while others will stay long. Counting = about estimating no. of transactions Timing = Estimating when a customer will leave in the future What defines the model? We see that the individual level and heterogeneity are described by different distributions. These combinations make up the model, e.g., Poisson and Gamma distrbution combination to estimate respectively Individual level and Heterogeneity, is called the NBD (negative binomial distribution). And the combination of Exponential and Gamma distribution is called the Pareto distribution. Hence to estimate the Pareto distribution, you naturally need the parameters estimating both the Exponential and Gamma distribution. We are going to apply integrated models. What is integrated models?: These models apply the the different combinations seen in the first table. Hence we want a distribution both for estimating no. of transactions but also a model to describe when a customer will leave and that is why we both use NBD with BG and Pareto. We see that the integrated models yields two distributions, one for the dropout (Pareto or BG) and one for the transactions (NBD). We see that from the table above: If we are in a continuous setting and want to count no. of transactions, then we go with NBD. This is reflected by the poisson distribution and the heterogenirty of the customers can be estimated by a gamma distribution. Meaning that a given customer has a fixed transaction rate although the customers are also assumed to be heterogenous, hence 1 customer 1 transaction rate. If we have a discrete time period (hence not continous purchase = contractual), we can estimate the transaction rate with a shifted geometirc model and the heterogenity (dropout rate) can be estimated with a Beta distribution. This is applied with the sBG model. If we have a timing model, this is basically where we do not know when a customer makes a purchase and they can dropout at any given time. We assume that any customer is having lifetime, and this can be estimated with an exponential distribution. We also assume heterogeneity in the customers, i.e., different tendencies to drop out, this is estimated with a beta distribution. We use the Pareto model for this approach. 1.3.1 Pareto/NBD vs. BG/NBD The difference between these is the timing, meaning when a customer will leave. In the Pareto estimate of timing, a customer can leave at any point in time, where the BG model only allows to leave after a purchase. That makes the Pareto model more flexible than the BG model. Lets take an example of the BG/NBD: BG/NBD Example We see in section (a), that the higher the death rate the fewer purchases do we expect a customer to make, and the higher the purchase rate, the more transactions do we estimate him to have. Although, the probability of customer being alive is is actually decreasing when we increase purchase rate while holding death rate constant. That is due to how the model is build, as the BG model only allows for a customer dying after a purchase. Therefore when the purchase rate is higher, then he will also make more purchases, but for each purchase there is also a probability of dying. Therefore we also see in the last table, we see that we observe a customer in 10 periods, 1 being the first and 10 the last. We see that the more recent the customer has placed a purchase, the greater is the expected number of purchases in a 24 period. Hence implying that the longer time since the last purchase, the greater is the probability it is just because the customer is lost. In the CDNow case, he compared the two models, and we see that they are very similar. Pareto/NBD vs. BG/NBD We see that they are very similar in the way they replicate the actual data. In the following sections I will go through the Pareto/NBD and the BG/NBD model. 1.3.2 Pareto/NBD NBD = Negative Binomial Distribution Here we have a model, that aim to predict (or estimate) the transaction rate and the dropout rate. Also the model allows for heterogeneity in the customers, meaning that all are not the same and so will the dropout not be. We are working with 4 parameters, two for estimating the transaction rate and two for estimating the dropout rate. Notice that the model is similar to the e.g., the BG/NBD, that is because this is the foundation hereof. We see that the Pareto/NBD is from 1959 and the BG/NBD is from 2005, hence it builds on the Pareto/NBD. The reason for making the BG/NDB is to make it less cumbersome to estimate, although with the computing power now, we see that the Pareto/NBD model can easily be estimated. Therefore the Pareto/NBD is often preferred over the BG/NBD. Assumptions: The transaction rate is the same for a customer, but is (may be) different for each customer, hence we have heterogeneity About timing: A customer can be lost a any given time. E.g., if the average dropout rate = 0.1, then we expect that customer to survive 10 periods. Hence \\(E(\\tau)=1/\\mu\\) We assume that the transaction rate and dropout rate is independent of each other. Although this assumption can be questioned. 1.3.2.1 Data preparation We only need three pieces of information for every person, this is also called the CBS (Customer-by-sufficient-statistics): How many transactions the customer in the training period - Frequency Denoted as \\(x\\) The time of their last transaction - Recency Denoted as \\(t.x\\) The total time for which they were observed Denoted as \\(T.cal\\). For time of the calibration period. Thus we must construct a data frame with a row for each customer and the information above. We use dc.ReadLines, this is basically the same as read.csv. Notice that elog is a file with a lot of transactions. Thus we have customer identifier, date and amount. library(BTYD) #Buy till you die. cdnowElog &lt;- system.file(&quot;data/cdnowElog.csv&quot;, package = &quot;BTYD&quot;) elog &lt;- dc.ReadLines(cdnowElog, cust.idx = 2 ,date.idx = 3, sales.idx = 5) elog[1:3,] cust date sales 1 19970101 29.33 1 19970118 29.73 1 19970802 14.96 Notice the name ‘elog’, that is for event log. Hence it logs all of the events. We want to transform the date to a date type instead. elog$date &lt;- as.Date(elog$date, &quot;%Y%m%d&quot;); elog[1:3,] cust date sales 1 1997-01-01 29.33 1 1997-01-18 29.73 1 1997-08-02 14.96 The following is able to merge transactions in the same customer. Before running the code we have 6919 observations (events). elog &lt;- dc.MergeTransactionsOnSameDate(elog); After merging we have 6696 events. Thus it appears that some customers made several purchases on the same day. Now we can split the data to get a train (calibration) and test period. end.of.cal.period &lt;- as.Date(&quot;1997-09-30&quot;) #Insert the last date in the train period. elog.cal &lt;- elog[which(elog$date &lt;= end.of.cal.period), ] split.data &lt;- dc.SplitUpElogForRepeatTrans(elog.cal); clean.elog &lt;- split.data$repeat.trans.elog; Now we are going to create a customer-by-time matrix. That is simply a matrix with each customer on each row and then days as columns. The following is just a snip. #Customer-by-time: Repeatitive transactions freq.cbt &lt;- dc.CreateFreqCBT(clean.elog); freq.cbt[1:3,1:5] ## date ## cust 1997-01-08 1997-01-09 1997-01-10 1997-01-11 1997-01-12 ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## 6 0 0 0 1 0 Since we initially deleted all first transactions, the frequence customer-by-time matrix does not have any of the customer who made zero repeat transactions. These customers are still important hence they will be included in the matrix with the following. #Customer-by-time: All first transactions transactions tot.cbt &lt;- dc.CreateFreqCBT(elog) #Merging both customer-by-time matrices. cal.cbt &lt;- dc.MergeCustomers(tot.cbt, freq.cbt) birth.periods &lt;- split.data$cust.data$birth.per last.dates &lt;- split.data$cust.data$last.date cal.cbs.dates &lt;- data.frame(birth.periods, last.dates ,end.of.cal.period) cal.cbs &lt;- dc.BuildCBSFromCBTAndDates(cal.cbt ,cal.cbs.dates ,per=&quot;week&quot;) # we want to see weekly data Notes: 1. cbs = customer-by-sufficient-statistics 2. We use per = \"week\" to get data shown per week, as ultimately it show the same as pr. day and returns a more simple data frame. In general the procedure above can be executed in the following command: dc.ElogToCbsCbt(). But they have chosen to show the process, so one is able to make iterations to the data if that is needed in other applications. You’ll be glad to hear that, for the process described above, the package contains a single function to do everything for you: dc.ElogToCbsCbt. 1.3.2.2 Parameter Estimation First we must estimate the parameters. Notice that the function takes a starting point with the following values for the parameters (1,1,1,1). params &lt;- pnbd.EstimateParameters(cal.cbs); params ## [1] 0.5533971 10.5801985 0.6060625 11.6562237 We see that the parameters are r, \\(\\alpha\\), s, and \\(\\beta\\), where: r and \\(\\alpha\\) relates to the NBD model, i.e., transactions s and \\(\\beta\\) relates to the Pareto model, i.e., the dropout Based on this information we can calculate the dropout rate, being \\(\\frac{s}{\\beta}=\\) Dropout rate. Hence it being names(params) &lt;- c(&quot;r&quot;,&quot;alpha&quot;,&quot;s&quot;,&quot;beta&quot;) #Dropoutrate and dropout rate varaince params[&quot;s&quot;]/params[&quot;beta&quot;] #Dropout rate = 0.052 ## s ## 0.05199476 params[&quot;s&quot;]/params[&quot;beta&quot;]^2 #Variance of 0.00446 ## s ## 0.004460686 #Average transaction params[&quot;r&quot;]/params[&quot;alpha&quot;] #Dropout rate = 0.0523 ## r ## 0.05230499 params[&quot;r&quot;]/params[&quot;alpha&quot;]^2 #Variance of 0.00494 ## r ## 0.004943668 #Average lifetime params[&quot;beta&quot;]/params[&quot;s&quot;] #19.2 periods ## beta ## 19.23271 LL &lt;- pnbd.cbs.LL(params, cal.cbs); LL ## [1] -9594.976 The log likelihood estimates the fit to the data, thus we have a log-likelihood of -9494.98. We are not able to deduct much from the number itself. But we are able to compare it with other models. Basically, we want to select the model with a log-likelihood that is as close to 0 as possible. In the following we run consecutive estimations with its own output as its starting point. p.matrix &lt;- c(params, LL); params &lt;- unname(params) #Previously I named them, if so, the functions does not work for (i in 1:2){ params &lt;- pnbd.EstimateParameters(cal.cbs, params) LL &lt;- pnbd.cbs.LL(params, cal.cbs); p.matrix.row &lt;- c(params, LL); p.matrix &lt;- rbind(p.matrix, p.matrix.row); } colnames(p.matrix) &lt;- c(&quot;r&quot;, &quot;alpha&quot;, &quot;s&quot;, &quot;beta&quot;, &quot;LL&quot;); rownames(p.matrix) &lt;- 1:3; p.matrix ## r alpha s beta LL ## 1 0.5533971 10.58020 0.6060625 11.65622 -9594.976 ## 2 0.5534354 10.57952 0.6060276 11.65666 -9594.976 ## 3 0.5533710 10.57952 0.6059074 11.65812 -9594.976 We see that the log-likelihood, hence the fit, is the same, but some of the parameters are changing slightly. We see that the solution is stable. We have the following parameters: Describing the the gamma mixing distribution of the NBD transaction process r Alpha See figure 1.1 Describing the gamme mixing distrbibution of the Pareto (or gamma exponentiated) dropout process s beta See figure 1.2 pnbd.PlotTransactionRateHeterogeneity(params) Figure 1.1: Transaction rate heterogeneity of estimated parameters Few have a very high transaction rate, where the majority lies in the south eastern region. pnbd.PlotDropoutRateHeterogeneity(params) Figure 1.2: Dropout rate heterogeneity of estimated parameters Few have a very high dropout rate, where the majority lies in the south eastern region. 1.3.2.3 Individual Level Estimations Now we can also make some estimations on the individual level. We can do: Estimating purchases in the calibration period and hold out period. Calculating probability that the customer is still alive at the end of the calibration period. First we can estimate the number of transactions we expect a newly acquired customer to make in a given time period. pnbd.Expectation(params ,t=52 #We have weekly data ) ## [1] 1.473434 We expect a newly acquired customer to make 1.47 repeat purchases in a time period of one year (one period). Calling information from an individual customer. cal.cbs[&quot;1516&quot;,] ## x t.x T.cal ## 26.00000 30.85714 31.00000 Now we can store the information, to be used for estimating purchases in the holdout period. x &lt;- cal.cbs[&quot;1516&quot;, &quot;x&quot;] t.x &lt;- cal.cbs[&quot;1516&quot;, &quot;t.x&quot;] T.cal &lt;- cal.cbs[&quot;1516&quot;, &quot;T.cal&quot;] pnbd.ConditionalExpectedTransactions(params ,T.star = 52 #The holdout period ,x #Frequency ,t.x #Recency ,T.cal #Length of train period ) ## [1] 25.45647 We expect 25.5 purchases in the hold out period. We see that the hold out is 52 weeks, as we have data on 78 weeks. Second we can calculate the probability that the customer is still alive in the end of the hold out period. pnbd.PAlive(params, x, t.x, T.cal) ## [1] 0.997874 We see that there is a probability of 99%, hence it is almost certain. Conditional expectation function We see the increasing frequency paradox in action: for (i in seq(10, 25, 5)){ cond.expectation &lt;- pnbd.ConditionalExpectedTransactions( params, T.star = 52, x = i, #We iterate through Frequency, holding other params fixed t.x = 20, T.cal = 39) cat (&quot;x:&quot;,i,&quot;\\t Expectation:&quot;,cond.expectation, fill = TRUE) } ## x: 10 Expectation: 0.7062289 ## x: 15 Expectation: 0.1442396 ## x: 20 Expectation: 0.02250658 ## x: 25 Expectation: 0.00309267 We see that the higher the frequency holding recency fixed at 20, the greater is the possibility of a customer being dead. The table shows expected transactions in the hold out period. That is because if we have a customer with 25 transactions in the train period, but the last transaction is in period 20 (week 20), we would have expected him to place a purchase in the most recent weeks, because that is how his buying patterns has looked previously. Although he has not, hence he is probably lost. On the other hand a customer with 10 transactions in a 39 week calibration period, it is not very unlikely that he just hasn’t placed an order between week 20 and 39, hence there is a greater chance of an active/sleeping customer, than a lost customer. We see that the same happens in the BG model variate, as there they are only able to leave after a purchase and not at any given time as in the Pareto/NBD. 1.3.2.4 Plotting / Goodness-of-fit In the training period We want to assess how the model fits the actual transactions pnbd.PlotFrequencyInCalibration(params, cal.cbs ,censor = 7 #The frequency groups ) ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 freq.6 ## n.x.actual 1411.000 439.0000 214.0000 100.0000 62.0000 38.00000 29.00000 ## n.x.expected 1434.034 397.2457 194.0224 112.4039 70.5903 46.43514 31.47627 ## freq.7+ ## n.x.actual 64.00000 ## n.x.expected 70.79194 Notice that all bins reflect the frequency and then the y show the amount of customers in the repeat transactions group, notice that group 0 only made one purchase We see that the model estimates the actual transactions very well. The hold out period elog &lt;- dc.SplitUpElogForRepeatTrans(elog)$repeat.trans.elog x.star &lt;- rep(0, nrow(cal.cbs)) cal.cbs &lt;- cbind(cal.cbs, x.star) elog.custs &lt;- elog$cust for (i in 1:nrow(cal.cbs)){ current.cust &lt;- rownames(cal.cbs)[i] tot.cust.trans &lt;- length(which(elog.custs == current.cust)) cal.trans &lt;- cal.cbs[i, &quot;x&quot;] cal.cbs[i, &quot;x.star&quot;] &lt;- tot.cust.trans - cal.trans } cal.cbs[1:3,] ## x t.x T.cal x.star ## 1 2 30.428571 38.85714 1 ## 2 1 1.714286 38.85714 0 ## 3 0 0.000000 38.85714 0 Now we can plot the expected frequency. T.star &lt;- 39 # length of the holdout period censor &lt;- 7 #No. of bins for transactions x.star &lt;- cal.cbs[,&quot;x.star&quot;] comp &lt;- pnbd.PlotFreqVsConditionalExpectedFrequency(params, T.star ,cal.cbs, x.star ,censor #Bins of transactions ) We see that the hold out period is estimated very well in this example. Instead of looking at the plot, we can also show a matrix containing the numbers. rownames(comp) &lt;- c(&quot;act&quot;, &quot;exp&quot;, &quot;bin&quot;) comp ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 ## act 0.2367116 0.6970387 1.392523 1.560000 2.532258 2.947368 ## exp 0.1384724 0.5995607 1.195989 1.714041 2.398545 2.907467 ## bin 1411.0000000 439.0000000 214.000000 100.000000 62.000000 38.000000 ## freq.6 freq.7+ ## act 3.862069 6.359375 ## exp 3.818906 6.403484 ## bin 29.000000 64.000000 Now we want to look at weekly transactions. tot.cbt &lt;- dc.CreateFreqCBT(elog) d.track.data &lt;- rep(0, 7 * 78) origin &lt;- as.Date(&quot;1997-01-01&quot;) for (i in colnames(tot.cbt)){ date.index &lt;- difftime(as.Date(i), origin) + 1; d.track.data[date.index] &lt;- sum(tot.cbt[,i]); } w.track.data &lt;- rep(0, 78) for (j in 1:78){ w.track.data[j] &lt;- sum(d.track.data[(j*7-6):(j*7)]) } T.cal &lt;- cal.cbs[,&quot;T.cal&quot;] T.tot &lt;- 78 n.periods.final &lt;- 78 inc.tracking &lt;- pnbd.PlotTrackingInc(params, T.cal, T.tot, w.track.data, n.periods.final) We see that the model generalizes pretty well the period. Instead of looking at the plot we can also show the underlying numbers. inc.tracking[,20:25] ## [,1] [,2] [,3] [,4] [,5] [,6] ## actual 73.00000 55.0000 70.00000 33.00000 56.00000 99.00000 ## expected 78.30848 76.4191 74.64776 72.98278 71.41403 69.93268 We can also show a cummulated sum over the weeks, notice that the hold period and the training period is separated by the dashed line. cum.tracking.data &lt;- cumsum(w.track.data) #Cumulative transaction cum.tracking &lt;- pnbd.PlotTrackingCum(params ,T.cal #Lengths of cal period ,T.tot #78, total length ,cum.tracking.data #Cumulative transactions pr. week whole period ,n.periods.final#78 calibration legnth + hold out length ) We can also manually assess the predicted and actual no. of transactions. cum.tracking[,20:25] ## [,1] [,2] [,3] [,4] [,5] [,6] ## actual 1359.000 1414.000 1484.000 1517.000 1573.00 1672.000 ## expected 1308.856 1385.275 1459.923 1532.906 1604.32 1674.253 1.3.3 BG/NBD Used in a non-contractual situation in which customers can make a purchase at any time. The model describes the rate at which customers make purchases and the rate at which they drop out. This is done with four parameters and also allowing for heterogeneity. This is a simplified version of the Pareto/NBD, where a customer can only dropout immediately after placing a purchase and not at any given time as in the Pareto/NBD. The following is the BG/NBD model on the CDNow data. Note: calibration = training 1.3.3.1 Data Preparation We only need three pieces of information for every person: How many transactions the customer in the training period - Frequency Denoted as \\(x\\) The time of their last transaction - Recency Denoted as \\(t.x\\) The total time for which they were observed Denoted as \\(T.cal\\). For time of the calibration period. Thus we must construct a data frame with a row for each customer and the information above. We use dc.ReadLines, this is basically the same as read.csv. Notice that elog is a file with a lot of transactions. Thus we have customer identifier, date and amount. cdnowElog &lt;- system.file(&quot;data/cdnowElog.csv&quot;, package = &quot;BTYD&quot;) elog &lt;- dc.ReadLines(cdnowElog, cust.idx = 2 ,date.idx = 3, sales.idx = 5) elog[1:3,] cust date sales 1 19970101 29.33 1 19970118 29.73 1 19970802 14.96 Notice the name ‘elog’, that is for event log. Hence it logs all of the events. We want to transform the date to a date type instead. elog$date &lt;- as.Date(elog$date, &quot;%Y%m%d&quot;); elog[1:3,] cust date sales 1 1997-01-01 29.33 1 1997-01-18 29.73 1 1997-08-02 14.96 The following is able to merge transactions in the same customer. Before running the code we have 6919 observations (events). elog &lt;- dc.MergeTransactionsOnSameDate(elog); After merging we have 6696 events. Thus it appears that some customers made several purchases on the same day. Now we can split the data to get a train (calibration) and test period. end.of.cal.period &lt;- as.Date(&quot;1997-09-30&quot;) #Insert the last date in the train period. elog.cal &lt;- elog[which(elog$date &lt;= end.of.cal.period), ] elog.cal[1:3,] cust date sales 1 1 1997-01-01 29.33 416 1 1997-01-18 29.73 4364 1 1997-08-02 14.96 We see that customer 1 has three orders with different amounts in the snippet. Notice, the BG/NBD model deals with repeat transactions, hence the first transaction is ignored. split.data &lt;- dc.SplitUpElogForRepeatTrans(elog.cal); clean.elog &lt;- split.data$repeat.trans.elog #50% of the total data Now we are going to create a customer-by-time matrix. That is simply a matrix with each customer on each row and then days as columns. The following is just a snip. #Customer-by-time: Repeatitive transactions freq.cbt &lt;- dc.CreateFreqCBT(clean.elog) freq.cbt[1:3,1:5] ## date ## cust 1997-01-08 1997-01-09 1997-01-10 1997-01-11 1997-01-12 ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## 6 0 0 0 1 0 Since we initially deleted all first transactions, the frequence customer-by-time matrix does not have any of the customer who made zero repeat transactions. These customers are still important hence they will be included in the matrix with the following. #Customer-by-time: All first transactions transactions tot.cbt &lt;- dc.CreateFreqCBT(elog) #Merging both customer-by-time matrices. cal.cbt &lt;- dc.MergeCustomers(tot.cbt, freq.cbt) Now we have constructed the data frame for analysis. birth.periods &lt;- split.data$cust.data$birth.per last.dates &lt;- split.data$cust.data$last.date cal.cbs.dates &lt;- data.frame(birth.periods, last.dates ,end.of.cal.period) cal.cbs &lt;- dc.BuildCBSFromCBTAndDates(cal.cbt ,cal.cbs.dates ,per=&quot;week&quot;) # we want to see weekly data Notes: 1. cbs = customer-by-sufficient-statistics 2. We use per = \"week\" to get data shown per week, as ultimately it show the same as pr. day and returns a more simple data frame. In general the procedure above can be executed in the following command: dc.ElogToCbsCbt(). But they have chosen to show the process, so one is able to make iterations to the data if that is needed in other applications. 1.3.3.2 Parameter Estimation We use the following formula to estiamte the parameters. They take starting point in par.start = c(1, 3, 1, 3), and iterate from there. params &lt;- bgnbd.EstimateParameters(cal.cbs); params #this is respecitvely r, alpha, a and beta ## p1 p2 p3 p4 ## 0.2425982 4.4136842 0.7929899 2.4261667 See Pareto/NBD for manual calculation of average transaction and dropout rate. #Log-Likelyhood of parameters LL &lt;- bgnbd.cbs.LL(params, cal.cbs); LL ## [1] -9582.429 p.matrix &lt;- c(params, LL); for (i in 1:2){ params &lt;- bgnbd.EstimateParameters(cal.cbs, params); LL &lt;- bgnbd.cbs.LL(params, cal.cbs); p.matrix.row &lt;- c(params, LL); p.matrix &lt;- rbind(p.matrix, p.matrix.row); } colnames(p.matrix) &lt;- c(&quot;r&quot;, &quot;alpha&quot;, &quot;a&quot;, &quot;b&quot;, &quot;LL&quot;); rownames(p.matrix) &lt;- 1:3; p.matrix ## r alpha a b LL ## 1 0.2425982 4.413684 0.7929899 2.426167 -9582.429 ## 2 0.2425965 4.413685 0.7929888 2.426166 -9582.429 ## 3 0.2425967 4.413659 0.7929869 2.426164 -9582.429 We see that all rows are basically the same, the model must then be stable. We have the following parameters: r = referring to the transaction - describes the gamma mixing distribution of the NBD transaction process. See figure 1.3 alpha = refers to the transaction - describes the gamma mixing distribution of the NBD transaction process. See figure 1.3 a = alpha that manipulates the beta distribution - explains dropout rate. See figure 1.4 b = beta, that manipulates the beta distribution - explains dropout rate. See figure 1.4 They mention that it is good to try different starting points, i guess that could merely be incorporated into the loop. bgnbd.PlotTransactionRateHeterogeneity(params) Figure 1.3: Transaction rate heterogeneity of estimated parameters bgnbd.PlotDropoutRateHeterogeneity(params) Figure 1.4: Dropout probability heterogeneity of estimated parameters 1.3.3.3 Individual Level Estimation This is about predicting on individual customer level. First we can estimate the number of transactions we expect a newly acquired customer to make in a give time period. bgnbd.Expectation(params ,t=52) # = 1 year, has we have 52 weeks ## p3 ## 1.444004 We see that he is expected to make 1.44 purchases. The following show customer 1516, which is the same as we used in the previous example. cal.cbs[&quot;1516&quot;,] ## x t.x T.cal ## 26.00000 30.85714 31.00000 We see that he made 26 repeat transactions in the calibration period. Period 30.8 was the most recent period of a purchase, indicating that he bought towards the end of period 30. Now we can save the attributes in objects. x &lt;- cal.cbs[&quot;1516&quot;, &quot;x&quot;] t.x &lt;- cal.cbs[&quot;1516&quot;, &quot;t.x&quot;] T.cal &lt;- cal.cbs[&quot;1516&quot;, &quot;T.cal&quot;] Finally we can calculate the expected number of purchases in the holdout period. bgnbd.ConditionalExpectedTransactions(params ,T.star = 52 #Prediction period, hence 52 weeks. ,x = x #No. of repeat trans. in cal. period. ,t.x = t.x #Most recent period with purchase ,T.cal = T.cal) #Length of cal. period. ## p3 ## 25.75659 We see that the customer is expected to make 25.76 purchases in the hold-out-period. Notice that we had 25.5 in the Pareto/NBD, hence they are very similar. We can calculate the probability of a person buying alive, notice that for this we don’t use a specific customer, although we use the transactions and recency for the customer, if we are having similar customer we would expect them to be equal. bgnbd.PAlive(params = params ,x = x #No. of repeat transactions in the cal period ,t.x = t.x #Time of most recent repeat transaction ,T.cal = T.cal) #Length of calibration period ## p3 ## 0.9688523 We see that there is a probability of 96.8% that he will be alive after the end of the calibration period. That also seems intuitively high, as customer with the most recent transaction was in 38.4, hence this customer must also be close to the end of the calibration period. for (i in seq(10, 25, 5)){ cond.expectation &lt;- bgnbd.ConditionalExpectedTransactions( params, T.star = 52, x = i, t.x = 20, T.cal = 39) cat (&quot;x:&quot;,i,&quot;\\t Expectation:&quot;,cond.expectation, fill = TRUE) } ## x: 10 Expectation: 0.3474606 ## x: 15 Expectation: 0.04283391 ## x: 20 Expectation: 0.004158973 ## x: 25 Expectation: 0.0003583685 As with P/NBD we simulate different frequencies given the same recency and end of calibration period. increasing frequency paradox: We see that this example is even more extreme, as we in this model only allow a customer to leave immediately after a purchase and not at any given time, So if we say that a customer has 25 transactions in the hold out period, but he has not bought anything after week 20, then we are almost certain that it is not a sleeping customer, but he is dead, as we do not expect to place any transactions. 1.3.3.4 Plotting / Goodness-of-fit First of all we can plot the actual vs. predicted in the calibration period. bgnbd.PlotFrequencyInCalibration(params = params ,cal.cbs = cal.cbs ,censor = 7 #All 7+ are binned. ) ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 freq.6 ## n.x.actual 1411.000 439.0000 214.0000 100.0000 62.00000 38.00000 29.00000 ## n.x.expected 1407.682 460.3225 192.4637 101.1657 59.84786 38.12006 25.54923 ## freq.7+ ## n.x.actual 64.00000 ## n.x.expected 71.22628 We see that the model replicates the variance that we observe in the data very well. What we see is amount of customers on y and amount of transactions in training period. Now we construct the hold/out period. elog &lt;- dc.SplitUpElogForRepeatTrans(elog)$repeat.trans.elog x.star &lt;- rep(0, nrow(cal.cbs)) cal.cbs &lt;- cbind(cal.cbs, x.star) elog.custs &lt;- elog$cust; for (i in 1:nrow(cal.cbs)){ current.cust &lt;- rownames(cal.cbs)[i] tot.cust.trans &lt;- length(which(elog.custs == current.cust)) cal.trans &lt;- cal.cbs[i, &quot;x&quot;] cal.cbs[i, &quot;x.star&quot;] &lt;- tot.cust.trans - cal.trans } cal.cbs[1:3,] ## x t.x T.cal x.star ## 1 2 30.428571 38.85714 1 ## 2 1 1.714286 38.85714 0 ## 3 0 0.000000 38.85714 0 We fill in the characteristics to the model. And we can plot T.star &lt;- 39 # length of the holdout period censor &lt;- 7 # This censor serves the same purpose described above x.star &lt;- cal.cbs[,&quot;x.star&quot;] comp &lt;- bgnbd.PlotFreqVsConditionalExpectedFrequency(params ,T.star ,cal.cbs ,x.star ,censor) Figure 1.5: Actual vs. conditional expected transactions in the holdout period. We see that it is good at replicating the model, perhaps a bit worse than the Pareto/NBD. rownames(comp) &lt;- c(&quot;act&quot;, &quot;exp&quot;, &quot;bin&quot;) comp ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 ## act 0.2367116 0.6970387 1.392523 1.560000 2.532258 2.947368 ## exp 0.2250893 0.5231364 1.044126 1.520256 2.163824 2.653789 ## bin 1411.0000000 439.0000000 214.000000 100.000000 62.000000 38.000000 ## freq.6 freq.7+ ## act 3.862069 6.359375 ## exp 3.503957 6.157036 ## bin 29.000000 64.000000 Now we can assess how well the our model predict how many transactions will occur in each week. First, we need to convert the total transactions pr. day into a customer-by-time matrix (cbt). And then we can transform them into weekly observations instead of daily. tot.cbt &lt;- dc.CreateFreqCBT(elog) We can also plot per week instead of binning transactions. d.track.data &lt;- rep(0,7 * 78) #Corresponding to the amount of daily transactions in the holdout origin &lt;- as.Date(&quot;1997-01-01&quot;) for (i in colnames(tot.cbt)){ date.index &lt;- difftime(as.Date(i), origin) + 1; d.track.data[date.index] &lt;- sum(tot.cbt[,i]); } w.track.data &lt;- rep(0, 78) for (j in 1:78){ w.track.data[j] &lt;- sum(d.track.data[(j*7-6):(j*7)]) } Now we can plot and compare the number of transactions with the predicted (expected). T.cal &lt;- cal.cbs[,&quot;T.cal&quot;] T.tot &lt;- 78 #78 because we have 78 weeks in total, if daily then 7*78=546 n.periods.final &lt;- 78 #Notice, often this will just be the same, but does not have to be inc.tracking &lt;- bgnbd.PlotTrackingInc(params, T.cal ,T.tot, w.track.data ,n.periods.final) Figure 1.6: Actual vs. expected incremental purchasing behaviour. inc.tracking[,20:25] ## [,1] [,2] [,3] [,4] [,5] [,6] ## actual 73.00000 55.00000 70.00000 33.00000 56.00000 99.00000 ## expected 76.86531 74.88843 73.04554 71.32166 69.70412 68.18213 We can now plot the following with the actual vs. the expected (predicted). cum.tracking.data &lt;- cumsum(w.track.data) cum.tracking &lt;- bgnbd.PlotTrackingCum(params, T.cal ,T.tot, cum.tracking.data ,n.periods.final) Figure 1.7: Actual vs. expected cumulative purchasing behaviour. cum.tracking[,20:25] ## [,1] [,2] [,3] [,4] [,5] [,6] ## actual 1359.000 1414.000 1484.000 1517.000 1573.000 1672.0 ## expected 1312.458 1387.346 1460.392 1531.713 1601.418 1669.6 Notice that the dashed line show the shift from calibration to hold out sample. We see that our model fits the data very well! And also captures the trend. 1.3.4 BG/BB - Not curicullum This is a part of the curriculum, but it was given, I am just leaving it here #### Data Preparation simElog &lt;- system.file(&quot;data/discreteSimElog.csv&quot; ,package = &quot;BTYD&quot;) elog &lt;- dc.ReadLines(simElog, cust.idx = 1, date.idx = 2) elog[1:3,] elog$date &lt;- as.Date(elog$date, &quot;%Y-%m-%d&quot;) max(elog$date); min(elog$date); # let&#39;s make the calibration period end somewhere in-between T.cal &lt;- as.Date(&quot;1977-01-01&quot;) simData &lt;- dc.ElogToCbsCbt(elog, per=&quot;year&quot;, T.cal) cal.cbs &lt;- simData$cal$cbs freq&lt;- cal.cbs[,&quot;x&quot;] rec &lt;- cal.cbs[,&quot;t.x&quot;] trans.opp &lt;- 7 # transaction opportunities cal.rf.matrix &lt;- dc.MakeRFmatrixCal(freq, rec, trans.opp) cal.rf.matrix[1:5,] #### Parameter Estimation data(donationsSummary); rf.matrix &lt;- donationsSummary$rf.matrix params &lt;- bgbb.EstimateParameters(rf.matrix); LL &lt;- bgbb.rf.matrix.LL(params, rf.matrix); p.matrix &lt;- c(params, LL); for (i in 1:2){ params &lt;- bgbb.EstimateParameters(rf.matrix, params); LL &lt;- bgbb.rf.matrix.LL(params, rf.matrix); p.matrix.row &lt;- c(params, LL); p.matrix &lt;- rbind(p.matrix, p.matrix.row); } colnames(p.matrix) &lt;- c(&quot;alpha&quot;, &quot;beta&quot;, &quot;gamma&quot;, &quot;delta&quot;, &quot;LL&quot;); rownames(p.matrix) &lt;- 1:3; p.matrix; #### Individual Level Estimations bgbb.Expectation(params, n=10); # customer A n.cal = 6 n.star = 10 x = 0 t.x = 0 bgbb.ConditionalExpectedTransactions(params, n.cal ,n.star, x, t.x) # customer B x = 4 t.x = 5 bgbb.ConditionalExpectedTransactions(params, n.cal ,n.star, x, t.x) #### Plotting / Goodness-of-fit bgbb.PlotFrequencyInCalibration(params, rf.matrix) holdout.cbs &lt;- simData$holdout$cbs x.star &lt;- holdout.cbs[,&quot;x.star&quot;] n.star &lt;- 5 # length of the holdout period x.star &lt;- donationsSummary$x.star comp &lt;- bgbb.PlotFreqVsConditionalExpectedFrequency(params, n.star, rf.matrix, x.star) rownames(comp) &lt;- c(&quot;act&quot;, &quot;exp&quot;, &quot;bin&quot;) comp comp &lt;- bgbb.PlotRecVsConditionalExpectedFrequency(params, n.star, rf.matrix, x.star) rownames(comp) &lt;- c(&quot;act&quot;, &quot;exp&quot;, &quot;bin&quot;) comp inc.track.data &lt;- donationsSummary$annual.trans n.cal &lt;- 6 xtickmarks &lt;- 1996:2006 inc.tracking &lt;- bgbb.PlotTrackingInc(params, rf.matrix ,inc.track.data ,xticklab = xtickmarks) rownames(inc.tracking) &lt;- c(&quot;act&quot;, &quot;exp&quot;) inc.tracking cum.track.data &lt;- cumsum(inc.track.data) cum.tracking &lt;- bgbb.PlotTrackingCum(params, rf.matrix, cum.track.data ,xticklab = xtickmarks) rownames(cum.tracking) &lt;- c(&quot;act&quot;, &quot;exp&quot;) cum.tracking 1.3.5 Examples from the lecture 1.3.5.1 Pareto/NBD PREDICTING LONG TERM CUSTOMER VALUE WITH BTYD PACKAGE Pareto/NBD (negative binomial distribution) modeling of repeat-buying behavior in a noncontractual setting Based on Schmittlein, Morrison, and Colombo (1987), “Counting Your Customers: Who Are They and What Will They Do Next?” Management Science, 33, 1-24. Required data for model is: “customer-by-sufficient-statistic” (cbs) matrix with the ‘sufficient’ stats being: This is the most important table, this is the information that is needed to run the model. frequency of transaction (no of repeat transactions) recency (time of last transaction) and total time observed Main model params are: beta unobserved shape parameter for Pareto dropout process s unobserved scale parameter for Pareto dropout process r unobserved shape parameter for NBD transaction alpha unobserved scale parameter for NBD transaction Data are divided into earlier calibration and later holdout segments, using a single date as the cut point. “cal” data are used to predict “holdout” data library(BTYD) # Read in data cdnowElog &lt;- system.file(&quot;data/cdnowElog.csv&quot;, package = &quot;BTYD&quot;) # The raw data file raw &lt;- read.csv(cdnowElog) head(raw) masterid sampleid date cds sales 4 1 19970101 2 29.33 4 1 19970118 2 29.73 4 1 19970802 1 14.96 4 1 19971212 2 26.48 21 2 19970101 3 63.34 21 2 19970113 1 11.77 We see the id of the customer and the date of transactions, and also amount of cd’s purchases and with the amount of sale.s # Create event log from file &quot;cdnowElog.csv&quot;, which has # customer IDs in the second column, dates in the third column, and # sales numbers in the fifth column. elog &lt;- dc.ReadLines(system.file(&quot;data/cdnowElog.csv&quot;, package=&quot;BTYD&quot;),2,3,5) elog[,&quot;date&quot;] &lt;- as.Date(elog[,&quot;date&quot;], &quot;%Y%m%d&quot;) head(elog) #take a look on the raw data cust date sales 1 1997-01-01 29.33 1 1997-01-18 29.73 1 1997-08-02 14.96 1 1997-12-12 26.48 2 1997-01-01 63.34 2 1997-01-13 11.77 This is a new dataset where we have changed the names and formated the date. Notice, that this should be the format of all tables if you want to run the model data &lt;- dc.ElogToCbsCbt(elog, per=&quot;week&quot;, T.cal=as.Date(&quot;1997-09-30&quot;)) attributes(data) ## $names ## [1] &quot;cal&quot; &quot;holdout&quot; &quot;cust.data&quot; We see that we have different attributes on the data. Notice that it is stored as a list. attributes(data$cal) ## $names ## [1] &quot;cbs&quot; &quot;cbt&quot; We see that the calibration data has both tables cbs and cbt. head(data$cal$cbs) #take a look on the calibration data ## x t.x T.cal ## 1 2 30.428571 38.85714 ## 2 1 1.714286 38.85714 ## 3 0 0.000000 38.85714 ## 4 0 0.000000 38.85714 ## 5 0 0.000000 38.85714 ## 6 7 29.428571 38.85714 We get the information: x = number of transactions. We see that there are only two transactions now, before we had 4, that is because one is in the hold out sample and we removed the first transaction. 1.3.5.1.1 Estimate parameters for Pareto/NBD model based upon calibration data # initial estimate (params2 &lt;- pnbd.EstimateParameters(data$cal$cbs)) ## [1] 0.5533971 10.5801985 0.6060625 11.6562237 We see the four params in the following order, r, alpha, s, beta # look at log likelihood (LL &lt;- pnbd.cbs.LL(params2, data$cal$cbs)) ## [1] -9594.976 # make a series of estimates, see if they converge to the same solution p.matrix &lt;- c(params2, LL) for (i in 1:20) { params2 &lt;- pnbd.EstimateParameters(data$cal$cbs, params2) LL &lt;- pnbd.cbs.LL(params2, data$cal$cbs) p.matrix.row &lt;- c(params2, LL) p.matrix &lt;- rbind(p.matrix, p.matrix.row) } #colnames(p.matrix) &lt;- c(&quot;beta&quot;,&quot;s&quot;,&quot;r&quot;,&quot;Alpha&quot;,&quot;LL&quot;) # examine p.matrix ## [,1] [,2] [,3] [,4] [,5] ## p.matrix 0.5533971 10.58020 0.6060625 11.65622 -9594.976 ## p.matrix.row 0.5534354 10.57952 0.6060276 11.65666 -9594.976 ## p.matrix.row 0.5533710 10.57952 0.6059074 11.65812 -9594.976 ## p.matrix.row 0.5533553 10.57867 0.6059434 11.65781 -9594.976 ## p.matrix.row 0.5533415 10.57861 0.6059408 11.65791 -9594.976 ## p.matrix.row 0.5533439 10.57851 0.6059382 11.65797 -9594.976 ## p.matrix.row 0.5533318 10.57843 0.6059379 11.65807 -9594.976 ## p.matrix.row 0.5533340 10.57836 0.6059363 11.65811 -9594.976 ## p.matrix.row 0.5533217 10.57825 0.6059374 11.65826 -9594.976 ## p.matrix.row 0.5533239 10.57819 0.6059363 11.65830 -9594.976 ## p.matrix.row 0.5533034 10.57791 0.6059435 11.65883 -9594.976 ## p.matrix.row 0.5533058 10.57788 0.6059432 11.65886 -9594.976 ## p.matrix.row 0.5533276 10.57829 0.6059982 11.66026 -9594.976 ## p.matrix.row 0.5533231 10.57829 0.6059994 11.66027 -9594.976 ## p.matrix.row 0.5533233 10.57822 0.6059985 11.66032 -9594.976 ## p.matrix.row 0.5533194 10.57822 0.6059995 11.66034 -9594.976 ## p.matrix.row 0.5533199 10.57816 0.6059987 11.66037 -9594.976 ## p.matrix.row 0.5533164 10.57816 0.6059997 11.66040 -9594.976 ## p.matrix.row 0.5533170 10.57812 0.6059991 11.66043 -9594.976 ## p.matrix.row 0.5533139 10.57811 0.6060002 11.66046 -9594.976 ## p.matrix.row 0.5533146 10.57808 0.6059997 11.66049 -9594.976 We see that the parameters are very stable, hence they dont change much in the iterations. We see the average dropout rate: s/beta #The average dropout rate 0.6060/11.65 # = 0.05201717. i.e. ## [1] 0.05201717 r = p.matrix[1,3] Alpha = p.matrix[1,4] r/Alpha ## p.matrix ## 0.05199476 We see the average transaction rate: #THe average transaction rate 0.5533/10.58 #=0.05229679 i.e. ## [1] 0.05229679 beta = p.matrix[1,1] s = p.matrix[1,2] beta/s ## p.matrix ## 0.05230499 Notice, it is a miracle that it so similar to the dropout rate. # use final set of values (params2 &lt;- p.matrix[dim(p.matrix)[1],1:4]) ## [1] 0.5533146 10.5780771 0.6059997 11.6604916 #The drop-out rate - Pareto part pnbd.PlotDropoutRateHeterogeneity(params2) ## [,1] [,2] [,3] [,4] [,5] ## x.axis.ticks 0 0.003138666 0.006277332 0.009415997 0.01255466 ## heterogeneity Inf 28.044909886 20.575655163 16.907492071 14.55321079 ## [,6] [,7] [,8] [,9] [,10] ## x.axis.ticks 0.01569333 0.01883199 0.02197066 0.02510933 0.02824799 ## heterogeneity 12.84936813 11.52895283 10.45967842 9.56698323 8.80494055 ## [,11] [,12] [,13] [,14] [,15] [,16] ## x.axis.ticks 0.03138666 0.03452532 0.03766399 0.04080265 0.04394132 0.04707999 ## heterogeneity 8.14335724 7.56136637 7.04396357 6.58000684 6.16099837 5.78030958 ## [,17] [,18] [,19] [,20] [,21] [,22] ## x.axis.ticks 0.05021865 0.05335732 0.05649598 0.05963465 0.06277332 0.06591198 ## heterogeneity 5.43266965 5.11381734 4.82025747 4.54908656 4.29786548 4.06452456 ## [,23] [,24] [,25] [,26] [,27] [,28] ## x.axis.ticks 0.06905065 0.07218931 0.07532798 0.07846664 0.08160531 0.08474398 ## heterogeneity 3.84729188 3.64463819 3.45523411 3.27791632 3.11166077 2.95556109 ## [,29] [,30] [,31] [,32] [,33] [,34] ## x.axis.ticks 0.08788264 0.09102131 0.09415997 0.09729864 0.1004373 0.103576 ## heterogeneity 2.80881100 2.67069006 2.54055175 2.41781369 2.3019494 2.192481 ## [,35] [,36] [,37] [,38] [,39] [,40] ## x.axis.ticks 0.1067146 0.1098533 0.112992 0.1161306 0.1192693 0.122408 ## heterogeneity 2.0889748 1.9910337 1.898295 1.8104268 1.7271229 1.648102 ## [,41] [,42] [,43] [,44] [,45] [,46] ## x.axis.ticks 0.1255466 0.1286853 0.131824 0.1349626 0.1381013 0.141240 ## heterogeneity 1.5731038 1.5018885 1.434233 1.3699320 1.3087923 1.250636 ## [,47] [,48] [,49] [,50] [,51] [,52] ## x.axis.ticks 0.1443786 0.1475173 0.150656 0.1537946 0.1569333 0.1600720 ## heterogeneity 1.1952962 1.1426181 1.092456 1.0446756 0.9991485 0.9557561 ## [,53] [,54] [,55] [,56] [,57] [,58] ## x.axis.ticks 0.1632106 0.1663493 0.169488 0.1726266 0.1757653 0.1789039 ## heterogeneity 0.9143868 0.8749356 0.837304 0.8013992 0.7671341 0.7344262 ## [,59] [,60] [,61] [,62] [,63] [,64] ## x.axis.ticks 0.1820426 0.1851813 0.1883199 0.1914586 0.1945973 0.1977359 ## heterogeneity 0.7031982 0.6733769 0.6448933 0.6176821 0.5916817 0.5668339 ## [,65] [,66] [,67] [,68] [,69] [,70] ## x.axis.ticks 0.2008746 0.2040133 0.2071519 0.2102906 0.2134293 0.2165679 ## heterogeneity 0.5430835 0.5203783 0.4986689 0.4779083 0.4580523 0.4390587 ## [,71] [,72] [,73] [,74] [,75] [,76] ## x.axis.ticks 0.2197066 0.2228453 0.2259839 0.2291226 0.2322613 0.2353999 ## heterogeneity 0.4208875 0.4035008 0.3868625 0.3709386 0.3556964 0.3411050 ## [,77] [,78] [,79] [,80] [,81] [,82] ## x.axis.ticks 0.2385386 0.2416773 0.2448159 0.2479546 0.2510933 0.2542319 ## heterogeneity 0.3271352 0.3137588 0.3009494 0.2886817 0.2769315 0.2656760 ## [,83] [,84] [,85] [,86] [,87] [,88] ## x.axis.ticks 0.2573706 0.2605093 0.2636479 0.2667866 0.2699253 0.2730639 ## heterogeneity 0.2548932 0.2445624 0.2346637 0.2251782 0.2160880 0.2073757 ## [,89] [,90] [,91] [,92] [,93] [,94] ## x.axis.ticks 0.2762026 0.2793413 0.2824799 0.2856186 0.2887572 0.2918959 ## heterogeneity 0.1990251 0.1910205 0.1833469 0.1759901 0.1689366 0.1621733 ## [,95] [,96] [,97] [,98] [,99] [,100] ## x.axis.ticks 0.2950346 0.2981732 0.3013119 0.3044506 0.3075892 0.3107279 ## heterogeneity 0.1556879 0.1494685 0.1435038 0.1377830 0.1322958 0.1270324 The average lifetime: Hence we take one and divide it by the average dropout rate. 1/(0.6060/11.65) #i.e. ## [1] 19.22442 r = p.matrix[1,3] Alpha = p.matrix[1,4] 1/(r/Alpha) ## p.matrix ## 19.23271 We can then plot what characterizes the unobserved lifetime for a customer with an average dropoutrate # Plot average tendency to drop-out x &lt;- seq(0, 40, length.out=1000) dat &lt;- data.frame(x=x, densx=dexp(x, rate=0.052)) #Notice, 0.052 is the average dropout rate library(ggplot2) ggplot(dat, aes(x=x, y=densx)) + geom_line() # The transaction rate - NBD part pnbd.PlotTransactionRateHeterogeneity(params2) ## [,1] [,2] [,3] [,4] [,5] ## x.axis.ticks 0 0.003317791 0.006635582 0.009953373 0.01327116 ## heterogeneity Inf 28.378669973 20.104088608 16.195153025 13.75101668 ## [,6] [,7] [,8] [,9] [,10] ## x.axis.ticks 0.01658895 0.01990675 0.02322454 0.02654233 0.02986012 ## heterogeneity 12.01723584 10.69531406 9.63934342 8.76801657 8.03175296 ## [,11] [,12] [,13] [,14] [,15] [,16] ## x.axis.ticks 0.03317791 0.0364957 0.03981349 0.04313128 0.04644907 0.04976686 ## heterogeneity 7.39825343 6.8453831 6.35735156 5.92252045 5.53207687 5.17919351 ## [,17] [,18] [,19] [,20] [,21] [,22] ## x.axis.ticks 0.05308465 0.05640245 0.05972024 0.06303803 0.06635582 0.06967361 ## heterogeneity 4.85847735 4.56559617 4.29701856 4.04982850 3.82159058 3.61024998 ## [,23] [,24] [,25] [,26] [,27] [,28] ## x.axis.ticks 0.0729914 0.07630919 0.07962698 0.08294477 0.08626256 0.08958035 ## heterogeneity 3.4140571 3.23151010 3.06131004 2.90232606 2.75356748 2.61416160 ## [,29] [,30] [,31] [,32] [,33] [,34] ## x.axis.ticks 0.09289815 0.09621594 0.09953373 0.1028515 0.1061693 0.1094871 ## heterogeneity 2.48333568 2.36040227 2.24474714 2.1358192 2.0331225 1.9362085 ## [,35] [,36] [,37] [,38] [,39] [,40] ## x.axis.ticks 0.1128049 0.1161227 0.1194405 0.1227583 0.1260761 0.1293938 ## heterogeneity 1.8446710 1.7581406 1.6762806 1.5987832 1.5253664 1.4557714 ## [,41] [,42] [,43] [,44] [,45] [,46] ## x.axis.ticks 0.1327116 0.1360294 0.1393472 0.142665 0.1459828 0.1493006 ## heterogeneity 1.3897599 1.3271122 1.2676255 1.211112 1.1573975 1.1063207 ## [,47] [,48] [,49] [,50] [,51] [,52] ## x.axis.ticks 0.1526184 0.1559362 0.1592540 0.1625718 0.1658895 0.1692073 ## heterogeneity 1.0577313 1.0114894 0.9674649 0.9255359 0.8855889 0.8475175 ## [,53] [,54] [,55] [,56] [,57] [,58] ## x.axis.ticks 0.1725251 0.1758429 0.1791607 0.1824785 0.1857963 0.1891141 ## heterogeneity 0.8112221 0.7766093 0.7435917 0.7120869 0.6820176 0.6533111 ## [,59] [,60] [,61] [,62] [,63] [,64] ## x.axis.ticks 0.1924319 0.1957497 0.1990675 0.2023852 0.2057030 0.2090208 ## heterogeneity 0.6258990 0.5997167 0.5747033 0.5508016 0.5279574 0.5061194 ## [,65] [,66] [,67] [,68] [,69] [,70] ## x.axis.ticks 0.2123386 0.2156564 0.2189742 0.2222920 0.2256098 0.2289276 ## heterogeneity 0.4852393 0.4652714 0.4461723 0.4279011 0.4104190 0.3936892 ## [,71] [,72] [,73] [,74] [,75] [,76] ## x.axis.ticks 0.2322454 0.2355632 0.2388809 0.2421987 0.2455165 0.2488343 ## heterogeneity 0.3776767 0.3623486 0.3476734 0.3336212 0.3201639 0.3072744 ## [,77] [,78] [,79] [,80] [,81] [,82] ## x.axis.ticks 0.2521521 0.2554699 0.2587877 0.2621055 0.2654233 0.2687411 ## heterogeneity 0.2949273 0.2830983 0.2717641 0.2609029 0.2504937 0.2405165 ## [,83] [,84] [,85] [,86] [,87] [,88] ## x.axis.ticks 0.2720589 0.2753766 0.2786944 0.2820122 0.2853300 0.2886478 ## heterogeneity 0.2309525 0.2217835 0.2129924 0.2045626 0.1964787 0.1887256 ## [,89] [,90] [,91] [,92] [,93] [,94] ## x.axis.ticks 0.2919656 0.2952834 0.2986012 0.301919 0.3052368 0.3085546 ## heterogeneity 0.1812891 0.1741557 0.1673125 0.160747 0.1544474 0.1484026 ## [,95] [,96] [,97] [,98] [,99] [,100] ## x.axis.ticks 0.3118723 0.3151901 0.3185079 0.3218257 0.3251435 0.3284613 ## heterogeneity 0.1426017 0.1370345 0.1316912 0.1265623 0.1216390 0.1169126 So what is the interpretation of 0.0523, which is the average transaction rate. It means that on average the customers have 0.05 transactions per period (week). So the customer buys a CD every 20 week # Overall fit - Calibration data # Aggregate plots pnbd.PlotFrequencyInCalibration(params2, data$cal$cbs, 7) ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 freq.6 ## n.x.actual 1411.000 439.0000 214.0000 100.0000 62.00000 38.00000 29.00000 ## n.x.expected 1434.049 397.2334 194.0175 112.4031 70.59158 46.43743 31.47897 ## freq.7+ ## n.x.actual 64.00000 ## n.x.expected 70.78907 Using the holdout period # Fit Holdout period # Plot of performance in hold-out period T.star &lt;- 39 # length of the holdout period censor &lt;- 7 # This censor serves the same purpose as above x.star &lt;- data$holdout$cbs[,&quot;x.star&quot;] comp &lt;- pnbd.PlotFreqVsConditionalExpectedFrequency(params2, T.star, data$cal$cbs, x.star, censor) rownames(comp) &lt;- c(&quot;act&quot;, &quot;exp&quot;, &quot;bin&quot;) comp ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 ## act 0.2367116 0.6970387 1.392523 1.560000 2.532258 2.947368 ## exp 0.1384572 0.5995278 1.195945 1.713991 2.398495 2.907417 ## bin 1411.0000000 439.0000000 214.000000 100.000000 62.000000 38.000000 ## freq.6 freq.7+ ## act 3.862069 6.359375 ## exp 3.818842 6.403450 ## bin 29.000000 64.000000 The plot compares what we have observed and what we expect from the model. The following calculates the expected number of transactions in a given period. # A randomly selected individual pnbd.Expectation(params2,78) ## [1] 1.909942 We see that for any given customer we expect around 2 purchases for the period. #Probability that a randomly selected customer makes 0 purchases in the hold-out period pnbd.pmf.General(params2,T.star,t.end = 78,0) ## [1] 0.8109832 That is a probability of 81% # Analysing a specific customer - No of transactions data$cal$cbs[&quot;1516&quot;,] ## x t.x T.cal ## 26.00000 30.85714 31.00000 x &lt;- data$cal$cbs[&quot;1516&quot;, &quot;x&quot;] t.x &lt;- data$cal$cbs[&quot;1516&quot;, &quot;t.x&quot;] T.cal &lt;- data$cal$cbs[&quot;1516&quot;, &quot;T.cal&quot;] # x # t.x # T.cal We see that the customer has made 26 purchases in the calibration period. He bought something in period 30.8 and we see the holdout period is 31 periods, hence we expect him to be alive in the end of the hold out period. Now we want to calculate the number of transactions n period ahead given the information that we have about the customer. # T.star no of periods ahead df &lt;- pnbd.ConditionalExpectedTransactions(params2,T.star=1:5,x,t.x,T.cal) t(t(df)) ## [,1] ## [1,] 0.6328092 ## [2,] 1.2568947 ## [3,] 1.8725683 ## [4,] 2.4801239 ## [5,] 3.0798392 Notice, that the table is accumulated, hence in period 1 we expect 0.63 transactions, then in period 1 and 2 we expect 1.25 transactions and lastly in period etc. The following show how much it changes from period to period (week). res &lt;- data.frame(diff(as.matrix(df))) print(res) ## diff.as.matrix.df.. ## 1 0.6240856 ## 2 0.6156736 ## 3 0.6075556 ## 4 0.5997153 1.3.5.2 DERT Calculation # DERT discounted expected residual no. of transactions # 15% compounded annually has been converted to 0.0027 compounded continuously, # as we are dealing with weekly data and not annual data. d &lt;- 0.0027 pnbd.DERT(params2,x,t.x,T.cal,d) ## [1] 86.04656 We see that the DERT = 86, hence we see that the later we get the transactions, that amount is not as valuable today. # calculate the discounted expected residual transactions of a customer # who made 2 repeat transactions in a calibration period that was 38.86 # weeks long, with the last transaction occurring in the middle of # the 31th week. pnbd.DERT(params2, x=2, t.x=30.43, T.cal=38.86, d) ## [1] 6.483942 # We can also use vectors to compute DERT for several customers: pnbd.DERT(params2, x=1:10, t.x = 30.43, T.cal=38.86, d) ## [1] 3.994777 6.483942 8.895852 11.218449 13.438376 15.541043 17.510758 ## [8] 19.330967 20.984605 22.454582 We see that x iterates form 1 to 10, hence customers with 10 repeatative purchases in the calibration has a discounted expected residual no. of transactions that is increasing. pnbd.Plot.DERT(params2, x = 0:14 #Frequencies ,t.x = 0:38 #Recencies ,T.cal = 38.86 #Length training period ,d #The discount rate ,type = &quot;contour&quot;) ## 0 1 2 3 4 5 ## 0 0.4770585 0.5779056 0.3199351 0.1294895 0.04460094 0.01397432 ## 1 0.5184917 0.6832675 0.4167989 0.1856906 0.07018641 0.02409025 ## 2 0.5592651 0.7947977 0.5300382 0.2581760 0.10636616 0.03971331 ## 3 0.5993534 0.9116617 0.6600571 0.3494857 0.15607082 0.06301878 ## 4 0.6387393 1.0330181 0.8069337 0.4621052 0.22264492 0.09675453 ## 5 0.6774115 1.1580344 0.9704061 0.5983581 0.30980825 0.14431123 ## 6 0.7153638 1.2859006 1.1498716 0.7602863 0.42158225 0.20977683 ## 7 0.7525940 1.4158414 1.3444000 0.9495253 0.56217436 0.29796320 ## 8 0.7891031 1.5471246 1.5527617 1.1671812 0.73581558 0.41438948 ## 9 0.8248950 1.6790690 1.7734666 1.4137219 0.94655002 0.56520349 ## 10 0.8599754 1.8110487 2.0048151 1.6888919 1.19798164 0.75702129 ## 11 0.8943521 1.9424966 2.2449544 1.9916628 1.49299089 0.99666644 ## 12 0.9280339 2.0729051 2.4919389 2.3202264 1.83344279 1.29079726 ## 13 0.9610312 2.2018261 2.7437904 2.6720355 2.21991653 1.64542303 ## 14 0.9933550 2.3288698 2.9985543 3.0438914 2.65149127 2.06533082 ## 15 1.0250169 2.4537020 3.2543503 3.4320723 3.12562336 2.55346991 ## 16 1.0560292 2.5760412 3.5094141 3.8324921 3.63814310 3.11036768 ## 17 1.0864046 2.6956549 3.7621307 4.2408754 4.18338533 3.73367015 ## 18 1.1161560 2.8123560 4.0110568 4.6529342 4.75444977 4.41790410 ## 19 1.1452965 2.9259983 4.2549356 5.0645323 5.34356693 5.15453727 ## 20 1.1738393 3.0364728 4.4927016 5.4718254 5.94252951 5.93236950 ## 21 1.2017976 3.1437033 4.7234790 5.8713696 6.54314006 6.73822689 ## 22 1.2291847 3.2476429 4.9465745 6.2601955 7.13762631 7.55787120 ## 23 1.2560135 3.3482703 5.1614649 6.6358466 7.71898452 8.37699457 ## 24 1.2822971 3.4455859 5.3677826 6.9963880 8.28122567 9.18215837 ## 25 1.3080485 3.5396092 5.5652986 7.3403882 8.81951601 9.96155675 ## 26 1.3332801 3.6303756 5.7539053 7.6668818 9.33021804 10.70553071 ## 27 1.3580047 3.7179339 5.9335989 7.9753201 9.81084821 11.40681183 ## 28 1.3822343 3.8023438 6.1044629 8.2655135 10.25997364 12.06052117 ## 29 1.4059810 3.8836740 6.2666520 8.5375727 10.67707041 12.66397928 ## 30 1.4292567 3.9620003 6.4203787 8.7918504 11.06236429 13.21639451 ## 31 1.4520729 4.0374039 6.5658995 9.0288877 11.41666999 13.71849386 ## 32 1.4744408 4.1099700 6.7035044 9.2493657 11.74124065 14.17214757 ## 33 1.4963717 4.1797868 6.8335074 9.4540638 12.03763452 14.58002290 ## 34 1.5178761 4.2469441 6.9562376 9.6438245 12.30760232 14.94528679 ## 35 1.5389648 4.3115328 7.0720331 9.8195240 12.55299569 15.27136555 ## 36 1.5596478 4.3736437 7.1812347 9.9820494 12.77569585 15.56176058 ## 37 1.5799354 4.4333675 7.2841821 10.1322804 12.97755994 15.81991459 ## 38 1.5998372 4.4907936 7.3812097 10.2710759 13.16038262 16.04912020 ## 6 7 8 9 10 ## 0 0.004114556 0.001159004 3.157681e-04 8.381558e-05 2.178551e-05 ## 1 0.007764951 0.002393929 7.138094e-04 2.073603e-04 5.898752e-05 ## 2 0.013911768 0.004659695 1.509334e-03 4.762941e-04 1.471827e-04 ## 3 0.023846808 0.008624041 3.015561e-03 1.027220e-03 3.426460e-04 ## 4 0.039348724 0.015283693 5.738362e-03 2.098668e-03 7.515759e-04 ## 5 0.062804337 0.026082679 1.046679e-02 4.090760e-03 1.565463e-03 ## 6 0.097341684 0.043058286 1.839462e-02 7.651648e-03 3.116206e-03 ## 7 0.146968673 0.069016251 3.127967e-02 1.379946e-02 5.959438e-03 ## 8 0.216706710 0.107734234 5.164741e-02 2.409017e-02 1.099716e-02 ## 9 0.312702391 0.164188268 8.304433e-02 4.084317e-02 1.965375e-02 ## 10 0.442292476 0.244789707 1.303433e-01 6.743709e-02 3.412274e-02 ## 11 0.613988357 0.357609411 2.000964e-01 1.086871e-01 5.770463e-02 ## 12 0.837337733 0.512550652 3.009202e-01 1.713104e-01 9.525758e-02 ## 13 1.122616176 0.721412828 4.438759e-01 2.644750e-01 1.537813e-01 ## 14 1.480304501 0.997766489 6.427829e-01 4.004069e-01 2.431453e-01 ## 15 1.920325119 1.356542077 9.143596e-01 5.949913e-01 3.769505e-01 ## 16 2.451047326 1.813230636 1.278041e+00 8.682471e-01 5.734687e-01 ## 17 3.078129072 2.382619933 1.755282e+00 1.244470e+00 8.565240e-01 ## 18 3.803334027 3.077060969 2.368140e+00 1.751744e+00 1.256055e+00 ## 19 4.623528992 3.904386812 3.136983e+00 2.420453e+00 1.807924e+00 ## 20 5.530098955 4.865774384 4.077365e+00 3.280398e+00 2.552366e+00 ## 21 6.508986773 5.954000958 5.196353e+00 4.356373e+00 3.530366e+00 ## 22 7.541459566 7.152618292 6.489054e+00 5.662461e+00 4.777472e+00 ## 23 8.605543747 8.436466762 7.936315e+00 7.196076e+00 6.315236e+00 ## 24 9.677905483 9.773656234 9.504633e+00 8.933520e+00 8.141779e+00 ## 25 10.735844497 11.128729468 1.114882e+01 1.082906e+01 1.022448e+01 ## 26 11.759057333 12.466364174 1.281711e+01 1.281881e+01 1.249841e+01 ## 27 12.730910829 13.754825307 1.445762e+01 1.482916e+01 1.487299e+01 ## 28 13.639107893 14.968509526 1.602433e+01 1.678736e+01 1.724600e+01 ## 29 14.475770682 16.089249658 1.748154e+01 1.863136e+01 1.952054e+01 ## 30 15.237068094 17.106409934 1.880573e+01 2.031618e+01 2.161946e+01 ## 31 15.922558337 18.016063870 1.998532e+01 2.181591e+01 2.349298e+01 ## 32 16.534410940 18.819649453 2.101878e+01 2.312205e+01 2.511909e+01 ## 33 17.076635826 19.522464544 2.191207e+01 2.423957e+01 2.649864e+01 ## 34 17.554400495 20.132261326 2.267597e+01 2.518241e+01 2.764821e+01 ## 35 17.973474277 20.658081246 2.332386e+01 2.596939e+01 2.859317e+01 ## 36 18.339807487 21.109376736 2.386998e+01 2.662112e+01 2.936224e+01 ## 37 18.659234269 21.495404913 2.432831e+01 2.715782e+01 2.998382e+01 ## 38 18.937278694 21.824848001 2.471182e+01 2.759818e+01 3.048392e+01 ## 11 12 13 14 ## 0 5.565656e-06 1.401498e-06 3.486114e-07 8.580445e-08 ## 1 1.649322e-05 4.545512e-06 1.237476e-06 3.333597e-07 ## 2 4.470460e-05 1.338392e-05 3.958167e-06 1.158323e-06 ## 3 1.123421e-04 3.630590e-05 1.159028e-05 3.661323e-06 ## 4 2.645538e-04 9.178966e-05 3.145989e-05 1.066963e-05 ## 5 5.888245e-04 2.183060e-04 7.995208e-05 2.897502e-05 ## 6 1.247342e-03 4.921262e-04 1.918008e-04 7.396986e-05 ## 7 2.529353e-03 1.058118e-03 4.372585e-04 1.788017e-04 ## 8 4.933310e-03 2.181213e-03 9.526403e-04 4.117052e-04 ## 9 9.292292e-03 4.329827e-03 1.992853e-03 9.076120e-04 ## 10 1.696073e-02 8.307430e-03 4.019035e-03 1.923915e-03 ## 11 3.008594e-02 1.545486e-02 7.840820e-03 3.935931e-03 ## 12 5.199376e-02 2.795414e-02 1.484150e-02 7.795931e-03 ## 13 8.772304e-02 4.927422e-02 2.732605e-02 1.499145e-02 ## 14 1.447461e-01 8.480928e-02 4.904654e-02 2.805497e-02 ## 15 2.339067e-01 1.427686e-01 8.597678e-02 5.119875e-02 ## 16 3.705864e-01 2.353773e-01 1.474241e-01 9.127493e-02 ## 17 5.760557e-01 3.804256e-01 2.475742e-01 1.591894e-01 ## 18 8.788608e-01 6.031450e-01 4.075479e-01 2.719163e-01 ## 19 1.315912e+00 9.382389e-01 6.579669e-01 4.552454e-01 ## 20 1.932658e+00 1.431630e+00 1.041828e+00 7.472864e-01 ## 21 2.781397e+00 2.141033e+00 1.617066e+00 1.202455e+00 ## 22 3.916526e+00 3.133885e+00 2.457475e+00 1.895014e+00 ## 23 5.385739e+00 4.480675e+00 3.649677e+00 2.920073e+00 ## 24 7.217279e+00 6.241995e+00 5.283019e+00 4.388341e+00 ## 25 9.405651e+00 8.449499e+00 7.429806e+00 6.409740e+00 ## 26 1.190091e+01 1.108506e+01 1.011687e+01 9.062447e+00 ## 27 1.460780e+01 1.406709e+01 1.329680e+01 1.235119e+01 ## 28 1.739844e+01 1.725417e+01 1.683447e+01 1.617153e+01 ## 29 2.013567e+01 2.046970e+01 2.052333e+01 2.030592e+01 ## 30 2.269872e+01 2.353957e+01 2.413115e+01 2.446720e+01 ## 31 2.500153e+01 2.632692e+01 2.745534e+01 2.837445e+01 ## 32 2.699899e+01 2.875046e+01 3.036197e+01 3.182197e+01 ## 33 2.868251e+01 3.078400e+01 3.279551e+01 3.470910e+01 ## 34 3.006977e+01 3.244323e+01 3.476449e+01 3.702918e+01 ## 35 3.119358e+01 3.376890e+01 3.631731e+01 3.883688e+01 ## 36 3.209279e+01 3.481216e+01 3.751973e+01 4.021484e+01 ## 37 3.280617e+01 3.562473e+01 3.843937e+01 4.124994e+01 ## 38 3.336903e+01 3.625349e+01 3.913730e+01 4.202045e+01 abline(h = 1,col = &quot;darkred&quot;,cex = 0.8,lty = 2) abline(h = 12,col = &quot;darkblue&quot;,cex = 0.8,lty = 2) We see from the plot that if the recency is high, then we in general expect a customer to make many transactions, we see that all scenarios on the same line are worth the same. Lets take two examples. A customer with 1 transaction (red line): Has purchased something in approximately week 5, we expect him to do one more transaction, although if we look at a customer with 12 transactions, he must have a recency between week 15-20 to expect that he is going to place an order. That is because for the low frequency customer, it is not unusual to not buy something, to we think he is just sleeping and eventually come back one last time. Where the buying pattern for the other customer has drastically changed. Then we have a scenario of a customer making a lot of orders, we see that if he also pruchased very recently we will also expect him to place many orders in the future. Notice as the no. of transactions are already discounted, one can merely multiply the margin by the no. of transactions to find CLV. Now we can also look at a given customer to see if he is still alive. # P(alive) data$cal$cbs[&quot;1516&quot;,] ## x t.x T.cal ## 26.00000 30.85714 31.00000 x &lt;- data$cal$cbs[&quot;1516&quot;, &quot;x&quot;] t.x &lt;- data$cal$cbs[&quot;1516&quot;, &quot;t.x&quot;] T.cal &lt;- data$cal$cbs[&quot;1516&quot;, &quot;T.cal&quot;] # x # t.x # T.cal x = 26 t.x = 30.85 T.cal = 31 pnbd.PAlive(params2, x, t.x, T.cal) ## [1] 0.9978738 We see that he has a probability of 99% of being alive at the end of the calibration period. palive.pnbd &lt;- pnbd.PAlive(params2, x = 1:5, t.x = 12, T.cal = 39) for (i in 1:5) { cat(&quot;x =&quot;, i, &quot;:&quot;, sprintf(&quot;%5.2f %%&quot;, 100*palive.pnbd[i]), &quot;\\n&quot;) } ## x = 1 : 45.49 % ## x = 2 : 33.19 % ## x = 3 : 22.14 % ## x = 4 : 13.61 % ## x = 5 : 7.83 % We see that as number of transactions inceases the probability of being alive decrease. The reason for this is that we se that the t.x. is in period 12 (the most recent purchase). We see that the customer who has bought five times before period 12, hence i bought a lot. is pretty certain that he is not a customer anymore. Where the customer who only made one purchase before period 12, may still be a customer, bu his pattern just tells that he does not buy that often. # P(Alive) across customers p.alives &lt;- pnbd.PAlive(params2,data$cal$cbs[,&quot;x&quot;],data$cal$cbs[,&quot;t.x&quot;],data$cal$cbs[,&quot;T.cal&quot;]) plot(density(p.alives)) The figure explains where the customers mainly lie of being alive. Hence in the region between 0.3 and 0.4, the most customers lie. Now we can try to calculate the number of active customers. # Number of active customers library(tidyverse) no_of_active &lt;- sum(p.alives) no_of_active ## [1] 1051.878 That is the sum of the probability of p.alives. This is an estimate of how many customers that are actually alive. One could also have made a cutoff saying that we will not accumulate on customer that have less than a certain threshold, e.g., 20%. This is what he does in the following, where he does: Creating a data frame Sorting the customers Plotting the spread Calculating how many active customers there are in the specific scenario. p.alive1 &lt;- as.data.frame(p.alives) head(p.alive1) p.alives 0.8691602 0.1679958 0.2951073 0.2951073 0.2951073 0.7494963 p.alive2 &lt;- tibble::rowid_to_column(p.alive1, &quot;ID&quot;) head(p.alive2) ID p.alives 1 0.8691602 2 0.1679958 3 0.2951073 4 0.2951073 5 0.2951073 6 0.7494963 newalive &lt;- arrange(p.alive2,desc(p.alives)) newalive2 &lt;- tibble::rowid_to_column(newalive, &quot;ID1&quot;) head(newalive2,6) ID1 ID p.alives 1 59 1 2 495 1 3 519 1 4 584 1 5 720 1 6 761 1 plot(newalive2$ID1,newalive2$p.alives,xlab=&quot;Sorted customers&quot;, ylab=&quot;P(Alive)&quot;) abline(h =0.5) We see that there are approximately 500 active customers. 1.3.5.3 BG/NBG example Notice, this is showing how this can be applied to any data set. Remember that that column names should be x, t.x, T.cal Notice, that this is not a super good files, as it is with customer with a loyalty cards, to they make many repeatitive purchases library(BTYD) cal1 &lt;- read.csv(&quot;Data/BTYD/cal2010.csv&quot;, header=TRUE,sep=&#39;;&#39;) cal2010 &lt;- as.matrix(cal1,nrow=3084,ncol=3) colnames(cal2010) &lt;- c(&quot;x&quot;,&quot;t.x&quot;,&quot;T.cal&quot;) head(cal2010,10) ## x t.x T.cal ## [1,] 77 85 85 ## [2,] 83 85 85 ## [3,] 56 83 85 ## [4,] 65 85 85 ## [5,] 77 85 85 ## [6,] 78 85 85 ## [7,] 78 85 85 ## [8,] 57 83 85 ## [9,] 83 85 85 ## [10,] 79 85 85 params &lt;- bgnbd.EstimateParameters(cal2010,par.start = c(1, 1, 1, 1), max.param.value = 10000) params ## p1 p2 p3 p4 ## 11.979204509 14.759439087 0.004696903 6.505009679 LL &lt;- bgnbd.cbs.LL(params, cal2010) LL ## [1] -251853.9 p.matrix &lt;- c(params, LL); p.matrix ## p1 p2 p3 p4 ## 1.197920e+01 1.475944e+01 4.696903e-03 6.505010e+00 -2.518539e+05 for (i in 1:2){ params &lt;- bgnbd.EstimateParameters(cal2010, params); LL &lt;- bgnbd.cbs.LL(params, cal2010); p.matrix.row &lt;- c(params, LL); p.matrix &lt;- rbind(p.matrix, p.matrix.row); } colnames(p.matrix) &lt;- c(&quot;r&quot;, &quot;alpha&quot;, &quot;a&quot;, &quot;b&quot;, &quot;LL&quot;); rownames(p.matrix) &lt;- 1:3; p.matrix; ## r alpha a b LL ## 1 11.97920 14.75944 0.004696903 6.505010 -251853.9 ## 2 11.97929 14.75931 0.004696945 6.505151 -251853.9 ## 3 11.97919 14.75943 0.004696956 6.505187 -251853.9 bgnbd.PlotTransactionRateHeterogeneity(params) ## [,1] [,2] [,3] [,4] [,5] ## x.axis.ticks 0 1.468813e-02 2.937627e-02 4.406440e-02 5.875254e-02 ## heterogeneity 0 1.607127e-14 2.611953e-11 1.803659e-09 3.417673e-08 ## [,6] [,7] [,8] [,9] [,10] ## x.axis.ticks 7.344067e-02 8.812881e-02 1.028169e-01 1.175051e-01 1.321932e-01 ## heterogeneity 3.188407e-07 1.900070e-06 8.310764e-06 2.898648e-05 8.504691e-05 ## [,11] [,12] [,13] [,14] [,15] ## x.axis.ticks 0.1468813448 0.1615694793 0.176257614 0.190945748 0.205633883 ## heterogeneity 0.0002177151 0.0004991105 0.001044562 0.002025089 0.003678372 ## [,16] [,17] [,18] [,19] [,20] [,21] ## x.axis.ticks 0.220322017 0.23501015 0.24969829 0.26438642 0.2790746 0.2937627 ## heterogeneity 0.006316514 0.01032904 0.01618007 0.02439906 0.0355653 0.0502867 ## [,22] [,23] [,24] [,25] [,26] [,27] ## x.axis.ticks 0.30845082 0.32313896 0.3378271 0.3525152 0.3672034 0.3818915 ## heterogeneity 0.06917424 0.09281349 0.1217351 0.1563863 0.1971043 0.2440947 ## [,28] [,29] [,30] [,31] [,32] [,33] ## x.axis.ticks 0.3965796 0.4112678 0.4259559 0.4406440 0.4553322 0.4700203 ## heterogeneity 0.2974142 0.3569603 0.4224665 0.4935051 0.5694952 0.6497169 ## [,34] [,35] [,36] [,37] [,38] [,39] ## x.axis.ticks 0.4847084 0.4993966 0.5140847 0.5287728 0.543461 0.5581491 ## heterogeneity 0.7333301 0.8193970 0.9069070 0.9948030 1.082009 1.1674554 ## [,40] [,41] [,42] [,43] [,44] [,45] ## x.axis.ticks 0.5728372 0.5875254 0.6022135 0.6169016 0.6315898 0.6462779 ## heterogeneity 1.2501053 1.3289753 1.4031568 1.4718315 1.5342855 1.5899181 ## [,46] [,47] [,48] [,49] [,50] [,51] ## x.axis.ticks 0.6609661 0.6756542 0.6903423 0.7050305 0.7197186 0.7344067 ## heterogeneity 1.6382485 1.6789183 1.7116911 1.7364497 1.7531903 1.7620148 ## [,52] [,53] [,54] [,55] [,56] [,57] ## x.axis.ticks 0.7490949 0.763783 0.7784711 0.7931593 0.8078474 0.8225355 ## heterogeneity 1.7631221 1.756797 1.7434007 1.7233560 1.6971386 1.6652640 ## [,58] [,59] [,60] [,61] [,62] [,63] ## x.axis.ticks 0.8372237 0.8519118 0.8665999 0.8812881 0.8959762 0.9106643 ## heterogeneity 1.6282766 1.5867388 1.5412215 1.4922951 1.4405213 1.3864463 ## [,64] [,65] [,66] [,67] [,68] [,69] ## x.axis.ticks 0.9253525 0.9400406 0.9547287 0.9694169 0.984105 0.9987931 ## heterogeneity 1.3305948 1.2734652 1.2155255 1.1572106 1.098920 1.0410155 ## [,70] [,71] [,72] [,73] [,74] [,75] ## x.axis.ticks 1.0134813 1.0281694 1.0428575 1.0575457 1.072234 1.0869220 ## heterogeneity 0.9838234 0.9276314 0.8726912 0.8192185 0.767395 0.7173703 ## [,76] [,77] [,78] [,79] [,80] [,81] ## x.axis.ticks 1.1016101 1.1162982 1.1309864 1.1456745 1.160363 1.1750508 ## heterogeneity 0.6692633 0.6231647 0.5791393 0.5372285 0.497452 0.4598109 ## [,82] [,83] [,84] [,85] [,86] [,87] ## x.axis.ticks 1.1897389 1.2044270 1.2191152 1.2338033 1.2484914 1.2631796 ## heterogeneity 0.4242895 0.3908575 0.3594724 0.3300808 0.3026212 0.2770246 ## [,88] [,89] [,90] [,91] [,92] [,93] ## x.axis.ticks 1.2778677 1.2925558 1.3072440 1.3219321 1.3366202 1.3513084 ## heterogeneity 0.2532168 0.2311196 0.2106519 0.1917308 0.1742728 0.1581945 ## [,94] [,95] [,96] [,97] [,98] [,99] ## x.axis.ticks 1.3659965 1.3806846 1.3953728 1.4100609 1.42474904 1.43943718 ## heterogeneity 0.1434135 0.1298486 0.1174207 0.1060533 0.09567221 0.08620665 ## [,100] ## x.axis.ticks 1.45412531 ## heterogeneity 0.07758884 bgnbd.PlotDropoutRateHeterogeneity(params) ## [,1] [,2] [,3] [,4] [,5] ## x.axis.ticks 0 1.187082e-04 2.374164e-04 3.561247e-04 0.0004748329 ## heterogeneity Inf 3.832157e+01 1.921071e+01 1.282317e+01 9.6240849257 ## [,6] [,7] [,8] [,9] [,10] ## x.axis.ticks 0.0005935411 0.0007122493 0.0008309575 0.0009496657 0.001068374 ## heterogeneity 7.7023038611 6.4198859224 5.5031446971 4.8151220706 4.279676455 ## [,11] [,12] [,13] [,14] [,15] ## x.axis.ticks 0.001187082 0.00130579 0.001424499 0.001543207 0.001661915 ## heterogeneity 3.851094992 3.50027201 3.207794374 2.960216351 2.747930017 ## [,16] [,17] [,18] [,19] [,20] ## x.axis.ticks 0.001780623 0.001899331 0.00201804 0.002136748 0.002255456 ## heterogeneity 2.563886969 2.402799107 2.26062130 2.134206411 2.021069151 ## [,21] [,22] [,23] [,24] [,25] ## x.axis.ticks 0.002374164 0.002492873 0.002611581 0.002730289 0.002848997 ## heterogeneity 1.919220770 1.827050925 1.743241750 1.666704325 1.596531025 ## [,26] [,27] [,28] [,29] [,30] ## x.axis.ticks 0.002967705 0.003086414 0.003205122 0.00332383 0.003442538 ## heterogeneity 1.531959283 1.472343727 1.417134488 1.36586017 1.318114309 ## [,31] [,32] [,33] [,34] [,35] ## x.axis.ticks 0.003561247 0.003679955 0.003798663 0.003917371 0.004036079 ## heterogeneity 1.273544600 1.231844131 1.192744316 1.156009089 1.121430124 ## [,36] [,37] [,38] [,39] [,40] ## x.axis.ticks 0.004154788 0.004273496 0.004392204 0.004510912 0.00462962 ## heterogeneity 1.088822879 1.058023293 1.028885021 1.001277106 0.97508201 ## [,41] [,42] [,43] [,44] [,45] ## x.axis.ticks 0.004748329 0.004867037 0.004985745 0.005104453 0.005223162 ## heterogeneity 0.950193927 0.926517373 0.903965941 0.882461254 0.861932053 ## [,46] [,47] [,48] [,49] [,50] ## x.axis.ticks 0.00534187 0.005460578 0.005579286 0.005697994 0.005816703 ## heterogeneity 0.84231340 0.823546016 0.805575636 0.788352526 0.771831007 ## [,51] [,52] [,53] [,54] [,55] ## x.axis.ticks 0.005935411 0.006054119 0.006172827 0.006291536 0.006410244 ## heterogeneity 0.755969048 0.740727911 0.726071835 0.711967753 0.698385048 ## [,56] [,57] [,58] [,59] [,60] ## x.axis.ticks 0.006528952 0.00664766 0.006766368 0.006885077 0.007003785 ## heterogeneity 0.685295324 0.67267221 0.660491193 0.648729434 0.637365651 ## [,61] [,62] [,63] [,64] [,65] ## x.axis.ticks 0.007122493 0.007241201 0.007359909 0.007478618 0.007597326 ## heterogeneity 0.626379975 0.615753839 0.605469875 0.595511813 0.585864400 ## [,66] [,67] [,68] [,69] [,70] ## x.axis.ticks 0.007716034 0.007834742 0.007953451 0.008072159 0.008190867 ## heterogeneity 0.576513321 0.567445127 0.558647172 0.550107551 0.541815051 ## [,71] [,72] [,73] [,74] [,75] ## x.axis.ticks 0.008309575 0.008428283 0.008546992 0.0086657 0.008784408 ## heterogeneity 0.533759096 0.525929709 0.518317462 0.5109134 0.503709235 ## [,76] [,77] [,78] [,79] [,80] ## x.axis.ticks 0.008903116 0.009021825 0.009140533 0.009259241 0.009377949 ## heterogeneity 0.496696845 0.489868719 0.483217689 0.476736952 0.470420054 ## [,81] [,82] [,83] [,84] [,85] ## x.axis.ticks 0.009496657 0.009615366 0.009734074 0.009852782 0.00997149 ## heterogeneity 0.464260859 0.458253535 0.452392536 0.446672580 0.44108864 ## [,86] [,87] [,88] [,89] [,90] [,91] ## x.axis.ticks 0.0100902 0.01020891 0.01032761 0.01044632 0.01056503 0.01068374 ## heterogeneity 0.4356359 0.43030985 0.42510607 0.42002042 0.41504892 0.41018778 ## [,92] [,93] [,94] [,95] [,96] [,97] ## x.axis.ticks 0.01080245 0.01092116 0.01103986 0.01115857 0.01127728 0.01139599 ## heterogeneity 0.40543336 0.40078218 0.39623092 0.39177640 0.38741557 0.38314551 ## [,98] [,99] [,100] ## x.axis.ticks 0.0115147 0.01163341 0.01175211 ## heterogeneity 0.3789634 0.37486657 0.37085243 bgnbd.Expectation(params, t=52) ## p3 ## 41.92832 bgnbd.PlotFrequencyInCalibration(params, cal2010, 85) ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 freq.6 ## n.x.actual 0.000000e+00 4.000000 2.000000 4.000000 2.000000 4.000000 2.000000 ## n.x.expected 3.529993e-07 2.225145 1.927474 1.699974 1.520557 1.375656 1.256601 ## freq.7 freq.8 freq.9 freq.10 freq.11 freq.12 freq.13 ## n.x.actual 3.000000 6.000000 4.000000 5.0000000 5.0000000 3.0000000 3.0000000 ## n.x.expected 1.157774 1.075629 1.008158 0.9546069 0.9153288 0.8917102 0.8861372 ## freq.14 freq.15 freq.16 freq.17 freq.18 freq.19 freq.20 ## n.x.actual 3.0000000 5.0000000 6.000000 10.000000 5.000000 12.000000 9.000000 ## n.x.expected 0.9019715 0.9435202 1.015989 1.125409 1.278537 1.482724 1.745754 ## freq.21 freq.22 freq.23 freq.24 freq.25 freq.26 freq.27 ## n.x.actual 9.000000 6.000000 10.000000 4.000000 7.000000 4.00000 12.000000 ## n.x.expected 2.075664 2.480535 2.968283 3.546425 4.221866 5.00068 5.887913 ## freq.28 freq.29 freq.30 freq.31 freq.32 freq.33 freq.34 ## n.x.actual 5.000000 16.000000 10.000000 7.00000 14.00000 7.00000 15.0000 ## n.x.expected 6.887405 8.001637 9.231614 10.57678 12.03495 13.60233 15.2735 ## freq.35 freq.36 freq.37 freq.38 freq.39 freq.40 freq.41 ## n.x.actual 9.0000 17.00000 12.00000 15.00000 17.00000 11.00000 18.00000 ## n.x.expected 17.0415 18.89789 20.83288 22.83548 24.89363 26.99441 29.12421 ## freq.42 freq.43 freq.44 freq.45 freq.46 freq.47 freq.48 ## n.x.actual 16.00000 36.00000 15.00000 22.00000 22.00000 16.00000 19.0000 ## n.x.expected 31.26893 33.41418 35.54548 37.64844 39.70895 41.71335 43.6486 ## freq.49 freq.50 freq.51 freq.52 freq.53 freq.54 freq.55 ## n.x.actual 20.0000 23.00000 25.00000 17.00000 30.00000 22.00000 24.00000 ## n.x.expected 45.5024 47.26333 48.92094 50.46586 51.88983 53.18578 54.34785 ## freq.56 freq.57 freq.58 freq.59 freq.60 freq.61 freq.62 ## n.x.actual 27.00000 29.00000 31.00000 33.00000 23.00000 31.00000 31.00000 ## n.x.expected 55.37139 56.25297 56.99036 57.58247 58.02933 58.33204 58.49268 ## freq.63 freq.64 freq.65 freq.66 freq.67 freq.68 freq.69 ## n.x.actual 35.00000 28.00000 34.0000 51.00000 47.00000 47.00000 41.00000 ## n.x.expected 58.51425 58.40059 58.1563 57.78664 57.29747 56.69516 55.98646 ## freq.70 freq.71 freq.72 freq.73 freq.74 freq.75 freq.76 ## n.x.actual 62.00000 61.00000 45.00000 53.00000 74.00000 73.00000 78.0000 ## n.x.expected 55.17848 54.27856 53.29424 52.23314 51.10291 49.91118 48.6655 ## freq.77 freq.78 freq.79 freq.80 freq.81 freq.82 ## n.x.actual 93.00000 109.00000 94.00000 123.00000 161.00000 184.00000 ## n.x.expected 47.37326 46.04169 44.67778 43.28826 41.87956 40.45781 ## freq.83 freq.84 freq.85+ ## n.x.actual 205.00000 225.000 327.00000 ## n.x.expected 39.02881 37.598 36.17045 "],["customer-satisfaction-and-loyalty.html", "2 Customer Satisfaction and Loyalty 2.1 Customer surveys 2.2 NPS 2.3 CFA and SEM Application Lavann 2.4 PLS 2.5 Segmentation (Clustering)", " 2 Customer Satisfaction and Loyalty I’ll start of with some notes on constructing a survey. 2.1 Customer surveys The following sections are separated into five subsections: Concepts and examples Sampling and response rate Pretesting and measurement validity Questionnaire design Question types Scale labeling Question style Social desirability bias Optimizing versus satisfaction Simple size 2.1.1 (1) Concepts and examples We have six types of data: Customer identification data - information on the customer and who they are. Demographic data - information on a more macroscale, where they are from, ethnicities, age etc. Psychographic or lifestyle data - psychological characteristics and traits such as values, desires, goals, interests, and lifestyle choices. Transaction data - Do we have transactional information on the customer? What should it be used for and should it be preprocessed. Marketing action data Other types of data - e.g., financial and competitive data To get such data we can have three different sources. Internal secondary data External secondary data Primary data - survey, focus groups, in depth-interviews, observational techniques. Notice that this is the most costly and time consuming, that is also why getting external secondary data can be extremely expensive. Another thing that one must be aware of is innovation in survey designs, for instance, are we able to ask about what a customer wants in the future and if they answer. If you could rely on such answers, then I guess it would be very easy to be running a business. In addition of this, it is often experienced, that people are reluctant to answer honestly in surveys, this can be due to several things, for instance you are: Forced to answer - hence no motivation You don’t care about the purpose - hence no motivation. 2.1.2 (2) Sampling and response rate We need the sample to be representative of the population, e.g., equal amount of males and females to reflect real life. Naturally that comes down to what the purpose of the survey is. We often get a problem with low response rate. And those that actually respond might share similarities, e.g., they are having a lot of free time, they need the rewards that you may get from answering etc. So you want to be cautous about how to get them to answer your survey. We can overcome this by assigning statistical weigts to the underrepresented groups. Often we see that young and old adults, males and people with the highest income are underrepresented. Also there is a tendency to have few responses from busy people and those living in bigger cities. On the other hand then people with low education and income is often overrepresented. Then how to do the sampling? We have the following two approaches: Non-probability sampling - this is pretty much taking whom ever you feel like, for instance sharing a questionnaire on facebook, hence it will be distributed to your friends etc. This is not representative Probability sampling - this is the go to approach. Here we have four approaches. This will be explained following the picture. Sampling techniques Probability sampling techniques Simple random sampling: that is just a computer randomly selecting numbers. And then you select these observations. Systematic sampling: we do 1) selecting a suitable sampling frame, 2) Each element is assigned a number from 1 to N (Population for instance Denmark), 3) Determine sampling interval, 4) Pcik every ith element in succession from the sampling frame. Stratified sampling: where you group people in different stratas. They can be assigned on one or more parameters. This is widely used. See an illustration below Cluster sampling. Stratified sampling Subconclusion In the end, you always ask youself, is it representative. When you have the probabilty sampling methods, you randomly select a number of people that should answer, e.g., you selesct 1000, but only 543 answers, then do you know what people that actually replied, hence, do you know who answered and is it still representative? If you don’t know, then the data is very likely to be representative. Also often people will answer because you ‘forced’ them to answer, hence they are not really reliable, as we don’t know how honest and thorough they where. That is also elaborated in the following slide. So again, if you need 1000 responses, and you force all 1000 to respond, then they will often just rush through and thus they are not reliable. Hence that is not better than not letting them answer. 2.1.3 (3) Pretesting and measurement validity We need to pretest the sampling, the tradition way, conventional way. How we do it, is just giving the test to a couple of people and see if they understand the questions. This is key, because if they do not understand then you cannot expect that the mass will understand the questions. In general there are two approaches: Conventaional way: That is giving the questions to persons and they tell you their oppinion. One must be aware that this is very subjective and also a bit random. Moden pretesting way: This has two alternatives Behavior coding: you have a third person, an observer, will take notes about the responends actions. Cognitive pretesting: This is an alternative, the respondent must think out loud, hence verbalize whatever comes to mind when reading the questions. Subconclusion Conventional way has low reliability. And the behavior coding is the most reliable. An complete alternative can be to have conversational interviews although this is very time consuming and often not applied. 2.1.4 (4) Questionnaire design The following subsections will cover this. (4a) Question types Are we going to make open or closed questions. It has the following advantages and disadvantages. Closed questions: Respondents answer what they are given, henve we need to cover the different outcomes to have their experience reflected correctly. Open questions: Some respondents have difficulty of expressing their feelings or people just get lazy and not answer faithfully. Also you will have a ton of different answers. Hence they both have pros and cons, so one must be aware of when to choose what. In general we must be cautious about how we use strong words, like very, extreme etc. because it can have several meanings to each individual. To avoid mistakes, we can setup a questionnaire with questions that are essentially the same, but framed different, to get an idea of what the person intent to ask and also how consistent he is (that being for instance a weak satisfier). (4b) Scale labeling It is very good to have both a range defined by words, e.g., going from very bad to very good. Criticism of showing the numbers, let us say that it goes from -3 to 3, then the distance between -2 and -3 is one, but the numerical distance between two categories it not necessarily between -1 and -2. To solve this, people sometimes puts a line, e.g., between 0 and 100, then the repsonded is able to point out where he is on the range. Also, even though we produce numbers, people tend to do quantitative regression, while it is in fact a categorical variable. There are much criticism on using numbers. Also, the middle point, what does it mean, to the respondend it can be many many things. (4c) Question style Will it be an open or a closed question. (4d) Social desirability bias Imagine a questionnaire where you are asked about what happens in the past, for instance over the last year, how many times pr. month on average have you eaten a burger? 1 time, 2 - 3 times, 4 - 5 times, more than five times, 0 times This is often leading to over- and underreporting, as nobody can remember this. Theory has lead to stages when answering questinons (this should be taken with a grain of salt), see the following. Stages of answering questions In short, one must look up what they have in their memory and then try to fit it into the boxes thzat you give them. It is different how much effort a person is putting into the considerations, it can be generalized with: Optimizers: A thourough person really considering what to answer, these you want the most of. Weak satisfaction: This is just an optimizer which is less considerate. Strong satisficing: A person which will just select what he thinks is the most appropriate for the interviewer or researcher, these you do not want any of. This leads to the next section. (4e) Optimizing versus satisfaction Conditions to foster satisfaction. We need to make the questions as short as possible and use common words. It must be easy for them to understand to avoid them not to understand. This can be cooked down to: Task difficulty Respondents inability Respondents demotivation to optimize We now that the order of questions does matter: it creates the following effects. Primacy effects: That being prone to select the first question, e.g., weak satisfiers seem to go for this one, because they are just not too considerate Recency effects: Often when solutions is presented orally then they go for the last option. If we have a loooong list of questions, then people tend to either just agree or disagree. In general, people who want to save time, they tend to answer the same in each question, if they are similar. 2.1.5 (5) Simple size We can infer this statistically. Notice this example assume only one predictor variable. Sample size estimate We see that: z = the confidence level, we go for 5% hence 1.96. D = The interval you want to be within sigma = Variance, notice that this must be estimated, as to get true sigma you need characteristics on the population, and we don’t really have that. And now we can plug it into the function Sample size inference example Conducting a survey to estimate the monthly amount invested in savings schemes so that the estimate will be within 5 EUR, what would the sample size be? And we see that we need 496 respondents. Then what happens if we have multiple predictors? Then we can infer for each predictor and take the largest value. A rule of thumb is that we need at least 5 observations for a predictor 2.2 NPS # x &lt;- ##The ratings go here # library(NPS) # prop.table(table(x)) # nps.se(x) # nps.var(x) 2.3 CFA and SEM Application Lavann Data source: Hair et al., Multivariate Data Analysis, Pearson Education 2.3.1 Business Problem &amp; Objectives HBAT is a manufacturer of paper products who sells products to two market segments: the newsprint industry and the magazine industry. The current market is very competitive, so the manufacturer wants to understand how its customers perceive the company and make purchasing decisions, in order to enforce customers loyalty. The manufacturer commissioned a study asking its customers to complete a questionnaire on a secure website. In total, 100 customers - purchasing managers from different firms - buying from HBAT completed the questionnaire. The data consist of three main pieces of information: • A 1st type of information is available from HBAT ́s data warehouse and includes information on: customer type in terms of length of purchase relationship (X1) industry type(X2) size of the customer(X3) region of the customer(X4) distribution system(X5) • The 2nd type of information is collected based on the online questionnaire and includes consumers’ perceptions of HBAT ́s performance on 13 attributes using a continuous 0-10 (line) scale with 10 being “Excellent” and 0 being “Poor”. The 13 attributes are: X6 Product quality X7 E-commerce X8 Technical support X9 Complaint resolution X10 Advertising X11 Product line X12 Salesforce image X13 Competitive pricing X14 Warranty and claims X15 Packaging X16 Order and billing X17 Price flexibility X18 Delivery speed • The 3rd type of information relates to purchase outcomes and business relationships: satisfaction with HBAT, future purchase intention etc. (X19-X22) whether the firm would consider a strategic alliance/partnership with HBAT (X23). 2.3.2 Data The dataset (HBAT.sav) consists of data for n = 100 customers. Each observation contains information on 23 variables described above. Consistent with the marketing theory, there is an underlying factor structure in the data. When designing the study, the company has clearly 4 types of factors in their mind. They expect that the customer satisfaction is determined by the following four type of perceptions: perceptions about the product value, perceptions about the marketing actions, perceptions about the customer service and perceptions about the technical support.These factors are abstract constructs that can be measured in a survey using multi-item scales. The following items define each construct: X18 Delivery Speed X9 Complain resolution X16 Order and Billing, to express “Customer service” X11Product line X6 Product quality X13 Competing pricing, to express “Product value” X12 Salesforce image X7 E-commerce X10 Advertising, to express “Marketing” X8 Technical support X14 Warranty and claims, to express “Technical support” library(lavaan) library(foreign) data &lt;- read.spss(&quot;Data/CFA and SEM/HBAT.sav&quot;, to.data.frame=TRUE) 2.3.2.1 EFA In the following we are going to identify the factors and what variables that are related with the factors. # Exploratory factor analysis (EFA): explicitly assumes the existence of latent factors underlying the observed data. fit1 &lt;- factanal(~ x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 ,factors = 4 ,data = data ,lower = 0.1 ,rotation = &quot;varimax&quot;) print(fit1) ## ## Call: ## factanal(x = ~x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18, factors = 4, data = data, rotation = &quot;varimax&quot;, lower = 0.1) ## ## Uniquenesses: ## x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 ## 0.623 0.305 0.285 0.183 0.663 0.100 0.100 0.595 0.100 0.987 0.358 0.100 0.100 ## ## Loadings: ## Factor1 Factor2 Factor3 Factor4 ## x6 0.609 ## x7 0.826 ## x8 0.113 0.838 ## x9 0.879 0.153 0.140 ## x10 0.199 -0.128 0.530 ## x11 0.500 0.814 0.115 ## x12 0.125 -0.140 0.929 ## x13 -0.561 0.219 -0.198 ## x14 0.146 0.931 ## x15 0.109 ## x16 0.784 0.109 0.111 ## x17 0.553 -0.750 0.204 -0.132 ## x18 0.940 0.122 0.183 ## ## Factor1 Factor2 Factor3 Factor4 ## SS loadings 2.910 2.030 1.990 1.657 ## Proportion Var 0.224 0.156 0.153 0.127 ## Cumulative Var 0.224 0.380 0.533 0.661 ## ## Test of the hypothesis that 4 factors are sufficient. ## The chi square statistic is 162.89 on 32 degrees of freedom. ## The p-value is 1.83e-19 #We see that the p-value is highly signinficant, hence we reject the model. A first exploration of the factors using the EFA reveals the factor loadings for each observable variable. High loadings (&gt;0.6 or &gt; 0.7) are expected for the items that theoretically define each construct. Items with cross-loadings should be removed. One can observe: x9, x16, x17, x18 load high on Factor 1 (Customer service) x6, x11, x13, x17 load high on Factor 2 (Product value) x7, x10?, x12 load high on Factor 3 (Marketing) x8 and x14 load high on Factor 4 (Technical support) x17 load high simultaneusly on two factors (Factor 1 and 2). This phenomenon is called “cross-loading”. one needs to remove the items with cross-loading from the measurement model. It means they do not measure a single construct. x11 might also be a candidate for deletion (loading high on factor 1 and 2); keep it for the moment. one can delete x15 which does not have high loadings on ANY of the four factors. Very low loadings (&lt;.10) are not displayed. # Run EFA without x15 and x17 fit2 &lt;- factanal(~ x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x16 + x18 ,factors = 4 ,data = data ,lower = 0.1 ,rotation = &quot;varimax&quot;) print(fit2,sort = T) ## ## Call: ## factanal(x = ~x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x16 + x18, factors = 4, data = data, rotation = &quot;varimax&quot;, lower = 0.1) ## ## Uniquenesses: ## x6 x7 x8 x9 x10 x11 x12 x13 x14 x16 x18 ## 0.635 0.305 0.285 0.163 0.668 0.100 0.100 0.599 0.100 0.342 0.100 ## ## Loadings: ## Factor1 Factor2 Factor3 Factor4 ## x9 0.895 0.135 0.128 ## x16 0.796 0.105 0.108 ## x18 0.918 0.193 0.154 ## x7 0.827 ## x10 0.180 0.537 ## x12 0.127 0.928 -0.144 ## x8 0.838 0.107 ## x14 0.932 0.140 ## x6 0.598 ## x11 0.519 0.120 0.786 ## x13 0.223 -0.202 -0.553 ## ## Factor1 Factor2 Factor3 Factor4 ## SS loadings 2.613 1.962 1.645 1.391 ## Proportion Var 0.238 0.178 0.150 0.126 ## Cumulative Var 0.238 0.416 0.565 0.692 ## ## Test of the hypothesis that 4 factors are sufficient. ## The chi square statistic is 26.7 on 17 degrees of freedom. ## The p-value is 0.0626 We want the cumulative variance to be above 60%, we see that we meet that criteria Test of the hypothesis that 4 factors are sufficient. The chi square statistic is 26.7 on 17 degrees of freedom. The p-value is 0.0626 (n.s.) hence we cannot reject, that there is a relationship. Using eigen value to assess amount of factors to select. # NOTE library(nFactors) ev &lt;- eigen(cor(data[,c(7:19)])) ev ## eigen() decomposition ## $values ## [1] 3.567074795 2.997644547 1.738077491 1.287225051 1.005237601 0.618614261 ## [7] 0.551427702 0.446993915 0.280735795 0.200708880 0.166195339 0.131058876 ## [13] 0.009005747 ## ## $vectors ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -0.008446162 0.36209283 0.04678935 0.47025543 -0.05007347 0.053218125 ## [2,] -0.244268729 -0.25887463 -0.35900707 0.34222834 0.09769943 0.376718762 ## [3,] -0.069090049 0.27405908 -0.52503623 -0.36487985 -0.01424126 -0.052463334 ## [4,] -0.457137214 0.12307431 0.20045850 -0.10240177 0.08595970 0.056968447 ## [5,] -0.255224876 -0.20968349 -0.18298044 0.32643625 -0.12554223 -0.780692662 ## [6,] -0.252450282 0.42196215 0.10369985 0.21751972 0.03205501 0.163391859 ## [7,] -0.290987483 -0.27281337 -0.35867289 0.29559280 0.01309635 0.195905988 ## [8,] 0.008246314 -0.43224128 0.01407530 -0.15377746 -0.02328483 0.358023704 ## [9,] -0.128457735 0.26796431 -0.52277710 -0.32283288 -0.13005779 0.001572649 ## [10,] -0.065686377 -0.01201495 0.09504456 0.01527331 -0.97011829 0.145935408 ## [11,] -0.430501041 0.10171830 0.16835005 -0.14893939 0.05661020 0.028760422 ## [12,] -0.278546553 -0.37514985 0.14985400 -0.34574836 -0.01358671 -0.154763215 ## [13,] -0.478712405 0.07283481 0.21649677 -0.08389812 0.03767846 0.020892771 ## [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 0.66589654 -0.33225521 -0.22462438 0.007874262 0.18916547 0.004949852 ## [2,] -0.20788462 -0.18096215 0.04333602 -0.568040361 0.12566307 0.242746705 ## [3,] 0.12857096 -0.02143816 -0.36226254 -0.317860485 -0.37815324 -0.340652073 ## [4,] 0.01886806 0.04489722 -0.30907596 0.143600911 -0.42151255 0.646668513 ## [5,] 0.13799303 0.24702527 0.12615240 -0.132664225 -0.08789281 0.064371162 ## [6,] -0.14409748 0.51451471 0.07780447 -0.095834270 0.07938639 -0.183829669 ## [7,] -0.09050524 -0.06131073 -0.12450579 0.663204133 -0.14586925 -0.308660927 ## [8,] 0.62750296 0.48310471 0.13005088 -0.074305149 -0.10264536 -0.009554218 ## [9,] 0.10712744 0.07358489 0.29554570 0.264416777 0.46117870 0.360900790 ## [10,] -0.10467064 -0.04524136 -0.02892652 -0.056036609 -0.08198456 -0.003969278 ## [11,] 0.13766958 -0.40015006 0.66839914 -0.055764820 -0.26409100 -0.225868516 ## [12,] 0.09410061 -0.28163148 -0.30107123 -0.053262879 0.37941812 -0.067534206 ## [13,] -0.06694700 0.21116321 -0.20232546 -0.066810707 0.38574841 -0.296113322 ## [,13] ## [1,] 0.003255188 ## [2,] -0.024896668 ## [3,] 0.007706804 ## [4,] -0.014147290 ## [5,] -0.012734529 ## [6,] 0.575848184 ## [7,] 0.049860958 ## [8,] -0.017852976 ## [9,] -0.013022281 ## [10,] -0.011434967 ## [11,] 0.008058268 ## [12,] 0.534199397 ## [13,] -0.615473481 # EV = a measure of how much of varaince each factor explain. # According to this, we select 5 factors, equal to the no. of values # that is above 1. We defined the individual constructs based on the theory (Stage 1) and developed the overall measurement model making also a first exploration with EFA. The next stages (4 and 5) would be to assess the measurement model validity (Confirmatory Factor Analsysis). Lastly, stages 5 and 6 implies to specify the structural model (SEM) and to asssess the structural model validity. Hence, the next two main operational tasks are: Set up a confirmatory factor analysis to confirm the measurement model Given the measurement model has been examined and validated in the CFA analysis, set up a SEM model, to test the structural relationships between the four constructs identified and the customers´ likelihood to continue doing business with HBAT (X19-Satisfaction, X20-Likelihood of recommendation and X21-Likelihood of future purchase). variable.names(data) ## [1] &quot;id&quot; &quot;x1&quot; &quot;x2&quot; &quot;x3&quot; &quot;x4&quot; &quot;x5&quot; &quot;x6&quot; &quot;x7&quot; &quot;x8&quot; &quot;x9&quot; &quot;x10&quot; &quot;x11&quot; ## [13] &quot;x12&quot; &quot;x13&quot; &quot;x14&quot; &quot;x15&quot; &quot;x16&quot; &quot;x17&quot; &quot;x18&quot; &quot;x19&quot; &quot;x20&quot; &quot;x21&quot; &quot;x22&quot; &quot;x23&quot; VariableLabels &lt;- unname(attr(data, &quot;variable.labels&quot;)) # data.label.table &lt;- attr(sav, &quot;label.table&quot;) # if you load it with read_sav() summary(data) ## id x1 x2 ## Min. : 1.00 Less than 1 year:32 Magazine industry :52 ## 1st Qu.: 25.75 1 to 5 years :35 Newsprint industry:48 ## Median : 50.50 Over 5 years :33 ## Mean : 50.50 ## 3rd Qu.: 75.25 ## Max. :100.00 ## x3 x4 x5 ## Small (0 to 499):49 USA/North America :39 Indirect through broker:57 ## Large (500+) :51 Outside North America:61 Direct to customer :43 ## ## ## ## ## x6 x7 x8 x9 ## Min. : 5.000 Min. :2.200 Min. :1.300 Min. :2.600 ## 1st Qu.: 6.575 1st Qu.:3.275 1st Qu.:4.250 1st Qu.:4.600 ## Median : 8.000 Median :3.600 Median :5.400 Median :5.450 ## Mean : 7.810 Mean :3.672 Mean :5.365 Mean :5.442 ## 3rd Qu.: 9.100 3rd Qu.:3.925 3rd Qu.:6.625 3rd Qu.:6.325 ## Max. :10.000 Max. :5.700 Max. :8.500 Max. :7.800 ## x10 x11 x12 x13 ## Min. :1.900 Min. :2.300 Min. :2.900 Min. :3.700 ## 1st Qu.:3.175 1st Qu.:4.700 1st Qu.:4.500 1st Qu.:5.875 ## Median :4.000 Median :5.750 Median :4.900 Median :7.100 ## Mean :4.010 Mean :5.805 Mean :5.123 Mean :6.974 ## 3rd Qu.:4.800 3rd Qu.:6.800 3rd Qu.:5.800 3rd Qu.:8.400 ## Max. :6.500 Max. :8.400 Max. :8.200 Max. :9.900 ## x14 x15 x16 x17 x18 ## Min. :4.100 Min. :1.70 Min. :2.000 Min. :2.60 Min. :1.600 ## 1st Qu.:5.400 1st Qu.:4.10 1st Qu.:3.700 1st Qu.:3.70 1st Qu.:3.400 ## Median :6.100 Median :5.00 Median :4.400 Median :4.35 Median :3.900 ## Mean :6.043 Mean :5.15 Mean :4.278 Mean :4.61 Mean :3.886 ## 3rd Qu.:6.600 3rd Qu.:6.30 3rd Qu.:4.800 3rd Qu.:5.60 3rd Qu.:4.425 ## Max. :8.100 Max. :9.50 Max. :6.700 Max. :7.30 Max. :5.500 ## x19 x20 x21 x22 ## Min. :4.700 Min. :4.60 Min. :5.500 Min. :37.10 ## 1st Qu.:6.000 1st Qu.:6.30 1st Qu.:7.100 1st Qu.:51.10 ## Median :7.050 Median :7.00 Median :7.700 Median :58.60 ## Mean :6.918 Mean :7.02 Mean :7.713 Mean :58.40 ## 3rd Qu.:7.625 3rd Qu.:7.60 3rd Qu.:8.400 3rd Qu.:65.35 ## Max. :9.900 Max. :9.90 Max. :9.900 Max. :77.10 ## x23 ## No, would not consider:55 ## Yes, would consider :45 ## ## ## ## 2.3.2.2 CFA model We use CFA to iterate through different models. 2.3.2.2.1 Initial model CS = Customer Service PV = Product value MK = Marketing TS = Technical support CFA.model &lt;- &#39;CS =~ x18 + x9 + x16 PV =~ x11 + x6 + x13 MK =~ x12 + x7 + x10 TS =~ x8 + x14 # Correlations between exogeneous constructs are optional because # by default, all exogenous latent variables in a CFA model are allowed to correlate CS ~~ PV CS ~~ MK CS ~~ TS PV ~~ MK PV ~~ TS MK ~~ TS&#39; # fit the model fit &lt;- cfa(CFA.model, data = data) # display summary output summary(fit , fit.measures=TRUE , standardized = TRUE , modindices = FALSE) #If TRUE, then we would get modificantion index ## lavaan 0.6-7 ended normally after 53 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 28 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 61.835 ## Degrees of freedom 38 ## P-value (Chi-square) 0.009 ## ## Model Test Baseline Model: ## ## Test statistic 655.315 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.960 ## Tucker-Lewis Index (TLI) 0.943 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1349.559 ## Loglikelihood unrestricted model (H1) -1318.642 ## ## Akaike (AIC) 2755.119 ## Bayesian (BIC) 2828.064 ## Sample-size adjusted Bayesian (BIC) 2739.632 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.079 ## 90 Percent confidence interval - lower 0.040 ## 90 Percent confidence interval - upper 0.114 ## P-value RMSEA &lt;= 0.05 0.097 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.087 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS =~ ## x18 1.000 0.703 0.961 ## x9 1.549 0.099 15.657 0.000 1.088 0.905 ## x16 1.035 0.092 11.198 0.000 0.727 0.787 ## PV =~ ## x11 1.000 1.547 1.182 ## x6 0.360 0.103 3.515 0.000 0.558 0.401 ## x13 -0.385 0.112 -3.444 0.001 -0.596 -0.388 ## MK =~ ## x12 1.000 1.071 1.003 ## x7 0.514 0.064 8.009 0.000 0.550 0.789 ## x10 0.566 0.104 5.434 0.000 0.605 0.540 ## TS =~ ## x8 1.000 1.014 0.666 ## x14 0.962 0.391 2.461 0.014 0.976 1.197 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS ~~ ## PV 0.608 0.113 5.406 0.000 0.560 0.560 ## MK 0.204 0.080 2.558 0.011 0.271 0.271 ## TS 0.089 0.075 1.190 0.234 0.125 0.125 ## PV ~~ ## MK 0.006 0.136 0.042 0.967 0.003 0.003 ## TS 0.265 0.176 1.506 0.132 0.169 0.169 ## MK ~~ ## TS 0.120 0.106 1.126 0.260 0.110 0.110 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x18 0.040 0.018 2.242 0.025 0.040 0.076 ## .x9 0.261 0.056 4.675 0.000 0.261 0.181 ## .x16 0.325 0.051 6.396 0.000 0.325 0.381 ## .x11 -0.681 0.427 -1.596 0.111 -0.681 -0.398 ## .x6 1.619 0.229 7.065 0.000 1.619 0.839 ## .x13 2.008 0.283 7.096 0.000 2.008 0.850 ## .x12 -0.008 0.112 -0.070 0.944 -0.008 -0.007 ## .x7 0.183 0.039 4.665 0.000 0.183 0.378 ## .x10 0.891 0.131 6.806 0.000 0.891 0.708 ## .x8 1.290 0.440 2.929 0.003 1.290 0.556 ## .x14 -0.288 0.373 -0.770 0.441 -0.288 -0.432 ## CS 0.494 0.077 6.392 0.000 1.000 1.000 ## PV 2.394 0.472 5.076 0.000 1.000 1.000 ## MK 1.146 0.196 5.843 0.000 1.000 1.000 ## TS 1.029 0.485 2.122 0.034 1.000 1.000 # Check the model fit (see slides for references). # NOTE: we get &quot;lavaan WARNING: some estimated ov variances are negative&quot;. # This is called in the literature &quot;Heywood case&quot;. Heywood cases or negative variance estimates, are a common occurrence in factor analysis and latent variable structural equation models. # There are several potential causes (https://journals.sagepub.com/doi/10.1177/0049124112442138). Here,eliminating the problematic item x11, will solve the problem. We see that we want to maximize TLI and CFI towards 1. Atm it looks well. We want the RMSEA (Root Mean Square Error of Approximation) to be below the 5% level. This we obtain in this example with pointe estimate of 0.079 where the 5% level is at 0.097, hence we are below. She writes in the slides (54) that we just need to be below 8%, this we also meet. she also writes that above 10% is a poor fit. We see that we get a warning, that is because there are negative variances, this is because of variable 11, that we ended up leaving in the model. Now we can ask for the modification indexes. # Ask for the modification indiceslin modificationindices(fit ,sort = T ,minimum.value = 10 ,op = &quot;~~&quot;) lhs op rhs mi epc sepc.lv sepc.all sepc.nox 68 x18 ~~ x11 15.70319 0.1283451 0.1283451 0.7733247 0.7733247 69 x18 ~~ x6 11.22736 -0.1347114 -0.1347114 -0.5264974 -0.5264974 85 x16 ~~ x11 10.73299 -0.1541396 -0.1541396 -0.3274085 -0.3274085 # MI reveal that x11 is correlated with x16 and x18; it means that x11 has substantial cross-loading on two factors (we also found this in EFA). Cross-loading goes against one of the principles of unidimensionality in SEM. We delete x11 from the analysis and re-run CFA. This confirms that we must do something with x11 (it should have been done in the exploratory assessment, but it was kept to show it) 2.3.2.2.2 Second model Model without x11 Notice that the EFA already suggested this, but we wanted to explore how it affected the overall model. And we end up seeing the consensus that we need to do something with V11. # CFA model after deleting x11 set.seed(1234) CFA.model &lt;- &#39;CS =~ x18 + x9 + x16 PV =~ x6 + x13 MK =~ x12 + x7 + x10 TS =~ x8 + x14&#39; # fit the model fit &lt;- cfa(CFA.model, data = data) # display summary output summary(fit, fit.measures=TRUE, standardized = TRUE, modindices = FALSE) ## lavaan 0.6-7 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 26 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 18.183 ## Degrees of freedom 29 ## P-value (Chi-square) 0.940 ## ## Model Test Baseline Model: ## ## Test statistic 530.377 ## Degrees of freedom 45 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.035 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1221.406 ## Loglikelihood unrestricted model (H1) -1212.314 ## ## Akaike (AIC) 2494.812 ## Bayesian (BIC) 2562.546 ## Sample-size adjusted Bayesian (BIC) 2480.432 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.016 ## P-value RMSEA &lt;= 0.05 0.989 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.039 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS =~ ## x18 1.000 0.677 0.926 ## x9 1.658 0.113 14.664 0.000 1.123 0.934 ## x16 1.107 0.098 11.294 0.000 0.749 0.811 ## PV =~ ## x6 1.000 0.625 0.450 ## x13 -2.197 0.962 -2.284 0.022 -1.372 -0.893 ## MK =~ ## x12 1.000 1.066 0.999 ## x7 0.518 0.063 8.249 0.000 0.552 0.792 ## x10 0.571 0.103 5.531 0.000 0.609 0.543 ## TS =~ ## x8 1.000 1.280 0.841 ## x14 0.604 0.135 4.467 0.000 0.774 0.948 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS ~~ ## PV 0.055 0.054 1.022 0.307 0.131 0.131 ## MK 0.191 0.077 2.471 0.013 0.265 0.265 ## TS 0.123 0.097 1.272 0.203 0.142 0.142 ## PV ~~ ## MK -0.199 0.112 -1.774 0.076 -0.299 -0.299 ## TS 0.239 0.144 1.665 0.096 0.299 0.299 ## MK ~~ ## TS 0.127 0.145 0.880 0.379 0.093 0.093 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x18 0.076 0.022 3.434 0.001 0.076 0.142 ## .x9 0.185 0.059 3.137 0.002 0.185 0.128 ## .x16 0.292 0.048 6.108 0.000 0.292 0.342 ## .x6 1.540 0.268 5.750 0.000 1.540 0.798 ## .x13 0.480 0.756 0.635 0.525 0.480 0.203 ## .x12 0.003 0.106 0.026 0.979 0.003 0.002 ## .x7 0.181 0.038 4.714 0.000 0.181 0.372 ## .x10 0.887 0.130 6.813 0.000 0.887 0.705 ## .x8 0.681 0.362 1.882 0.060 0.681 0.294 ## .x14 0.067 0.128 0.523 0.601 0.067 0.100 ## CS 0.458 0.077 5.936 0.000 1.000 1.000 ## PV 0.390 0.227 1.721 0.085 1.000 1.000 ## MK 1.136 0.193 5.886 0.000 1.000 1.000 ## TS 1.638 0.469 3.494 0.000 1.000 1.000 2.3.2.2.2.1 Intepretation of the model 2.3.2.2.2.1.1 1.) Examine the MODEL FIT a much better fit than we obtained before A decent model requires: CFI &gt;.90, TLI&gt;.90, RMSEA&lt; 0.08, SRMR &lt;.0.08. Check these indexes of model fit in your summary. 2.3.2.2.2.1.2 2). Examine the LOADINGS significance, size and sign The (std.) loadings should be at least +-.40. It is desirable to have high and significant loadings - it reflects items convergent validity. In one factor 2, competitive pricing (x13) and product quality (x6) have opposite signs. It means that the product quality and competitive pricing vary together, but move in direction opposite to each other. Perceptions are more positive whether product quality increases or price decreases. This trade-off leads to naming the factor product value. When variables have different signs, we need to be careful to reverse one when creating summated scales or using further in SEM analysis. Reverse scoring is the process by which the data values for a variable are reversed so that its correlation with the other variables are reversed (go from negative to positive). The purpose of reversing is to prevent a canceling out of variables with positive and negative loading. Reverse coding is typically required if we have some negatively phrased statement items in our questionnaire. For a categorical variable e.g. 1-5, the reversing implies: 1-&gt;5, 2-&gt;4, 3-&gt;2, 4-&gt;1, 5-&gt;1 For a continous variable: newvar = oldvar * (-1) summary(data$x13) str(data$x13) # reversing X13 : library(dplyr) library(tidyr) data &lt;- data %&gt;% mutate(x13r = x13 * (-1)) str(data$x13r) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.700 5.875 7.100 6.974 8.400 9.900 ## num [1:100] 6.8 5.3 4.5 8.8 6.8 8.5 8.9 6.9 9.3 8.4 ... ## num [1:100] -6.8 -5.3 -4.5 -8.8 -6.8 -8.5 -8.9 -6.9 -9.3 -8.4 ... 2.3.2.2.2.1.3 3). Examine RELIABILITY of the factors Reliability = assessment of degree of consistency between multiple measurements of a variable (back to the slides to refer to this concept). library(semTools) semTools::reliability(fit) ## CS PV MK TS ## alpha 0.8971043 -1.3290821 0.7829833 0.7977744 ## omega 0.9214780 0.2168081 0.8224869 0.8494229 ## omega2 0.9214780 0.2168081 0.8224869 0.8494229 ## omega3 0.9215495 0.2168081 0.8225741 0.8494228 ## avevar 0.8046227 0.5294845 0.6286031 0.7495027 # alpha = coefficient alpha (Cronbach, 1951) - should be &gt; than 0.5 or 0.6 (some textbooks) # omega = is similar to composite reliability index (CR) (Fornell &amp; Larcker (1981) - should be &gt; 0.7 # avevar = average variance extracted (AVE) (Fornell &amp; Larcker (1981)) - should be &gt; than 0.5. # For PV factor, reliability was calculed with the non-reversed item; let´s change that: We want the avevar to be above 0.60. The omega is the composite reliability, it should be higher than 0.7. We want the alpha to be close to 1. We see that PV is not meeting the two above. that is because we have a negative variable. Now we are going to reverse x13 to make the construct PV meet the requirements (or at least get closer to) set.seed(1234) CFA.model &lt;- &#39; # Measurement model CS =~ x18 + x9 + x16 PV =~ x6 + x13r MK =~ x12 + x7 + x10 TS =~ x8 + x14&#39; fit &lt;- cfa(CFA.model, data = data) summary(fit, fit.measures=TRUE, standardized = TRUE, modindices = FALSE) ## lavaan 0.6-7 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 26 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 18.183 ## Degrees of freedom 29 ## P-value (Chi-square) 0.940 ## ## Model Test Baseline Model: ## ## Test statistic 530.377 ## Degrees of freedom 45 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.035 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1221.406 ## Loglikelihood unrestricted model (H1) -1212.314 ## ## Akaike (AIC) 2494.812 ## Bayesian (BIC) 2562.546 ## Sample-size adjusted Bayesian (BIC) 2480.432 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.016 ## P-value RMSEA &lt;= 0.05 0.989 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.039 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS =~ ## x18 1.000 0.677 0.926 ## x9 1.658 0.113 14.664 0.000 1.123 0.934 ## x16 1.107 0.098 11.294 0.000 0.749 0.811 ## PV =~ ## x6 1.000 0.625 0.450 ## x13r 2.197 0.962 2.284 0.022 1.372 0.893 ## MK =~ ## x12 1.000 1.066 0.999 ## x7 0.518 0.063 8.249 0.000 0.552 0.792 ## x10 0.571 0.103 5.531 0.000 0.609 0.543 ## TS =~ ## x8 1.000 1.280 0.841 ## x14 0.604 0.135 4.467 0.000 0.774 0.948 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS ~~ ## PV 0.055 0.054 1.022 0.307 0.131 0.131 ## MK 0.191 0.077 2.471 0.013 0.265 0.265 ## TS 0.123 0.097 1.272 0.203 0.142 0.142 ## PV ~~ ## MK -0.199 0.112 -1.774 0.076 -0.299 -0.299 ## TS 0.239 0.144 1.665 0.096 0.299 0.299 ## MK ~~ ## TS 0.127 0.145 0.880 0.379 0.093 0.093 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x18 0.076 0.022 3.434 0.001 0.076 0.142 ## .x9 0.185 0.059 3.137 0.002 0.185 0.128 ## .x16 0.292 0.048 6.108 0.000 0.292 0.342 ## .x6 1.540 0.268 5.750 0.000 1.540 0.798 ## .x13r 0.480 0.756 0.635 0.525 0.480 0.203 ## .x12 0.003 0.106 0.026 0.979 0.003 0.002 ## .x7 0.181 0.038 4.714 0.000 0.181 0.372 ## .x10 0.887 0.130 6.813 0.000 0.887 0.705 ## .x8 0.681 0.362 1.882 0.060 0.681 0.294 ## .x14 0.067 0.128 0.523 0.601 0.067 0.100 ## CS 0.458 0.077 5.936 0.000 1.000 1.000 ## PV 0.390 0.227 1.721 0.085 1.000 1.000 ## MK 1.136 0.193 5.886 0.000 1.000 1.000 ## TS 1.638 0.469 3.494 0.000 1.000 1.000 semTools::reliability(fit) ## CS PV MK TS ## alpha 0.8971043 0.5706463 0.7829833 0.7977744 ## omega 0.9214780 0.6637334 0.8224869 0.8494229 ## omega2 0.9214780 0.6637334 0.8224869 0.8494229 ## omega3 0.9215495 0.6637332 0.8225741 0.8494228 ## avevar 0.8046227 0.5294845 0.6286031 0.7495027 given alpha, omega and avevar values overpass the recommended values, one can conclude that all factors display good reliability. Although it does not appear as if the PV avevar does not meet the 60% goal. With x13 reversed, we see that the model is far better. Although it does not entirely meet the requirements. 2.3.2.2.2.1.4 4). Examine DISCRIMINANT VALIDITY of the factors each pair of latent correlations (correlations between the principal components) should be sufficiently below 1 (in absolute value), that the latent variables can be thought of representing two distinct contructs. (to recall, back to the slides to refer to this concept). discriminantValidity(fit, merge=TRUE) lhs op rhs est ci.lower ci.upper Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) CS ~~ PV 0.1305940 -0.0961131 0.3573012 32 2522.142 2582.061 51.51374 33.33044 3 3.00e-07 CS ~~ MK 0.2650079 0.0744574 0.4555583 32 2623.696 2683.615 153.06726 134.88395 3 0.00e+00 CS ~~ TS 0.1420574 -0.0659618 0.3500766 32 2596.806 2656.725 126.17795 107.99464 3 0.00e+00 PV ~~ MK -0.2993652 -0.5271069 -0.0716235 32 2514.275 2574.194 43.64625 25.46294 3 1.24e-05 PV ~~ TS 0.2992056 0.0620862 0.5363251 32 2517.564 2577.483 46.93603 28.75273 3 2.50e-06 MK ~~ TS 0.0934218 -0.1100485 0.2968921 32 2598.582 2658.501 127.95325 109.76994 3 0.00e+00 Output: The first set are factor correlation estimates and their confidence intervals. Are these correlations sufficiently low to claim discriminant validity of the four constructs? Based on Fornell &amp; Larcker (1981), the square root of each construct´s AVE should have a greater value than the inter-constructs corelations (alternatitvely, AVE &gt; corr^2). Let us check that: reliability_out = reliability (fit) AVEs = reliability_out[5,] sqrtAVEs = sqrt(AVEs) sqrtAVEs ## CS PV MK TS ## 0.8970076 0.7276569 0.7928449 0.8657383 Comparing the inter-constructs correlations (see “est”\" column in the output of discriminantValidity(fit, merge=TRUE)) with the sqrtAVEs, we conclude that cf. Fornell &amp; Larcker (1981) criterion, the four constructss display significant discriminat validity. Now we can plot the CFA. notice that the arrows are not yet directed hence you have not yet imposed the structure that we are going to do in SEM. 2.3.2.3 SEM NOTE: Three variables were not included in the CFA (x11, x15, x17) as these did not meet the assumptions, reasoning both in the EFA and CFA for x11. Reason: These variables did not load high on any of the main constructs If they are important, they can be treated as separate explanatory variables in SEM last DV in our SEM model will be x19-Satisfaction. in other words, we build a model to explain x19 Notice that we are also continuing to use the reversed version of x13r. SEM.model1 &lt;- &#39; # Measurement model CS =~ x18 + x9 + x16 PV =~ x6 + x13r MK =~ x12 + x7 + x10 TS =~ x8 + x14 # Structural model x19 ~ CS + PV + MK + TS&#39; Now we can fit the model fitSEM1 &lt;- sem(SEM.model1 ,data=data ,se=&quot;robust&quot; ,estimator = &quot;ML&quot;) #Maximum Likelihood, that is the standard summary(fitSEM1, fit.measures=TRUE, standardized = TRUE, rsquare=TRUE) ## lavaan 0.6-7 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 31 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 32.225 ## Degrees of freedom 35 ## P-value (Chi-square) 0.603 ## ## Model Test Baseline Model: ## ## Test statistic 688.965 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.007 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1308.074 ## Loglikelihood unrestricted model (H1) -1291.961 ## ## Akaike (AIC) 2678.148 ## Bayesian (BIC) 2758.908 ## Sample-size adjusted Bayesian (BIC) 2661.002 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.064 ## P-value RMSEA &lt;= 0.05 0.877 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.064 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS =~ ## x18 1.000 0.678 0.928 ## x9 1.652 0.127 12.995 0.000 1.120 0.932 ## x16 1.106 0.102 10.858 0.000 0.750 0.812 ## PV =~ ## x6 1.000 1.126 0.810 ## x13r 0.676 0.125 5.422 0.000 0.761 0.495 ## MK =~ ## x12 1.000 1.150 1.078 ## x7 0.447 0.048 9.309 0.000 0.514 0.737 ## x10 0.466 0.063 7.340 0.000 0.535 0.478 ## TS =~ ## x8 1.000 1.057 0.694 ## x14 0.886 0.452 1.960 0.050 0.936 1.148 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## x19 ~ ## CS 0.787 0.116 6.758 0.000 0.534 0.450 ## PV 0.663 0.110 6.022 0.000 0.746 0.629 ## MK 0.518 0.078 6.671 0.000 0.595 0.502 ## TS -0.039 0.045 -0.870 0.384 -0.042 -0.035 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS ~~ ## PV 0.090 0.099 0.912 0.362 0.118 0.118 ## MK 0.181 0.083 2.194 0.028 0.232 0.232 ## TS 0.099 0.091 1.088 0.277 0.138 0.138 ## PV ~~ ## MK -0.269 0.146 -1.838 0.066 -0.208 -0.208 ## TS 0.131 0.142 0.920 0.357 0.110 0.110 ## MK ~~ ## TS 0.137 0.131 1.046 0.296 0.112 0.112 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x18 0.074 0.017 4.328 0.000 0.074 0.139 ## .x9 0.191 0.045 4.274 0.000 0.191 0.132 ## .x16 0.291 0.041 7.080 0.000 0.291 0.341 ## .x6 0.663 0.203 3.261 0.001 0.663 0.343 ## .x13r 1.784 0.206 8.662 0.000 1.784 0.755 ## .x12 -0.184 0.085 -2.161 0.031 -0.184 -0.162 ## .x7 0.222 0.037 6.005 0.000 0.222 0.456 ## .x10 0.971 0.113 8.580 0.000 0.971 0.772 ## .x8 1.201 0.576 2.083 0.037 1.201 0.518 ## .x14 -0.212 0.424 -0.499 0.618 -0.212 -0.318 ## .x19 0.170 0.087 1.963 0.050 0.170 0.121 ## CS 0.460 0.077 5.942 0.000 1.000 1.000 ## PV 1.267 0.261 4.861 0.000 1.000 1.000 ## MK 1.322 0.186 7.119 0.000 1.000 1.000 ## TS 1.118 0.645 1.734 0.083 1.000 1.000 ## ## R-Square: ## Estimate ## x18 0.861 ## x9 0.868 ## x16 0.659 ## x6 0.657 ## x13r 0.245 ## x12 NA ## x7 0.544 ## x10 0.228 ## x8 0.482 ## x14 NA ## x19 0.879 If the message “lavaan WARNING: some estimated ov variances are negative” shows up. In this case, the problematic items are x12 and x14. It reflects that we would need more quality data and more items per construct to run this model. We set se=“robust” to produce robust standard errors; setting se=“boot” or se=“bootstrap” will produce bootstrap standard errors. Now we can check the information critera / indexes as we did in the CFA. The following is a summary of all important criteria. fitmeasures(fit) # alternative summary ## npar fmin chisq df ## 26.000 0.091 18.183 29.000 ## pvalue baseline.chisq baseline.df baseline.pvalue ## 0.940 530.377 45.000 0.000 ## cfi tli nnfi rfi ## 1.000 1.035 1.035 0.947 ## nfi pnfi ifi rni ## 0.966 0.622 1.022 1.022 ## logl unrestricted.logl aic bic ## -1221.406 -1212.314 2494.812 2562.546 ## ntotal bic2 rmsea rmsea.ci.lower ## 100.000 2480.432 0.000 0.000 ## rmsea.ci.upper rmsea.pvalue rmr rmr_nomean ## 0.016 0.989 0.048 0.048 ## srmr srmr_bentler srmr_bentler_nomean crmr ## 0.039 0.039 0.039 0.043 ## crmr_nomean srmr_mplus srmr_mplus_nomean cn_05 ## 0.043 0.039 0.039 235.044 ## cn_01 gfi agfi pgfi ## 273.711 0.966 0.935 0.509 ## mfi ecvi ## 1.056 0.702 Next, check the structural coeficients in summary(). Output partially reproduced below: Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all x19 ~ CS 0.787 0.116 6.758 0.000 0.534 0.450 PV 0.663 0.110 6.022 0.000 0.746 0.629 MK 0.518 0.078 6.671 0.000 0.595 0.502 TS -0.039 0.045 -0.870 0.384 -0.042 -0.035 Concl.: Customers perceptions about CS, PV and MK are positively and significantly correlated with satisfaction. TS (Technical Service) perceptions is not significantly related to customer satisfaction. check modification indices if relevant summary(fitSEM1, fit.measures=TRUE, standardized = TRUE, rsquare=TRUE, modindices=TRUE) modificationindices(fitSEM1, sort = T, minimum.value = 10, op = &quot;~~&quot;) ## lavaan 0.6-7 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 31 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 32.225 ## Degrees of freedom 35 ## P-value (Chi-square) 0.603 ## ## Model Test Baseline Model: ## ## Test statistic 688.965 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.007 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1308.074 ## Loglikelihood unrestricted model (H1) -1291.961 ## ## Akaike (AIC) 2678.148 ## Bayesian (BIC) 2758.908 ## Sample-size adjusted Bayesian (BIC) 2661.002 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.064 ## P-value RMSEA &lt;= 0.05 0.877 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.064 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS =~ ## x18 1.000 0.678 0.928 ## x9 1.652 0.127 12.995 0.000 1.120 0.932 ## x16 1.106 0.102 10.858 0.000 0.750 0.812 ## PV =~ ## x6 1.000 1.126 0.810 ## x13r 0.676 0.125 5.422 0.000 0.761 0.495 ## MK =~ ## x12 1.000 1.150 1.078 ## x7 0.447 0.048 9.309 0.000 0.514 0.737 ## x10 0.466 0.063 7.340 0.000 0.535 0.478 ## TS =~ ## x8 1.000 1.057 0.694 ## x14 0.886 0.452 1.960 0.050 0.936 1.148 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## x19 ~ ## CS 0.787 0.116 6.758 0.000 0.534 0.450 ## PV 0.663 0.110 6.022 0.000 0.746 0.629 ## MK 0.518 0.078 6.671 0.000 0.595 0.502 ## TS -0.039 0.045 -0.870 0.384 -0.042 -0.035 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS ~~ ## PV 0.090 0.099 0.912 0.362 0.118 0.118 ## MK 0.181 0.083 2.194 0.028 0.232 0.232 ## TS 0.099 0.091 1.088 0.277 0.138 0.138 ## PV ~~ ## MK -0.269 0.146 -1.838 0.066 -0.208 -0.208 ## TS 0.131 0.142 0.920 0.357 0.110 0.110 ## MK ~~ ## TS 0.137 0.131 1.046 0.296 0.112 0.112 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x18 0.074 0.017 4.328 0.000 0.074 0.139 ## .x9 0.191 0.045 4.274 0.000 0.191 0.132 ## .x16 0.291 0.041 7.080 0.000 0.291 0.341 ## .x6 0.663 0.203 3.261 0.001 0.663 0.343 ## .x13r 1.784 0.206 8.662 0.000 1.784 0.755 ## .x12 -0.184 0.085 -2.161 0.031 -0.184 -0.162 ## .x7 0.222 0.037 6.005 0.000 0.222 0.456 ## .x10 0.971 0.113 8.580 0.000 0.971 0.772 ## .x8 1.201 0.576 2.083 0.037 1.201 0.518 ## .x14 -0.212 0.424 -0.499 0.618 -0.212 -0.318 ## .x19 0.170 0.087 1.963 0.050 0.170 0.121 ## CS 0.460 0.077 5.942 0.000 1.000 1.000 ## PV 1.267 0.261 4.861 0.000 1.000 1.000 ## MK 1.322 0.186 7.119 0.000 1.000 1.000 ## TS 1.118 0.645 1.734 0.083 1.000 1.000 ## ## R-Square: ## Estimate ## x18 0.861 ## x9 0.868 ## x16 0.659 ## x6 0.657 ## x13r 0.245 ## x12 NA ## x7 0.544 ## x10 0.228 ## x8 0.482 ## x14 NA ## x19 0.879 ## ## Modification Indices: ## ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 36 CS =~ x6 0.356 -0.197 -0.133 -0.096 -0.096 ## 37 CS =~ x13r 0.356 0.133 0.090 0.059 0.059 ## 38 CS =~ x12 0.455 -0.092 -0.062 -0.058 -0.058 ## 39 CS =~ x7 0.001 0.002 0.001 0.002 0.002 ## 40 CS =~ x10 2.401 0.234 0.159 0.142 0.142 ## 41 CS =~ x8 0.158 -0.079 -0.054 -0.035 -0.035 ## 42 CS =~ x14 0.158 0.070 0.048 0.058 0.058 ## 43 PV =~ x18 2.442 -0.055 -0.062 -0.085 -0.085 ## 44 PV =~ x9 1.008 0.058 0.066 0.055 0.055 ## 45 PV =~ x16 0.612 0.043 0.049 0.053 0.053 ## 46 PV =~ x12 0.806 0.077 0.087 0.081 0.081 ## 47 PV =~ x7 1.280 -0.049 -0.055 -0.079 -0.079 ## 48 PV =~ x10 0.058 0.023 0.026 0.023 0.023 ## 49 PV =~ x8 1.002 0.095 0.107 0.070 0.070 ## 50 PV =~ x14 1.002 -0.084 -0.095 -0.116 -0.116 ## 51 MK =~ x18 0.376 0.017 0.020 0.027 0.027 ## 52 MK =~ x9 0.064 -0.012 -0.014 -0.011 -0.011 ## 53 MK =~ x16 0.274 -0.024 -0.028 -0.030 -0.030 ## 54 MK =~ x6 3.377 0.329 0.378 0.272 0.272 ## 55 MK =~ x13r 3.377 -0.222 -0.256 -0.166 -0.166 ## 56 MK =~ x8 0.708 -0.083 -0.095 -0.063 -0.063 ## 57 MK =~ x14 0.708 0.073 0.084 0.104 0.104 ## 58 TS =~ x18 0.404 -0.018 -0.019 -0.026 -0.026 ## 59 TS =~ x9 0.291 -0.025 -0.026 -0.022 -0.022 ## 60 TS =~ x16 2.951 0.078 0.083 0.090 0.090 ## 61 TS =~ x6 2.902 -0.287 -0.304 -0.219 -0.219 ## 62 TS =~ x13r 2.902 0.194 0.206 0.134 0.134 ## 63 TS =~ x12 0.211 0.031 0.032 0.030 0.030 ## 64 TS =~ x7 0.159 -0.014 -0.014 -0.021 -0.021 ## 65 TS =~ x10 0.071 -0.020 -0.021 -0.019 -0.019 ## 66 x18 ~~ x9 0.036 0.011 0.011 0.091 0.091 ## 67 x18 ~~ x16 0.045 -0.006 -0.006 -0.044 -0.044 ## 68 x18 ~~ x6 1.441 -0.041 -0.041 -0.185 -0.185 ## 69 x18 ~~ x13r 0.029 -0.008 -0.008 -0.022 -0.022 ## 70 x18 ~~ x12 0.427 -0.011 -0.011 -0.092 -0.092 ## 71 x18 ~~ x7 0.559 0.011 0.011 0.083 0.083 ## 72 x18 ~~ x10 2.482 0.051 0.051 0.192 0.192 ## 73 x18 ~~ x8 1.139 -0.034 -0.034 -0.114 -0.114 ## 74 x18 ~~ x14 0.091 0.005 0.005 0.041 0.041 ## 75 x18 ~~ x19 0.051 0.005 0.005 0.046 0.046 ## 76 x9 ~~ x16 0.004 0.003 0.003 0.014 0.014 ## 77 x9 ~~ x6 0.481 0.039 0.039 0.109 0.109 ## 78 x9 ~~ x13r 0.630 0.062 0.062 0.106 0.106 ## 79 x9 ~~ x12 0.473 0.018 0.018 0.099 0.099 ## 80 x9 ~~ x7 0.507 -0.017 -0.017 -0.080 -0.080 ## 81 x9 ~~ x10 0.916 -0.051 -0.051 -0.119 -0.119 ## 82 x9 ~~ x8 2.939 0.089 0.089 0.186 0.186 ## 83 x9 ~~ x14 2.066 -0.040 -0.040 -0.201 -0.201 ## 84 x9 ~~ x19 0.058 -0.009 -0.009 -0.050 -0.050 ## 85 x16 ~~ x6 0.031 0.009 0.009 0.021 0.021 ## 86 x16 ~~ x13r 0.049 0.017 0.017 0.024 0.024 ## 87 x16 ~~ x12 0.560 -0.020 -0.020 -0.084 -0.084 ## 88 x16 ~~ x7 0.836 0.022 0.022 0.085 0.085 ## 89 x16 ~~ x10 0.004 0.003 0.003 0.006 0.006 ## 90 x16 ~~ x8 1.439 -0.063 -0.063 -0.107 -0.107 ## 91 x16 ~~ x14 3.275 0.051 0.051 0.203 0.203 ## 92 x16 ~~ x19 0.000 0.001 0.001 0.003 0.003 ## 94 x6 ~~ x12 0.161 0.039 0.039 0.111 0.111 ## 95 x6 ~~ x7 0.256 0.026 0.026 0.067 0.067 ## 96 x6 ~~ x10 0.046 -0.021 -0.021 -0.026 -0.026 ## 97 x6 ~~ x8 0.021 0.014 0.014 0.016 0.016 ## 98 x6 ~~ x14 0.865 -0.067 -0.067 -0.179 -0.179 ## 99 x6 ~~ x19 7.958 1.408 1.408 4.191 4.191 ## 100 x13r ~~ x12 0.784 -0.081 -0.081 -0.141 -0.141 ## 101 x13r ~~ x7 0.004 -0.004 -0.004 -0.006 -0.006 ## 102 x13r ~~ x10 0.080 -0.037 -0.037 -0.028 -0.028 ## 103 x13r ~~ x8 1.321 0.146 0.146 0.100 0.100 ## 104 x13r ~~ x14 0.197 0.031 0.031 0.050 0.050 ## 105 x13r ~~ x19 7.958 -0.952 -0.952 -1.728 -1.728 ## 106 x12 ~~ x7 0.494 0.082 0.082 0.405 0.405 ## 107 x12 ~~ x10 2.507 -0.136 -0.136 -0.322 -0.322 ## 108 x12 ~~ x8 0.019 -0.007 -0.007 -0.014 -0.014 ## 109 x12 ~~ x14 0.125 0.012 0.012 0.059 0.059 ## 110 x12 ~~ x19 2.831 0.158 0.158 0.894 0.894 ## 111 x7 ~~ x10 1.855 0.066 0.066 0.142 0.142 ## 112 x7 ~~ x8 0.029 0.006 0.006 0.013 0.013 ## 113 x7 ~~ x14 0.114 -0.007 -0.007 -0.032 -0.032 ## 114 x7 ~~ x19 3.051 -0.066 -0.066 -0.341 -0.341 ## 115 x10 ~~ x8 0.446 -0.058 -0.058 -0.054 -0.054 ## 116 x10 ~~ x14 0.023 0.007 0.007 0.015 0.015 ## 117 x10 ~~ x19 0.124 0.021 0.021 0.052 0.052 ## 119 x8 ~~ x19 0.000 0.000 0.000 0.001 0.001 ## 120 x14 ~~ x19 0.000 0.000 0.000 -0.002 -0.002 lhs op rhs mi epc sepc.lv sepc.all sepc.nox First we see the summary, and we can assess for improvements, there does not appear to be any. no suggestion for improvement # If required, the bootstrap model parameters are available with: # PAR.boot &lt;- bootstrapLavaan(fitSEM1, R=10, type=&quot;ordinary&quot;,FUN=&quot;coef&quot;) # T.boot &lt;- bootstrapLavaan(fitSEM1, R=10, type=&quot;bollen.stine&quot;,FUN=fitMeasures, fit.measures=&quot;chisq&quot;) 2.3.3 Plotting the model and this is another plot This also show what variables that should be exlcuded. Note: to improve the model, exclude the items with the negative variance. hence the two items, x12 and x14 2.3.4 Consider a more complex SEM model involving a mediating effect. Consistent with the theory, Sem.model2 proposed x19 (Satisfaction) as mediator between the four latent constructs and Likelihood of future purchase (x21). SEM.model2 &lt;- &#39; # Measurement model CS =~ x18 + x9 + x16 PV =~ x6 + x13r MK =~ x12 + x7 + x10 TS =~ x8 + x14 # Structural model x19 ~ CS + PV + MK + TS x21 ~ x19&#39; # fit the model fitSEM2 &lt;- sem(SEM.model2, data=data, se=&quot;robust&quot;) summary(fitSEM2, fit.measures=TRUE) ## lavaan 0.6-7 ended normally after 55 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 33 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 45.798 ## Degrees of freedom 45 ## P-value (Chi-square) 0.439 ## ## Model Test Baseline Model: ## ## Test statistic 778.988 ## Degrees of freedom 66 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.999 ## Tucker-Lewis Index (TLI) 0.998 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1404.638 ## Loglikelihood unrestricted model (H1) -1381.738 ## ## Akaike (AIC) 2875.275 ## Bayesian (BIC) 2961.246 ## Sample-size adjusted Bayesian (BIC) 2857.023 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.013 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.068 ## P-value RMSEA &lt;= 0.05 0.814 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.065 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## CS =~ ## x18 1.000 ## x9 1.652 0.127 12.995 0.000 ## x16 1.106 0.102 10.858 0.000 ## PV =~ ## x6 1.000 ## x13r 0.676 0.125 5.422 0.000 ## MK =~ ## x12 1.000 ## x7 0.447 0.048 9.309 0.000 ## x10 0.466 0.063 7.340 0.000 ## TS =~ ## x8 1.000 ## x14 0.886 0.452 1.960 0.050 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## x19 ~ ## CS 0.787 0.116 6.758 0.000 ## PV 0.663 0.110 6.022 0.000 ## MK 0.518 0.078 6.671 0.000 ## TS -0.039 0.045 -0.870 0.384 ## x21 ~ ## x19 0.574 0.055 10.450 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## CS ~~ ## PV 0.090 0.099 0.912 0.362 ## MK 0.181 0.083 2.194 0.028 ## TS 0.099 0.091 1.088 0.277 ## PV ~~ ## MK -0.269 0.146 -1.838 0.066 ## TS 0.131 0.142 0.920 0.357 ## MK ~~ ## TS 0.137 0.131 1.046 0.296 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x18 0.074 0.017 4.328 0.000 ## .x9 0.191 0.045 4.274 0.000 ## .x16 0.291 0.041 7.080 0.000 ## .x6 0.663 0.203 3.261 0.001 ## .x13r 1.784 0.206 8.662 0.000 ## .x12 -0.184 0.085 -2.161 0.031 ## .x7 0.222 0.037 6.005 0.000 ## .x10 0.971 0.113 8.580 0.000 ## .x8 1.201 0.576 2.083 0.037 ## .x14 -0.212 0.424 -0.499 0.618 ## .x19 0.170 0.087 1.963 0.050 ## .x21 0.404 0.047 8.674 0.000 ## CS 0.460 0.077 5.942 0.000 ## PV 1.267 0.261 4.861 0.000 ## MK 1.322 0.186 7.119 0.000 ## TS 1.118 0.645 1.734 0.083 summary(fitSEM2, fit.measures=TRUE, standardized = TRUE, rsquare=TRUE, modindices=TRUE) modificationindices(fitSEM2, sort = T, minimum.value = 10, op = &quot;~~&quot;) ## lavaan 0.6-7 ended normally after 55 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 33 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 45.798 ## Degrees of freedom 45 ## P-value (Chi-square) 0.439 ## ## Model Test Baseline Model: ## ## Test statistic 778.988 ## Degrees of freedom 66 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.999 ## Tucker-Lewis Index (TLI) 0.998 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1404.638 ## Loglikelihood unrestricted model (H1) -1381.738 ## ## Akaike (AIC) 2875.275 ## Bayesian (BIC) 2961.246 ## Sample-size adjusted Bayesian (BIC) 2857.023 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.013 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.068 ## P-value RMSEA &lt;= 0.05 0.814 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.065 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS =~ ## x18 1.000 0.678 0.928 ## x9 1.652 0.127 12.995 0.000 1.120 0.932 ## x16 1.106 0.102 10.858 0.000 0.750 0.812 ## PV =~ ## x6 1.000 1.126 0.810 ## x13r 0.676 0.125 5.422 0.000 0.761 0.495 ## MK =~ ## x12 1.000 1.150 1.078 ## x7 0.447 0.048 9.309 0.000 0.514 0.737 ## x10 0.466 0.063 7.340 0.000 0.535 0.478 ## TS =~ ## x8 1.000 1.057 0.694 ## x14 0.886 0.452 1.960 0.050 0.936 1.148 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## x19 ~ ## CS 0.787 0.116 6.758 0.000 0.534 0.450 ## PV 0.663 0.110 6.022 0.000 0.746 0.629 ## MK 0.518 0.078 6.671 0.000 0.595 0.502 ## TS -0.039 0.045 -0.870 0.384 -0.042 -0.035 ## x21 ~ ## x19 0.574 0.055 10.450 0.000 0.574 0.731 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS ~~ ## PV 0.090 0.099 0.912 0.362 0.118 0.118 ## MK 0.181 0.083 2.194 0.028 0.232 0.232 ## TS 0.099 0.091 1.088 0.277 0.138 0.138 ## PV ~~ ## MK -0.269 0.146 -1.838 0.066 -0.208 -0.208 ## TS 0.131 0.142 0.920 0.357 0.110 0.110 ## MK ~~ ## TS 0.137 0.131 1.046 0.296 0.112 0.112 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x18 0.074 0.017 4.328 0.000 0.074 0.139 ## .x9 0.191 0.045 4.274 0.000 0.191 0.132 ## .x16 0.291 0.041 7.080 0.000 0.291 0.341 ## .x6 0.663 0.203 3.261 0.001 0.663 0.343 ## .x13r 1.784 0.206 8.662 0.000 1.784 0.755 ## .x12 -0.184 0.085 -2.161 0.031 -0.184 -0.162 ## .x7 0.222 0.037 6.005 0.000 0.222 0.456 ## .x10 0.971 0.113 8.580 0.000 0.971 0.772 ## .x8 1.201 0.576 2.083 0.037 1.201 0.518 ## .x14 -0.212 0.424 -0.499 0.618 -0.212 -0.318 ## .x19 0.170 0.087 1.963 0.050 0.170 0.121 ## .x21 0.404 0.047 8.674 0.000 0.404 0.466 ## CS 0.460 0.077 5.942 0.000 1.000 1.000 ## PV 1.267 0.261 4.861 0.000 1.000 1.000 ## MK 1.322 0.186 7.119 0.000 1.000 1.000 ## TS 1.118 0.645 1.734 0.083 1.000 1.000 ## ## R-Square: ## Estimate ## x18 0.861 ## x9 0.868 ## x16 0.659 ## x6 0.657 ## x13r 0.245 ## x12 NA ## x7 0.544 ## x10 0.228 ## x8 0.482 ## x14 NA ## x19 0.879 ## x21 0.534 ## ## Modification Indices: ## ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 38 CS =~ x6 0.356 -0.197 -0.133 -0.096 -0.096 ## 39 CS =~ x13r 0.356 0.133 0.090 0.059 0.059 ## 40 CS =~ x12 0.455 -0.092 -0.062 -0.058 -0.058 ## 41 CS =~ x7 0.001 0.002 0.001 0.002 0.002 ## 42 CS =~ x10 2.401 0.234 0.159 0.142 0.142 ## 43 CS =~ x8 0.158 -0.079 -0.054 -0.035 -0.035 ## 44 CS =~ x14 0.158 0.070 0.048 0.058 0.058 ## 45 PV =~ x18 2.442 -0.055 -0.062 -0.085 -0.085 ## 46 PV =~ x9 1.008 0.058 0.066 0.055 0.055 ## 47 PV =~ x16 0.612 0.043 0.049 0.053 0.053 ## 48 PV =~ x12 0.806 0.077 0.087 0.081 0.081 ## 49 PV =~ x7 1.280 -0.049 -0.055 -0.079 -0.079 ## 50 PV =~ x10 0.058 0.023 0.026 0.023 0.023 ## 51 PV =~ x8 1.002 0.095 0.107 0.070 0.070 ## 52 PV =~ x14 1.002 -0.084 -0.095 -0.116 -0.116 ## 53 MK =~ x18 0.376 0.017 0.020 0.027 0.027 ## 54 MK =~ x9 0.064 -0.012 -0.014 -0.011 -0.011 ## 55 MK =~ x16 0.274 -0.024 -0.028 -0.030 -0.030 ## 56 MK =~ x6 3.377 0.329 0.378 0.272 0.272 ## 57 MK =~ x13r 3.377 -0.222 -0.256 -0.166 -0.166 ## 58 MK =~ x8 0.708 -0.083 -0.095 -0.063 -0.063 ## 59 MK =~ x14 0.708 0.073 0.084 0.104 0.104 ## 60 TS =~ x18 0.404 -0.018 -0.019 -0.026 -0.026 ## 61 TS =~ x9 0.291 -0.025 -0.026 -0.022 -0.022 ## 62 TS =~ x16 2.951 0.078 0.083 0.090 0.090 ## 63 TS =~ x6 2.901 -0.287 -0.304 -0.219 -0.219 ## 64 TS =~ x13r 2.902 0.194 0.206 0.134 0.134 ## 65 TS =~ x12 0.211 0.031 0.032 0.030 0.030 ## 66 TS =~ x7 0.159 -0.014 -0.014 -0.021 -0.021 ## 67 TS =~ x10 0.071 -0.020 -0.021 -0.019 -0.019 ## 68 x18 ~~ x9 0.036 0.011 0.011 0.091 0.091 ## 69 x18 ~~ x16 0.045 -0.006 -0.006 -0.044 -0.044 ## 70 x18 ~~ x6 1.441 -0.041 -0.041 -0.185 -0.185 ## 71 x18 ~~ x13r 0.029 -0.008 -0.008 -0.022 -0.022 ## 72 x18 ~~ x12 0.427 -0.011 -0.011 -0.092 -0.092 ## 73 x18 ~~ x7 0.559 0.011 0.011 0.083 0.083 ## 74 x18 ~~ x10 2.482 0.051 0.051 0.192 0.192 ## 75 x18 ~~ x8 1.139 -0.034 -0.034 -0.114 -0.114 ## 76 x18 ~~ x14 0.091 0.005 0.005 0.041 0.041 ## 77 x18 ~~ x19 0.051 0.005 0.005 0.046 0.046 ## 78 x18 ~~ x21 0.001 0.000 0.000 -0.003 -0.003 ## 79 x9 ~~ x16 0.004 0.003 0.003 0.014 0.014 ## 80 x9 ~~ x6 0.481 0.039 0.039 0.109 0.109 ## 81 x9 ~~ x13r 0.630 0.062 0.062 0.106 0.106 ## 82 x9 ~~ x12 0.473 0.018 0.018 0.099 0.099 ## 83 x9 ~~ x7 0.507 -0.017 -0.017 -0.080 -0.080 ## 84 x9 ~~ x10 0.916 -0.051 -0.051 -0.119 -0.119 ## 85 x9 ~~ x8 2.939 0.089 0.089 0.186 0.186 ## 86 x9 ~~ x14 2.066 -0.040 -0.040 -0.201 -0.201 ## 87 x9 ~~ x19 0.058 -0.009 -0.009 -0.050 -0.050 ## 88 x9 ~~ x21 0.685 0.029 0.029 0.106 0.106 ## 89 x16 ~~ x6 0.031 0.009 0.009 0.021 0.021 ## 90 x16 ~~ x13r 0.049 0.017 0.017 0.024 0.024 ## 91 x16 ~~ x12 0.560 -0.020 -0.020 -0.084 -0.084 ## 92 x16 ~~ x7 0.836 0.022 0.022 0.085 0.085 ## 93 x16 ~~ x10 0.004 0.003 0.003 0.006 0.006 ## 94 x16 ~~ x8 1.439 -0.063 -0.063 -0.107 -0.107 ## 95 x16 ~~ x14 3.275 0.051 0.051 0.203 0.203 ## 96 x16 ~~ x19 0.000 0.001 0.001 0.003 0.003 ## 97 x16 ~~ x21 2.088 -0.053 -0.053 -0.154 -0.154 ## 99 x6 ~~ x12 0.161 0.039 0.039 0.111 0.111 ## 100 x6 ~~ x7 0.256 0.026 0.026 0.067 0.067 ## 101 x6 ~~ x10 0.046 -0.021 -0.021 -0.026 -0.026 ## 102 x6 ~~ x8 0.021 0.014 0.014 0.016 0.016 ## 103 x6 ~~ x14 0.865 -0.067 -0.067 -0.179 -0.179 ## 104 x6 ~~ x19 7.958 1.408 1.408 4.191 4.191 ## 105 x6 ~~ x21 2.959 0.100 0.100 0.193 0.193 ## 106 x13r ~~ x12 0.784 -0.081 -0.081 -0.141 -0.141 ## 107 x13r ~~ x7 0.004 -0.004 -0.004 -0.006 -0.006 ## 108 x13r ~~ x10 0.080 -0.037 -0.037 -0.028 -0.028 ## 109 x13r ~~ x8 1.321 0.146 0.146 0.100 0.100 ## 110 x13r ~~ x14 0.197 0.031 0.031 0.050 0.050 ## 111 x13r ~~ x19 7.958 -0.952 -0.952 -1.728 -1.728 ## 112 x13r ~~ x21 0.058 0.021 0.021 0.025 0.025 ## 113 x12 ~~ x7 0.494 0.082 0.082 0.405 0.405 ## 114 x12 ~~ x10 2.507 -0.136 -0.136 -0.322 -0.322 ## 115 x12 ~~ x8 0.019 -0.007 -0.007 -0.014 -0.014 ## 116 x12 ~~ x14 0.125 0.012 0.012 0.059 0.059 ## 117 x12 ~~ x19 2.831 0.158 0.158 0.894 0.894 ## 118 x12 ~~ x21 2.442 0.045 0.045 0.164 0.164 ## 119 x7 ~~ x10 1.855 0.066 0.066 0.142 0.142 ## 120 x7 ~~ x8 0.029 0.006 0.006 0.013 0.013 ## 121 x7 ~~ x14 0.114 -0.007 -0.007 -0.032 -0.032 ## 122 x7 ~~ x19 3.051 -0.066 -0.066 -0.341 -0.341 ## 123 x7 ~~ x21 3.033 -0.045 -0.045 -0.152 -0.152 ## 124 x10 ~~ x8 0.446 -0.058 -0.058 -0.054 -0.054 ## 125 x10 ~~ x14 0.023 0.007 0.007 0.015 0.015 ## 126 x10 ~~ x19 0.124 0.021 0.021 0.052 0.052 ## 127 x10 ~~ x21 0.288 -0.032 -0.032 -0.052 -0.052 ## 129 x8 ~~ x19 0.000 0.000 0.000 0.001 0.001 ## 130 x8 ~~ x21 5.305 0.134 0.134 0.193 0.193 ## 131 x14 ~~ x19 0.000 0.000 0.000 -0.002 -0.002 ## 132 x14 ~~ x21 4.602 -0.066 -0.066 -0.225 -0.225 ## 133 x19 ~~ x21 0.678 -0.032 -0.032 -0.122 -0.122 ## 134 x19 ~ x21 0.678 -0.079 -0.079 -0.062 -0.062 ## 135 x21 ~ CS 1.460 -0.155 -0.105 -0.113 -0.113 ## 136 x21 ~ PV 1.200 0.082 0.093 0.100 0.100 ## 137 x21 ~ MK 0.003 0.003 0.003 0.004 0.004 ## 138 x21 ~ TS 1.302 -0.057 -0.060 -0.065 -0.065 ## 140 CS ~ x21 1.192 -0.128 -0.188 -0.175 -0.175 ## 145 PV ~ x21 3.038 0.432 0.384 0.358 0.358 ## 150 MK ~ x21 0.432 0.111 0.096 0.090 0.090 ## 155 TS ~ x21 1.366 -0.157 -0.148 -0.138 -0.138 lhs op rhs mi epc sepc.lv sepc.all sepc.nox Model has a good fit Plotting the model library(lavaanPlot) labels &lt;- list(x19 = &quot;SATISFACTION&quot;, x21 = &quot;FUTURE PURCHASE&quot;) lavaanPlot(model = fitSEM2 , node_options = list(shape = &quot;box&quot; , fontname = &quot;Helvetica&quot;) , edge_options = list(color = &quot;grey&quot;) , coefs = TRUE , covs=TRUE , stand=TRUE , sig=.05 , stars=&quot;regress&quot; , labels = labels) We see that satisfaction is the mediator, where the future purchase is the final construct that we want to measure. # now # - summarize the findings # - are the all structural paths in the sem model significant? (if so, which hypotheses are supported?) # - which is the most important determinant of customer satisfaction? (check std. path coefficients and conclude) # - does satisfaction act as a sigificant mediator? (check the sig. of mediating patterns and conclude) # - how much variance in x21 (Likelihood of future purchase) the model explains? (check R^2 associated) 2.4 PLS 2.4.1 Definition of PLS We see that partial least squares can be used to make exploratiry analysis as well as prediction. Where SEM is used in a more confirmatory way of looking into relationships, where you assume some structure. Hence we also assume normality in SEM, this is not the case with PLS. See more information in the following illustration. SEMVSPLS 2.4.2 When to use PLS and when to use SEM PLS if: You have a goal of predicting a target construct The structural model is complex (many constructs and indicators) If the sample size is small, and then again, what is small?? Or if the data is not normally distributed. SEM if: The goal is theory testin or comparison of different theories Error terms requires additional specificatio such as the covariation The structural model is non-recursive The research requires a global goodness-of-fit criterion 2.4.3 Model construction We have two types of variables: Latent: These cannot be directly observed. These come in two classes: Exogenous latent variables: are independent variables Endogenous latent variable: are dependent variables. Manifest: This is a directly observable. These come in two classes: Exogenous: reveal exogenious latent variables Endogenous: reveal endogenious latent variables PLS is based on OLS and dont assume linearity and observations does not have to be independent. 2.4.4 Model Estimation It is called partial least squares as some of the model is changed while others is not changed, hence you change it partially. That comes in different procedures. The iterative process is where you first estimate the outer variables and then you move onwards. 2.4.5 Model evaluation Look at the following criteria: Loadings - items reliability. We want the above 0.7 as they indicate that the constrct explains more than 50% of the indicators variance. Thus proving reliability. Internal reliability. Discriminant validity The significance of the inner coefficients The models ability to explain the endogenous latent variables 2.4.6 Example for the lecture PLS is estimated based upon OLS It is always converging It provides an output although the constructs are poorly measured It works with less assumptions than covariance based SEM It has no overall fit criterion it is very popular expecially in consulting Description of the following example: In epsi_pls.R the purpose was to estimate the EPSI model in a slightly modified form as given below. The only diference to the standard EPSI model was that all questions related to quality aspects were items for an overall Quality construct and not divided into Quality with respect to service (Quality soft ware)and Quality with respect to Product (Quality Hard ware) EPSI If we take a look at the dataset (help(mobi) we can see that there are 24 items all in all. One question is about complaints (CUSCO). Complaints is not part of the model, so we do not include that one. We see that the measurement model is all the relationships between the questions (the yellow boxes) to the constructs. They are called the measurement model, as these are the variables that you can measure. Next step is to formulate the model. In semPLS we use a matrix format and not equations like in Lavaan. We describe the measurement model and the structural model separately and in this case we should end up with two matrices like the following: Mapping questions and latentvariables library(boot) #Bootstrapping of standard deviations to outer loadings and path coefficients library(semPLS) library(psych) # help(mobi) data(mobi) #names(mobi) Before we run the model it is always a good idea to have an overview of the dataset. We can use the function describe from library psych to give this overview. #The dataset summary(mobi) describe(mobi) #from library(psych) ## CUEX1 CUEX2 CUEX3 CUSA1 ## Min. : 1.00 Min. : 1.000 Min. : 1.000 Min. : 4.000 ## 1st Qu.: 7.00 1st Qu.: 7.000 1st Qu.: 6.000 1st Qu.: 7.000 ## Median : 8.00 Median : 8.000 Median : 8.000 Median : 8.000 ## Mean : 7.58 Mean : 7.532 Mean : 7.424 Mean : 7.988 ## 3rd Qu.: 8.00 3rd Qu.: 9.000 3rd Qu.: 9.000 3rd Qu.: 9.000 ## Max. :10.00 Max. :10.000 Max. :10.000 Max. :10.000 ## CUSA2 CUSA3 CUSCO CUSL1 ## Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 1.000 ## 1st Qu.: 6.000 1st Qu.: 7.000 1st Qu.: 6.000 1st Qu.: 6.000 ## Median : 7.000 Median : 7.000 Median : 7.000 Median : 8.000 ## Mean : 7.128 Mean : 7.316 Mean : 7.068 Mean : 7.452 ## 3rd Qu.: 8.000 3rd Qu.: 8.000 3rd Qu.: 9.000 3rd Qu.:10.000 ## Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 ## CUSL2 CUSL3 IMAG1 IMAG2 ## Min. : 1.000 Min. : 1.000 Min. : 1.00 Min. : 1.00 ## 1st Qu.: 3.000 1st Qu.: 7.000 1st Qu.: 7.00 1st Qu.: 7.00 ## Median : 4.000 Median : 8.000 Median : 8.00 Median : 8.00 ## Mean : 4.988 Mean : 7.668 Mean : 7.64 Mean : 7.78 ## 3rd Qu.: 6.750 3rd Qu.:10.000 3rd Qu.: 9.00 3rd Qu.: 9.00 ## Max. :10.000 Max. :10.000 Max. :10.00 Max. :10.00 ## IMAG3 IMAG4 IMAG5 PERQ1 ## Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 2.000 ## 1st Qu.: 5.000 1st Qu.: 7.000 1st Qu.: 7.000 1st Qu.: 7.000 ## Median : 7.000 Median : 8.000 Median : 8.000 Median : 8.000 ## Mean : 6.744 Mean : 7.588 Mean : 7.932 Mean : 7.944 ## 3rd Qu.: 8.000 3rd Qu.: 9.000 3rd Qu.: 9.000 3rd Qu.: 9.000 ## Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 ## PERQ2 PERQ3 PERQ4 PERQ5 ## Min. : 1.000 Min. : 1.0 Min. : 1.000 Min. : 3.000 ## 1st Qu.: 6.000 1st Qu.: 7.0 1st Qu.: 7.000 1st Qu.: 7.000 ## Median : 7.000 Median : 8.0 Median : 8.000 Median : 8.000 ## Mean : 7.192 Mean : 7.7 Mean : 7.916 Mean : 7.872 ## 3rd Qu.: 8.000 3rd Qu.: 9.0 3rd Qu.: 9.000 3rd Qu.: 9.000 ## Max. :10.000 Max. :10.0 Max. :10.000 Max. :10.000 ## PERQ6 PERQ7 PERV1 PERV2 ## Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 1.000 ## 1st Qu.: 7.000 1st Qu.: 7.000 1st Qu.: 5.000 1st Qu.: 6.000 ## Median : 8.000 Median : 8.000 Median : 6.000 Median : 7.000 ## Mean : 7.776 Mean : 7.592 Mean : 6.156 Mean : 6.916 ## 3rd Qu.: 9.000 3rd Qu.: 9.000 3rd Qu.: 8.000 3rd Qu.: 8.000 ## Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 vars n mean sd median trimmed mad min max range skew kurtosis se CUEX1 1 250 7.580 1.621765 8 7.680 1.4826 1 10 9 -0.6632888 0.8289969 0.1025694 CUEX2 2 250 7.532 1.792716 8 7.665 1.4826 1 10 9 -0.7996766 1.4470764 0.1133813 CUEX3 3 250 7.424 2.102834 8 7.630 1.4826 1 10 9 -0.8489926 0.6154474 0.1329949 CUSA1 4 250 7.988 1.233671 8 8.030 1.4826 4 10 6 -0.2201919 0.1527890 0.0780242 CUSA2 5 250 7.128 1.765242 7 7.180 1.4826 1 10 9 -0.5471886 0.5579660 0.1116437 CUSA3 6 250 7.316 1.747099 7 7.405 1.4826 1 10 9 -0.6671235 0.9615690 0.1104962 CUSCO 7 250 7.068 2.274217 7 7.285 2.2239 1 10 9 -0.7099909 0.2384216 0.1438341 CUSL1 8 250 7.452 2.658565 8 7.860 2.9652 1 10 9 -0.9946527 0.0768705 0.1681424 CUSL2 9 250 4.988 2.839031 4 4.800 2.9652 1 10 9 0.5449058 -0.9063918 0.1795561 CUSL3 10 250 7.668 2.218437 8 7.955 2.9652 1 10 9 -1.0592863 0.8637662 0.1403063 IMAG1 11 250 7.640 1.699988 8 7.765 1.4826 1 10 9 -0.9014379 1.6887323 0.1075167 IMAG2 12 250 7.780 1.687302 8 7.930 1.4826 1 10 9 -0.7931040 0.9620206 0.1067143 IMAG3 13 250 6.744 2.131817 7 6.905 1.4826 1 10 9 -0.7930042 0.6570813 0.1348279 IMAG4 14 250 7.588 1.841690 8 7.755 1.4826 1 10 9 -0.9231357 1.3210435 0.1164787 IMAG5 15 250 7.932 1.557266 8 8.070 1.4826 1 10 9 -1.0694987 2.6817548 0.0984901 PERQ1 16 250 7.944 1.421600 8 8.040 1.4826 2 10 8 -0.7203890 1.1786363 0.0899099 PERQ2 17 250 7.192 1.891414 7 7.330 1.4826 1 10 9 -0.8186657 0.7287376 0.1196235 PERQ3 18 250 7.700 1.821888 8 7.885 1.4826 1 10 9 -0.9064463 0.8310284 0.1152263 PERQ4 19 250 7.916 1.651622 8 8.105 1.4826 1 10 9 -1.0704382 1.7901560 0.1044578 PERQ5 20 250 7.872 1.453294 8 7.970 1.4826 3 10 7 -0.6770313 0.5583053 0.0919144 PERQ6 21 250 7.776 1.629862 8 7.925 1.4826 1 10 9 -0.9743906 1.7870540 0.1030815 PERQ7 22 250 7.592 1.843674 8 7.775 1.4826 1 10 9 -0.9593571 1.0528448 0.1166042 PERV1 23 250 6.156 2.183284 6 6.260 1.4826 1 10 9 -0.3971992 -0.2142253 0.1380830 PERV2 24 250 6.916 1.842423 7 6.980 1.4826 1 10 9 -0.6445980 0.9714600 0.1165251 Here we can see that all items apart from CUSL2 has a negative skewness. If skewness is negative then it means that the tail in the distribution is to the left. Less than -1 then highly skewed, between -0,5 and -1 then moderately skewed. Kurtosis is the degree of peakedness in a distribution (how heavy tails). Negative kurtosis is an indicator of a more even distribution across possible outcomes. It can be seen that CUSL2 has a positive skewness (more even distribution) and a negative kurtosis. Answers to this question behaves completely different compared to the rest. The question is also compared to the other questions that are more straightforward to answer. We can plot the distribution of the CUSL1 and 2. hist(mobi$CUSL1) Figure 2.1: Histogram CUSL1 hist(mobi$CUSL2) Figure 2.2: Histogram CUSL2 We see that they are not at all normally distributed. Notice that we are going to build the following model using in R. We see that we are going to use the arrows which are non-dotted. We see that in the package we have image that is the only variable that are not described by other constructs. Now we are going to define the structural model, by describing the varaibles that are linked to them, hence identifying the indicators of each of the latent variables. Now we can ‘draw’ the model. We saw in SEM that we wrote it as a function, here we make it as a matrix. 2.4.6.1 Making structural- and measurement model We relate the constructs and the varaibles in the following. #The structural model EPSIsm &lt;- matrix(c(&quot;IMAGE&quot;, &quot;VALUE&quot; ,&quot;EXP&quot;,&quot;VALUE&quot; ,&quot;QUAL&quot;,&quot;VALUE&quot; ,&quot;VALUE&quot;,&quot;SATISF&quot; ,&quot;SATISF&quot;,&quot;LOYAL&quot; ,&quot;IMAGE&quot;,&quot;LOYAL&quot;),byrow=TRUE,ncol=2) colnames(EPSIsm)=c(&quot;source&quot;,&quot;target&quot;) EPSIsm ## source target ## [1,] &quot;IMAGE&quot; &quot;VALUE&quot; ## [2,] &quot;EXP&quot; &quot;VALUE&quot; ## [3,] &quot;QUAL&quot; &quot;VALUE&quot; ## [4,] &quot;VALUE&quot; &quot;SATISF&quot; ## [5,] &quot;SATISF&quot; &quot;LOYAL&quot; ## [6,] &quot;IMAGE&quot; &quot;LOYAL&quot; #The measurement model EPSImm1 &lt;-matrix(c(&quot;IMAGE&quot;, &quot;IMAG1&quot;,&quot;IMAGE&quot;,&quot;IMAG2&quot;,&quot;IMAGE&quot;,&quot;IMAG3&quot;,&quot;IMAGE&quot;,&quot;IMAG4&quot;),byrow=TRUE,ncol=2) colnames(EPSImm1)=c(&quot;source&quot;,&quot;target&quot;) # EPSImm1 EPSImm2 &lt;-matrix(c(&quot;EXP&quot;, &quot;CUEX1&quot;,&quot;EXP&quot;,&quot;CUEX2&quot;,&quot;EXP&quot;,&quot;CUEX3&quot;),byrow=TRUE,ncol=2) colnames(EPSImm2)=c(&quot;source&quot;,&quot;target&quot;) EPSImm3 &lt;-matrix(c(&quot;QUAL&quot;, &quot;PERQ1&quot;,&quot;QUAL&quot;,&quot;PERQ2&quot;,&quot;QUAL&quot;,&quot;PERQ3&quot;,&quot;QUAL&quot;,&quot;PERQ4&quot;,&quot;QUAL&quot;,&quot;PERQ5&quot;, &quot;QUAL&quot;,&quot;PERQ6&quot;,&quot;QUAL&quot;,&quot;PERQ7&quot;),byrow=TRUE,ncol=2) colnames(EPSImm3)=c(&quot;source&quot;,&quot;target&quot;) head(EPSImm3,7) EPSImm4 &lt;-matrix(c(&quot;VALUE&quot;, &quot;PERV1&quot;,&quot;VALUE&quot;,&quot;PERV2&quot;),byrow=TRUE,ncol=2) colnames(EPSImm4)=c(&quot;source&quot;,&quot;target&quot;) EPSImm5 &lt;-matrix(c(&quot;SATISF&quot;, &quot;CUSA1&quot;,&quot;SATISF&quot;,&quot;CUSA2&quot;,&quot;SATISF&quot;,&quot;CUSA3&quot;),byrow=TRUE,ncol=2) colnames(EPSImm5)=c(&quot;source&quot;,&quot;target&quot;) EPSImm6 &lt;-matrix(c(&quot;LOYAL&quot;, &quot;CUSL1&quot;,&quot;LOYAL&quot;,&quot;CUSL2&quot;,&quot;LOYAL&quot;,&quot;CUSL3&quot;),byrow=TRUE,ncol=2) colnames(EPSImm6)=c(&quot;source&quot;,&quot;target&quot;) ## source target ## [1,] &quot;QUAL&quot; &quot;PERQ1&quot; ## [2,] &quot;QUAL&quot; &quot;PERQ2&quot; ## [3,] &quot;QUAL&quot; &quot;PERQ3&quot; ## [4,] &quot;QUAL&quot; &quot;PERQ4&quot; ## [5,] &quot;QUAL&quot; &quot;PERQ5&quot; ## [6,] &quot;QUAL&quot; &quot;PERQ6&quot; ## [7,] &quot;QUAL&quot; &quot;PERQ7&quot; EPSImm &lt;-rbind(EPSImm1,EPSImm2,EPSImm3,EPSImm4,EPSImm5,EPSImm6) EPSImm ## source target ## [1,] &quot;IMAGE&quot; &quot;IMAG1&quot; ## [2,] &quot;IMAGE&quot; &quot;IMAG2&quot; ## [3,] &quot;IMAGE&quot; &quot;IMAG3&quot; ## [4,] &quot;IMAGE&quot; &quot;IMAG4&quot; ## [5,] &quot;EXP&quot; &quot;CUEX1&quot; ## [6,] &quot;EXP&quot; &quot;CUEX2&quot; ## [7,] &quot;EXP&quot; &quot;CUEX3&quot; ## [8,] &quot;QUAL&quot; &quot;PERQ1&quot; ## [9,] &quot;QUAL&quot; &quot;PERQ2&quot; ## [10,] &quot;QUAL&quot; &quot;PERQ3&quot; ## [11,] &quot;QUAL&quot; &quot;PERQ4&quot; ## [12,] &quot;QUAL&quot; &quot;PERQ5&quot; ## [13,] &quot;QUAL&quot; &quot;PERQ6&quot; ## [14,] &quot;QUAL&quot; &quot;PERQ7&quot; ## [15,] &quot;VALUE&quot; &quot;PERV1&quot; ## [16,] &quot;VALUE&quot; &quot;PERV2&quot; ## [17,] &quot;SATISF&quot; &quot;CUSA1&quot; ## [18,] &quot;SATISF&quot; &quot;CUSA2&quot; ## [19,] &quot;SATISF&quot; &quot;CUSA3&quot; ## [20,] &quot;LOYAL&quot; &quot;CUSL1&quot; ## [21,] &quot;LOYAL&quot; &quot;CUSL2&quot; ## [22,] &quot;LOYAL&quot; &quot;CUSL3&quot; 2.4.6.2 Now we can estimate the model estimating and looking at the matrices reflecting the relationships. #The whole model EPSI &lt;- plsm(data=mobi ,strucmod=EPSIsm ,measuremod=EPSImm) EPSI[[&quot;D&quot;]] #The structural model (inner relations) EPSI[[&quot;M&quot;]] #The measurement model ## EXP IMAGE QUAL VALUE SATISF LOYAL ## EXP 0 0 0 1 0 0 ## IMAGE 0 0 0 1 0 1 ## QUAL 0 0 0 1 0 0 ## VALUE 0 0 0 0 1 0 ## SATISF 0 0 0 0 0 1 ## LOYAL 0 0 0 0 0 0 ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 1 0 0 0 0 0 ## CUEX2 1 0 0 0 0 0 ## CUEX3 1 0 0 0 0 0 ## IMAG1 0 1 0 0 0 0 ## IMAG2 0 1 0 0 0 0 ## IMAG3 0 1 0 0 0 0 ## IMAG4 0 1 0 0 0 0 ## PERQ1 0 0 1 0 0 0 ## PERQ2 0 0 1 0 0 0 ## PERQ3 0 0 1 0 0 0 ## PERQ4 0 0 1 0 0 0 ## PERQ5 0 0 1 0 0 0 ## PERQ6 0 0 1 0 0 0 ## PERQ7 0 0 1 0 0 0 ## PERV1 0 0 0 1 0 0 ## PERV2 0 0 0 1 0 0 ## CUSA1 0 0 0 0 1 0 ## CUSA2 0 0 0 0 1 0 ## CUSA3 0 0 0 0 1 0 ## CUSL1 0 0 0 0 0 1 ## CUSL2 0 0 0 0 0 1 ## CUSL3 0 0 0 0 0 1 #Estimation of model epsi &lt;- sempls(model=EPSI,data=mobi) epsi ## All 250 observations are valid. ## Converged after 8 iterations. ## Tolerance: 1e-07 ## Scheme: centroid ## Path Estimate ## lam_1_1 EXP -&gt; CUEX1 0.786 ## lam_1_2 EXP -&gt; CUEX2 0.583 ## lam_1_3 EXP -&gt; CUEX3 0.682 ## lam_2_1 IMAGE -&gt; IMAG1 0.751 ## lam_2_2 IMAGE -&gt; IMAG2 0.593 ## lam_2_3 IMAGE -&gt; IMAG3 0.628 ## lam_2_4 IMAGE -&gt; IMAG4 0.821 ## lam_3_1 QUAL -&gt; PERQ1 0.796 ## lam_3_2 QUAL -&gt; PERQ2 0.626 ## lam_3_3 QUAL -&gt; PERQ3 0.789 ## lam_3_4 QUAL -&gt; PERQ4 0.762 ## lam_3_5 QUAL -&gt; PERQ5 0.764 ## lam_3_6 QUAL -&gt; PERQ6 0.769 ## lam_3_7 QUAL -&gt; PERQ7 0.791 ## lam_4_1 VALUE -&gt; PERV1 0.903 ## lam_4_2 VALUE -&gt; PERV2 0.939 ## lam_5_1 SATISF -&gt; CUSA1 0.784 ## lam_5_2 SATISF -&gt; CUSA2 0.845 ## lam_5_3 SATISF -&gt; CUSA3 0.865 ## lam_6_1 LOYAL -&gt; CUSL1 0.825 ## lam_6_2 LOYAL -&gt; CUSL2 0.209 ## lam_6_3 LOYAL -&gt; CUSL3 0.911 ## beta_1_4 EXP -&gt; VALUE 0.036 ## beta_2_4 IMAGE -&gt; VALUE 0.232 ## beta_3_4 QUAL -&gt; VALUE 0.403 ## beta_4_5 VALUE -&gt; SATISF 0.610 ## beta_2_6 IMAGE -&gt; LOYAL 0.140 ## beta_5_6 SATISF -&gt; LOYAL 0.563 We see that we get the estimates, which are the coefficients. Just as in the covariance based approach it is important to have a good measurement model and the criteria are the same. Firstly we have to make sure that the items are good indicators for the latent constructs. Here we look at the loadings. The loadings are produced with the following line: # Evaluation of estimated model plsLoadings(epsi) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 0.79 . . . . . ## CUEX2 0.58 . . . . . ## CUEX3 0.68 . . . . . ## IMAG1 . 0.75 . . . . ## IMAG2 . 0.59 0.49 . . . ## IMAG3 . 0.63 . . . . ## IMAG4 . 0.82 . . . . ## PERQ1 . . 0.80 . 0.68 . ## PERQ2 . . 0.63 . . . ## PERQ3 . . 0.79 . 0.65 . ## PERQ4 . . 0.76 . . . ## PERQ5 . . 0.76 . . . ## PERQ6 . . 0.77 . . . ## PERQ7 . . 0.79 . 0.70 . ## PERV1 . . . 0.90 . . ## PERV2 . . . 0.94 . . ## CUSA1 . . 0.64 . 0.78 . ## CUSA2 . . . . 0.85 . ## CUSA3 . . . . 0.86 . ## CUSL1 . . . . . 0.83 ## CUSL2 . . . . . 0.21 ## CUSL3 . . . . . 0.91 We see that expectations are influenced by CUEX, and Image is defined by image etc. Although it is rather interesting to see that the satisfaction is defined by several variable. When we look on the loyalty, we see that question no. 2 (CUSL2) that there is a very low relationship. The rule of thumb, is that the relationship needs to be at least 0.7. So we also have other variables that are questionable. 2.4.6.3 Measuring quality of the constructs We can can compute the composite reliability by the following: #Composite reliability dgrho(epsi) ## Dillon-Goldstein&#39;s rho reflective MVs ## EXP 0.73 3 ## IMAGE 0.79 4 ## QUAL 0.90 7 ## VALUE 0.92 2 ## SATISF 0.87 3 ## LOYAL 0.72 3 We want the coefficients be higher than 0.8 (rule of thumb), hence we see that loyalty for instance is low. that is because of the question no. 2, that was very weak. # AVE is average of communality for a construct communality(epsi) #These are just the loadings squared ## communality reflective MVs ## EXP 0.47 3 ## IMAGE 0.50 4 ## QUAL 0.58 7 ## VALUE 0.85 2 ## SATISF 0.69 3 ## LOYAL 0.52 3 ## ## Average communality: 0.58 Commonality must be higher than correlation between the latent variables. If not, you can argue that two constructs should be merged. EXP = 0.47, meaning that on average EXP explains 47% of the variance in the items related to EXP. Therefore Rule of thumb, should be higher than 50%, that is also why the loadings should be higher than 70% as \\(0.7^2 = 0.5\\). Although 60% is a nice level as well now we can look at the inner relations, relationships between latent variables. We see that in the following matrix we only get estimates for endogenous (dependent variables), that is the reason we dont see any for EXP, IMA and QUA, because they are only endogenous in the measurement models. #Inner relations pC &lt;- pathCoeff(epsi) print(pC,abbreviate=TRUE,minlength=3) ## EXP IMA QUA VAL SAT LOY ## EXP . . . 0.036 . . ## IMA . . . 0.232 . 0.140 ## QUA . . . 0.403 . . ## VAL . . . . 0.610 . ## SAT . . . . . 0.563 ## LOY . . . . . . Example: if we increase EXP by 1 standard deviation, the VAL will increase by 0.036 standard deviations. we see that the coefficients are standardized. #The total effects tE &lt;- totalEffects(epsi) print(tE,abbreviate=TRUE,minlength=3) ## EXP IMA QUA VAL SAT LOY ## EXP . . . 0.036 0.022 0.012 ## IMA . . . 0.232 0.141 0.219 ## QUA . . . 0.403 0.246 0.139 ## VAL . . . . 0.610 0.344 ## SAT . . . . . 0.563 ## LOY . . . . . . The total effect includes the indirect effects, e.g., going between latent variables. #R2 rSquared(epsi) ## R-squared ## EXP . ## IMAGE . ## QUAL . ## VALUE 0.38 ## SATISF 0.37 ## LOYAL 0.44 We see that the \\(R^2\\) is rather low. It is common to say, that it needs to be at least 0.65, i.e., 65%. 2.4.6.4 Are the coefficients significant? We can run a bootstrap to assess whether the coefficients are significant. #Bootstrapping - estimation of standard errors set.seed(123) epsiBoot &lt;- bootsempls(epsi,nboot=500,start=&quot;ones&quot;,verbose=FALSE) epsiBoot ## Call: bootsempls(object = epsi, nboot = 500, start = &quot;ones&quot;, verbose = FALSE) ## ## Estimate Bias Std.Error ## EXP -&gt; CUEX1 0.7865 -0.015459 0.0716 ## EXP -&gt; CUEX2 0.5827 -0.019156 0.1474 ## EXP -&gt; CUEX3 0.6817 -0.003128 0.1026 ## IMAGE -&gt; IMAG1 0.7505 -0.002769 0.0417 ## IMAGE -&gt; IMAG2 0.5927 -0.006763 0.0764 ## IMAGE -&gt; IMAG3 0.6281 -0.002382 0.0632 ## IMAGE -&gt; IMAG4 0.8215 -0.001675 0.0355 ## QUAL -&gt; PERQ1 0.7962 -0.000784 0.0264 ## QUAL -&gt; PERQ2 0.6256 -0.006429 0.0539 ## QUAL -&gt; PERQ3 0.7893 -0.000843 0.0306 ## QUAL -&gt; PERQ4 0.7622 -0.001921 0.0467 ## QUAL -&gt; PERQ5 0.7644 -0.000716 0.0342 ## QUAL -&gt; PERQ6 0.7694 0.001351 0.0582 ## QUAL -&gt; PERQ7 0.7906 0.001880 0.0295 ## VALUE -&gt; PERV1 0.9035 -0.001996 0.0208 ## VALUE -&gt; PERV2 0.9386 0.000559 0.0071 ## SATISF -&gt; CUSA1 0.7841 -0.003195 0.0347 ## SATISF -&gt; CUSA2 0.8451 0.000466 0.0230 ## SATISF -&gt; CUSA3 0.8648 0.001230 0.0164 ## LOYAL -&gt; CUSL1 0.8253 -0.005565 0.0420 ## LOYAL -&gt; CUSL2 0.2087 0.001986 0.1127 ## LOYAL -&gt; CUSL3 0.9107 -0.002258 0.0138 ## EXP -&gt; VALUE 0.0356 0.019026 0.0822 ## IMAGE -&gt; VALUE 0.2317 -0.005509 0.1146 ## QUAL -&gt; VALUE 0.4035 -0.004219 0.1216 ## VALUE -&gt; SATISF 0.6105 -0.001294 0.0552 ## IMAGE -&gt; LOYAL 0.1396 0.013019 0.0762 ## SATISF -&gt; LOYAL 0.5635 -0.007305 0.0873 E.g., we see that the relationship between the expectations (EXP) and the value (VALUE) is rather low and there does not appear to be any relationship. # Structural model coefficients parallelplot(epsiBoot,reflinesAt=0, alpha=0.8, varnames=attr(epsiBoot$t,&quot;path&quot;)[23:28]) We see that the dotted line = 0. This gives an idea of the stability of the estimation that we have done. #Constructing observations plsWeights(epsi) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 0.56 . . . . . ## CUEX2 0.34 . . . . . ## CUEX3 0.52 . . . . . ## IMAG1 . 0.36 . . . . ## IMAG2 . 0.28 . . . . ## IMAG3 . 0.31 . . . . ## IMAG4 . 0.45 . . . . ## PERQ1 . . 0.20 . . . ## PERQ2 . . 0.13 . . . ## PERQ3 . . 0.20 . . . ## PERQ4 . . 0.17 . . . ## PERQ5 . . 0.20 . . . ## PERQ6 . . 0.17 . . . ## PERQ7 . . 0.24 . . . ## PERV1 . . . 0.48 . . ## PERV2 . . . 0.60 . . ## CUSA1 . . . . 0.35 . ## CUSA2 . . . . 0.38 . ## CUSA3 . . . . 0.47 . ## CUSL1 . . . . . 0.47 ## CUSL2 . . . . . 0.12 ## CUSL3 . . . . . 0.64 These are the factor scores. Notice, that this has nothing to do with the bootstrapping #Standardised observations for (constructs) epsi$factor_scores densityplot(epsi) densityplot(epsi,use=&quot;prediction&quot;) ## EXP IMAGE QUAL VALUE SATISF ## [1,] -0.658288481 -1.47979167 -1.601304825 -2.195687443 -1.31975186 ## [2,] 1.705062008 1.76533519 1.580150565 1.855563932 1.37048009 ## [3,] -0.408804865 -1.03652985 -0.610023359 0.214167984 -0.10886680 ## [4,] -0.333389664 -0.02013215 -0.307365027 -0.880095982 1.90740374 ## [5,] 0.686485052 0.71219193 0.996103664 -0.332963999 0.94173802 ## [6,] 1.014633965 0.85295798 1.027125702 1.855563932 -0.10886680 ## [7,] -1.154005617 -1.03381653 -0.039740834 -0.228487009 0.37396607 ## [8,] -2.790845510 0.29824424 -0.313829831 -0.228487009 -0.60712381 ## [9,] -0.408804865 -0.43102726 -0.507723115 -0.228487009 -0.39275277 ## [10,] -1.059900259 -0.33833674 -0.327519100 -0.775618992 -1.35841850 ## [11,] -0.350782060 0.12368648 0.199097106 0.214167984 0.24453412 ## [12,] 1.705062008 0.33542780 0.301951965 -0.228487009 0.10550424 ## [13,] 0.245540625 0.56111368 0.405396543 -0.554291495 0.10550424 ## [14,] 1.954545624 1.93001875 1.577843874 0.539972470 0.94173802 ## [15,] -0.736953778 -0.25131339 0.008080324 -1.544078471 -0.32323783 ## [16,] -0.603515772 0.69132700 1.132765394 -0.892469498 -0.03935186 ## [17,] -0.119988600 0.71996262 0.530897165 0.748926450 0.38939021 ## [18,] -1.102483004 -1.91755812 -0.782798203 0.214167984 -1.07453253 ## [19,] 0.877945863 0.91911641 0.646482277 0.435495480 0.92631387 ## [20,] -1.351966620 -0.90631652 -0.343531269 0.214167984 -0.16295759 ## [21,] -1.449322073 0.41519153 0.194718888 0.214167984 0.58833710 ## [22,] 0.126244920 -0.14501757 -0.146887809 -0.880095982 -1.57278954 ## [23,] -0.791726487 -0.37036419 -0.747904529 -0.228487009 -0.92967643 ## [24,] -0.159321249 -0.05975848 -0.035917678 1.308431949 0.37396607 ## [25,] -0.061965795 -0.30386651 -1.444576807 -1.322750974 -2.16380399 ## [26,] 0.761900254 1.93001875 1.785463911 0.761299966 1.47866167 ## [27,] 0.378978631 1.00403624 0.071315399 0.214167984 -0.53760887 ## [28,] 0.877945863 -1.31381980 -2.180453101 1.855563932 -0.53760887 ## [29,] -0.755643934 -0.05975848 -0.215194499 0.539972470 -0.10886680 ## [30,] 0.914028416 1.63817444 1.410950332 1.412908939 1.63894192 ## [31,] -0.564183123 0.09131977 0.617766930 1.087104453 0.65785204 ## [32,] -0.658288481 -0.37036419 -0.215749562 -0.880095982 -1.07453253 ## [33,] -1.027067803 0.78612105 -0.363325695 0.748926450 -0.28457119 ## [34,] 0.339645983 0.73872402 1.125292073 0.435495480 0.55748881 ## [35,] 0.997241569 -0.41226585 -2.968406646 -3.068623912 -2.86100789 ## [36,] 1.571624002 0.99820162 0.223219561 0.435495480 -0.39275277 ## [37,] -0.101298444 -0.43102726 -0.053728589 -1.322750974 -1.46660009 ## [38,] -0.560933027 -0.64649967 -0.351856229 -0.658768485 -0.53760887 ## [39,] -0.061965795 0.46808390 1.142683740 1.308431949 0.99582881 ## [40,] -0.159321249 -0.43652262 -0.108516485 0.318644974 -0.10886680 ## [41,] 1.954545624 -0.04133633 0.985042352 0.761299966 1.42457088 ## [42,] 0.476334085 0.28253542 0.508782224 0.435495480 0.42805686 ## [43,] -1.601450236 1.01180692 1.296477731 1.412908939 1.37048009 ## [44,] -0.061965795 -0.74712833 -0.477489487 -1.101423478 -0.12429095 ## [45,] 1.069406674 1.63817444 1.542124166 1.412908939 1.90740374 ## [46,] 0.877945863 0.83351807 1.476098692 0.435495480 0.65785204 ## [47,] 0.686485052 0.38282482 0.452513887 -1.544078471 -0.32323783 ## [48,] -0.716311286 -0.12625616 0.097351924 0.539972470 0.15959503 ## [49,] -0.680228733 -2.51383426 -2.280570690 -1.648555461 -1.91076631 ## [50,] -0.673728541 -1.66422364 0.417007621 -2.742819426 -1.30432771 ## [51,] 0.877945863 1.06809114 0.435752390 -0.449814505 0.80270814 ## [52,] -0.697621129 0.08582441 -0.786696464 -0.554291495 -0.60712381 ## [53,] 0.054079814 -0.01236146 0.333008660 0.761299966 0.38939021 ## [54,] 1.072656770 -0.77305062 -0.067442099 0.435495480 -0.10886680 ## [55,] -0.907772097 -0.20568064 -0.257326330 0.435495480 -0.32323783 ## [56,] 0.226850469 -0.69973131 -0.108947513 0.435495480 -0.10886680 ## [57,] -0.542242871 -0.55852726 -1.895611695 -0.880095982 -2.30084173 ## [58,] 0.090162367 0.77319425 0.950679500 0.761299966 0.65785204 ## [59,] 1.072656770 1.28017171 0.758251815 1.087104453 1.47866167 ## [60,] -1.063150355 -0.12625616 -0.107961422 -0.332963999 -0.10886680 ## [61,] -0.829106799 -0.87734165 -1.074723275 -1.427227964 -1.58821368 ## [62,] -0.791726487 -0.18725849 0.376091925 0.214167984 0.17501918 ## [63,] 0.783840506 0.01966599 -0.010594458 0.982627463 0.15959503 ## [64,] -0.506160318 -1.35734903 -1.280461433 -0.554291495 -0.82149484 ## [65,] 0.534356890 0.18434954 -0.076250768 -0.007159513 -0.66121460 ## [66,] 0.129495016 0.51337740 0.741170419 -1.218273984 -0.09344265 ## [67,] 0.761900254 -0.11638195 0.872617039 0.214167984 0.10550424 ## [68,] 0.378978631 -0.37036419 -0.626252668 -0.880095982 -2.76825045 ## [69,] 0.093412463 -0.06009774 0.262604967 -0.007159513 0.15959503 ## [70,] 0.187517820 -0.20601989 0.288139110 -0.332963999 -0.60712381 ## [71,] -0.159321249 0.20311095 0.564730662 0.214167984 0.44348101 ## [72,] 1.954545624 0.71219193 1.242835536 0.214167984 0.78145777 ## [73,] 0.512416638 0.54784763 1.504494141 -3.068623912 1.90740374 ## [74,] -1.001877454 -1.28274140 -0.943118413 0.214167984 -1.14404747 ## [75,] 1.260867485 -0.15828362 -0.219572717 0.214167984 -1.67897899 ## [76,] -0.520302619 -2.09727199 -0.448668578 -1.427227964 -0.87558564 ## [77,] 1.416245744 -0.37036419 0.058612221 0.761299966 -0.07020015 ## [78,] -1.637532788 -0.60120617 -1.238508334 -0.880095982 -0.37732863 ## [79,] -0.253426606 0.18434954 -0.116841445 -0.332963999 -0.32323783 ## [80,] -0.217344054 -0.40239164 -0.236298192 0.214167984 -0.17838174 ## [81,] 0.975301317 0.56111368 1.094025690 0.214167984 0.58833710 ## [82,] -1.001877454 0.32993244 -0.172024346 0.539972470 -0.48351808 ## [83,] 0.378978631 0.65929955 0.819232879 0.539972470 1.40914673 ## [84,] -0.658288481 -0.68096990 0.030982310 -0.007159513 -0.59169966 ## [85,] -0.003942991 -0.56402262 -0.137662860 -0.007159513 -0.37732863 ## [86,] -1.442821881 0.06706301 0.455032652 -0.124010019 0.03598930 ## [87,] 1.224784933 0.48134995 0.779910122 1.087104453 1.12526076 ## [88,] 1.108739323 0.70669657 0.923073830 1.308431949 0.42805686 ## [89,] 0.187517820 0.13661327 0.081181513 0.539972470 0.65785204 ## [90,] -0.408804865 0.52664345 0.996782313 0.539972470 0.30445113 ## [91,] 0.378978631 1.22972211 1.192105959 0.761299966 0.67327619 ## [92,] -0.061965795 -0.13952221 -0.640992717 -0.007159513 0.10550424 ## [93,] 0.320955827 0.64053815 0.714095570 0.539972470 0.85679893 ## [94,] -0.813666739 -0.41776121 -0.167829059 0.109690994 -0.17838174 ## [95,] -0.791726487 -2.65130722 -3.019805379 -1.427227964 -3.10622722 ## [96,] 0.531106794 1.55325461 0.722975593 1.412908939 0.91088972 ## [97,] -0.195403802 0.21848052 0.693498460 0.982627463 0.64242789 ## [98,] -0.350782060 -0.51628634 -0.520501399 0.214167984 -0.32323783 ## [99,] -1.099232908 -0.90665578 -0.522808090 -0.228487009 -0.10886680 ## [100,] 0.090162367 1.49225229 1.238730552 0.214167984 0.85679893 ## [101,] -0.195403802 -0.04133633 0.022657350 0.539972470 -0.53760887 ## [102,] 0.129495016 0.25050797 -0.099636462 0.214167984 0.10550424 ## [103,] 0.187517820 -0.21962519 0.071425161 0.539972470 0.37396607 ## [104,] -0.658288481 -0.53504775 -0.761646030 -0.007159513 0.51299595 ## [105,] 0.284873274 1.14995839 0.852781714 0.761299966 0.72736698 ## [106,] -1.116625304 -2.00553053 -0.241107438 -0.880095982 -1.35841850 ## [107,] 1.954545624 -0.84354993 0.205771157 1.203954959 0.94173802 ## [108,] 0.032139562 0.31700565 0.002108720 -0.671142002 0.64242789 ## [109,] -0.517052523 0.85601056 -0.095340423 0.761299966 0.15959503 ## [110,] 0.570439442 0.25050797 0.270375312 0.539972470 1.47866167 ## [111,] -0.369472216 -1.00694518 0.260533440 -0.228487009 0.38939021 ## [112,] 0.975301317 1.06503856 0.507141725 0.865776956 -0.05477601 ## [113,] 1.358222939 0.85261873 0.923073830 0.982627463 1.14068490 ## [114,] 0.877945863 1.21401329 1.785463911 0.761299966 1.47866167 ## [115,] 0.415061184 1.16288518 1.296874104 0.761299966 0.91088972 ## [116,] 0.220350277 0.47619385 -0.852860722 -1.544078471 -1.07453253 ## [117,] -1.752280638 1.47383014 0.877118844 1.855563932 1.33963179 ## [118,] 1.954545624 1.93001875 1.277148541 0.761299966 0.72736698 ## [119,] 1.571624002 0.80522170 1.241083907 1.308431949 0.64242789 ## [120,] 0.823173154 1.50585759 0.454660520 0.761299966 1.33963179 ## [121,] 1.224784933 0.79195566 0.436948955 0.539972470 0.28120862 ## [122,] 0.226850469 -0.79791718 -0.460103389 -1.869882957 -1.49744838 ## [123,] 1.455578392 1.93001875 1.785463911 1.855563932 1.90740374 ## [124,] -0.427495021 -1.44226886 -1.395064285 -0.554291495 -1.30432771 ## [125,] -0.639598325 -0.69973131 -1.705764852 -0.007159513 -0.53760887 ## [126,] -0.253426606 -2.12685667 0.181892124 -0.880095982 -0.67663875 ## [127,] 1.513601197 1.57201602 1.063558715 1.308431949 1.35505594 ## [128,] -0.849749292 -0.50302030 -0.006216240 -0.554291495 0.10550424 ## [129,] -0.061965795 -0.48765073 -0.247360871 -0.880095982 -0.87558564 ## [130,] -0.159321249 0.39643012 -0.207582844 0.214167984 -0.10886680 ## [131,] 0.498274337 -0.35092428 -0.610305188 0.748926450 -0.15513924 ## [132,] 0.148185172 -1.40474605 -0.438751600 -1.427227964 -1.35841850 ## [133,] -2.025002266 -1.73926926 -2.508971243 -0.880095982 -1.64230448 ## [134,] 0.457643929 1.01764154 1.110517368 1.203954959 -0.19380589 ## [135,] -0.907772097 -0.12930875 -0.245844406 -0.007159513 -0.10886680 ## [136,] -0.827809039 0.17108350 0.604504937 0.761299966 0.37396607 ## [137,] 1.954545624 1.93001875 1.785463911 1.855563932 1.90740374 ## [138,] 0.129495016 0.31700565 0.243907759 0.318644974 0.94173802 ## [139,] 0.917278512 0.70703583 0.966799046 -0.462188022 1.63894192 ## [140,] 0.570439442 0.38282482 0.141349927 1.191581443 0.37396607 ## [141,] 0.284873274 1.32756874 0.580667725 0.331018490 0.92631387 ## [142,] 0.129495016 0.41519153 0.164735621 0.214167984 0.10550424 ## [143,] -1.756828494 -1.33047768 -1.044256299 -1.101423478 -1.92619045 ## [144,] 0.415061184 1.30880733 -0.075264677 -0.566665012 0.37396607 ## [145,] 1.954545624 1.93001875 1.785463911 1.855563932 1.90740374 ## [146,] 0.877945863 -0.92168609 0.923073830 1.308431949 0.94173802 ## [147,] -0.661538577 -1.38092728 -2.120610154 -1.205900468 -1.91076631 ## [148,] -1.695555593 -0.28544436 0.306330184 -0.007159513 0.10550424 ## [149,] -2.121059960 -1.11324100 -1.910937263 -0.880095982 -0.01610936 ## [150,] 0.914028416 0.36779450 0.490487605 -0.228487009 -0.16295759 ## [151,] 0.877945863 1.24814426 1.033699034 0.761299966 0.44348101 ## [152,] 0.378978631 0.39643012 0.060683748 -0.228487009 0.37396607 ## [153,] -0.333389664 0.57777156 -0.427196169 -1.439601481 -0.48351808 ## [154,] 1.954545624 -1.46085845 0.474332819 0.748926450 0.81813228 ## [155,] 0.531106794 0.13905605 0.115040616 -0.007159513 0.10550424 ## [156,] 0.975301317 0.25050797 -0.006216240 -0.007159513 -0.17838174 ## [157,] 0.378978631 -0.22478130 -0.577996108 0.214167984 -0.10886680 ## [158,] -0.506160318 0.95114386 0.442436851 0.761299966 0.65785204 ## [159,] -1.813553539 -1.86643001 -1.303236082 -0.775618992 -0.66121460 ## [160,] 0.534356890 0.20311095 -0.013273280 0.331018490 0.15959503 ## [161,] 0.281623178 1.21401329 0.201168633 0.539972470 -0.10886680 ## [162,] 0.187517820 -0.07885914 -0.138376165 -0.554291495 -0.12429095 ## [163,] 0.129495016 0.31700565 0.757265724 0.318644974 0.92631387 ## [164,] -0.600265676 -1.22241759 -0.805600843 -0.216113492 -1.66554698 ## [165,] 0.914028416 -0.31713256 -2.455209890 -2.847296416 -2.04019826 ## [166,] -0.473327862 0.31151029 0.782463543 -0.228487009 0.91088972 ## [167,] 1.260867485 1.30880733 1.450062167 0.214167984 0.37396607 ## [168,] 0.223600373 -1.19039014 -0.522435959 0.318644974 -0.87558564 ## [169,] -1.546677527 0.44687972 -0.272411305 0.097317477 -0.32323783 ## [170,] -0.022633147 -1.46408284 -1.471403111 -0.228487009 -0.32323783 ## [171,] -0.007193087 -0.82655280 -1.011397898 -0.332963999 -0.87558564 ## [172,] 0.877945863 -1.66422364 1.581973547 1.855563932 0.91088972 ## [173,] -0.408804865 0.05684954 0.382752145 0.539972470 0.15959503 ## [174,] 0.187517820 -0.63018105 -0.061931330 -0.880095982 -1.19813826 ## [175,] 0.628462247 1.18198584 0.856173841 0.097317477 1.15610905 ## [176,] 0.914028416 0.57987508 0.143656618 0.435495480 0.67327619 ## [177,] -0.061965795 0.19937986 0.030110763 1.855563932 1.12526076 ## [178,] 1.033324122 0.46258855 0.254328487 0.214167984 0.31987527 ## [179,] 0.339645983 -0.37036419 -0.460286320 0.214167984 0.53424631 ## [180,] 0.129495016 0.68827442 -0.899494357 -0.007159513 -0.10886680 ## [181,] 0.148185172 0.00639994 -0.005502935 -0.332963999 -0.10886680 ## [182,] -1.027067803 -0.64860320 -2.655626426 -1.857509440 -1.08995667 ## [183,] 1.380163191 1.93001875 0.919681702 0.097317477 0.15959503 ## [184,] 0.378978631 0.39643012 0.129890427 0.761299966 0.10550424 ## [185,] -1.254611166 -0.49752494 -1.269085071 -0.332963999 -1.65772862 ## [186,] 1.455578392 1.61941304 1.577843874 1.855563932 1.90740374 ## [187,] -0.603515772 -0.10749476 0.962851856 0.761299966 -0.05477601 ## [188,] -3.412358544 -1.23602289 -2.773824296 -3.068623912 -2.64663685 ## [189,] -1.254611166 -1.11839710 -1.643068277 -0.554291495 -0.87558564 ## [190,] 0.281623178 0.77319425 0.268303785 0.214167984 -0.10886680 ## [191,] 1.705062008 -0.09972407 -0.571172416 -1.765405967 1.08659411 ## [192,] 0.512416638 1.93001875 1.391487139 1.191581443 0.83554857 ## [193,] 0.917278512 1.30914659 1.438072294 -0.007159513 0.80270814 ## [194,] 1.954545624 -0.70471996 1.419275292 1.529759446 0.87222308 ## [195,] -0.600265676 -0.23770809 -0.462639676 -0.775618992 -0.89100978 ## [196,] 0.573689539 -1.44192960 -1.385896866 -0.880095982 -1.10538082 ## [197,] 1.954545624 0.61671938 1.785463911 0.748926450 1.90740374 ## [198,] 1.954545624 1.06809114 1.292000168 1.855563932 1.37048009 ## [199,] 0.129495016 -0.01236146 0.013135376 -0.554291495 -0.37732863 ## [200,] 1.166762128 0.06706301 0.244413893 -0.007159513 0.94173802 ## [201,] 0.378978631 0.39643012 0.613943774 -0.228487009 0.42805686 ## [202,] -0.217344054 0.18401029 0.561869357 0.435495480 1.05574582 ## [203,] -0.311449411 -1.87565646 -1.956525237 -1.427227964 0.37396607 ## [204,] -0.889081940 -2.33150581 -1.138193514 -1.322750974 -1.35841850 ## [205,] -0.061965795 0.41179969 -0.975839399 -0.880095982 -0.05477601 ## [206,] -1.235921010 -1.41573676 -0.617654474 0.214167984 -1.69639527 ## [207,] 0.570439442 0.47585459 0.536942509 1.308431949 0.35854192 ## [208,] -0.119988600 0.18434954 0.621340173 0.865776956 1.10201826 ## [209,] -0.676978637 0.51032482 0.088471901 0.214167984 -0.17838174 ## [210,] -0.736953778 0.21814126 0.155348097 1.203954959 0.91088972 ## [211,] -0.159321249 -0.94044750 -0.943118413 0.214167984 -0.64579046 ## [212,] -0.408804865 0.44382714 0.019265222 0.214167984 -0.39275277 ## [213,] -0.986437394 -0.67757807 -0.375638744 -0.996946488 -0.80607070 ## [214,] -0.849749292 -0.06009774 -0.456993538 1.087104453 0.15959503 ## [215,] -1.695555593 -1.03897263 -1.791543613 -0.111636502 -1.15947161 ## [216,] -2.483339089 -1.35700977 -1.071455183 -1.218273984 -0.21704839 ## [217,] -1.406739329 0.39643012 0.028145245 0.761299966 -0.12429095 ## [218,] 0.975301317 0.95114386 0.747620164 -0.880095982 0.87222308 ## [219,] 0.320955827 0.01966599 0.417113630 -0.332963999 0.58833710 ## [220,] -3.213099781 -3.50369987 -3.609178518 -1.974359947 -2.90728033 ## [221,] -0.408804865 -0.55041732 -0.140199147 -0.880095982 -0.71530540 ## [222,] 0.013449406 0.57404047 -0.169421078 0.318644974 0.44348101 ## [223,] -0.061965795 0.78646030 0.316224288 0.214167984 0.37396607 ## [224,] -0.752393838 -0.07851989 -0.159246061 -1.322750974 -0.80607070 ## [225,] 0.324205923 -0.82994463 -0.988063069 -1.101423478 0.62700375 ## [226,] 0.148185172 0.55527906 -0.415202881 -0.111636502 -0.10886680 ## [227,] -1.005127550 -0.62030683 -0.951344027 -2.091210453 -1.21356241 ## [228,] 0.262933022 1.21062145 0.705611920 0.435495480 0.85679893 ## [229,] -0.849749292 -0.28544436 -0.293281201 -1.869882957 -0.37732863 ## [230,] 1.954545624 1.93001875 1.577843874 1.855563932 1.90740374 ## [231,] -2.251247870 -1.01987197 -1.173200365 -1.101423478 -1.64230448 ## [232,] -0.369472216 -0.04133633 -0.184248015 0.097317477 0.01473894 ## [233,] -0.178011405 0.57064863 0.907350656 1.308431949 0.87222308 ## [234,] -0.560933027 -0.61447222 -0.371616448 0.761299966 -0.39275277 ## [235,] -0.603515772 -1.50537470 -2.200570703 -0.996946488 -1.14404747 ## [236,] -0.907772097 -0.79452535 -0.594086296 0.214167984 -0.96052472 ## [237,] -0.889081940 -1.24894968 -0.535353026 -0.228487009 -0.32323783 ## [238,] -0.119988600 -0.43652262 -1.037806554 -0.775618992 -0.37732863 ## [239,] -0.022633147 -0.36697236 -2.052351943 0.109690994 -1.55736539 ## [240,] -0.755643934 -0.27183906 -0.726800836 -0.228487009 0.37396607 ## [241,] -0.874939640 1.22388750 0.847393164 0.435495480 1.62351777 ## [242,] 1.954545624 -3.48897806 0.484818593 -3.068623912 0.13635253 ## [243,] -1.367406680 0.87104088 1.011030397 -0.880095982 1.47866167 ## [244,] 1.455578392 1.32756874 0.927575634 0.097317477 0.69651869 ## [245,] -1.601450236 -0.99089710 -0.202920982 -0.437440989 -0.60712381 ## [246,] -0.408804865 -0.99157561 -1.460929702 0.097317477 -1.71181942 ## [247,] 1.166762128 0.02821393 0.722904239 -0.880095982 0.65785204 ## [248,] 0.129495016 -0.25341691 0.065295315 0.761299966 0.15959503 ## [249,] -0.600265676 0.10458582 -0.779406075 -0.880095982 -0.53760887 ## [250,] 0.324205923 0.96990526 0.827027466 0.423121964 0.09008009 ## LOYAL ## [1,] -0.7404134392 ## [2,] 0.9996673370 ## [3,] -0.5790374090 ## [4,] 1.0853564905 ## [5,] 0.4626923930 ## [6,] 1.0425119138 ## [7,] 0.0594945246 ## [8,] -0.2521486311 ## [9,] -0.5361928323 ## [10,] -0.0742825511 ## [11,] 0.5055369697 ## [12,] -1.3361007960 ## [13,] -0.0690392057 ## [14,] 1.0853564905 ## [15,] 0.1134480059 ## [16,] 0.3282931035 ## [17,] 0.9996673370 ## [18,] -2.2275634725 ## [19,] 0.9509572011 ## [20,] -0.5573040138 ## [21,] 0.4409589977 ## [22,] -0.9393907007 ## [23,] -0.4017935428 ## [24,] 0.3552698442 ## [25,] -2.8502275700 ## [26,] 1.2567347975 ## [27,] -0.3589489661 ## [28,] 0.7626044303 ## [29,] 0.3711376803 ## [30,] 1.2567347975 ## [31,] 0.6880241813 ## [32,] -1.5831659797 ## [33,] 0.9568227602 ## [34,] 0.4626923930 ## [35,] -3.0761815722 ## [36,] -0.1118837824 ## [37,] -2.3889395026 ## [38,] -0.2943709940 ## [39,] 1.0525141907 ## [40,] 0.7954467301 ## [41,] 0.6880241813 ## [42,] 0.5853605641 ## [43,] 0.9996673370 ## [44,] 0.0007821118 ## [45,] 0.8594024884 ## [46,] -1.9698737982 ## [47,] -0.7562812753 ## [48,] 0.2795829676 ## [49,] -1.8460989994 ## [50,] -0.9335251415 ## [51,] 0.4626923930 ## [52,] -0.5790374090 ## [53,] 0.5107803151 ## [54,] 0.2367383908 ## [55,] -0.3161043893 ## [56,] -0.4017935428 ## [57,] -0.9933441820 ## [58,] 0.1510492373 ## [59,] 1.1282010673 ## [60,] -0.8102347566 ## [61,] -1.2715228240 ## [62,] 0.2795829676 ## [63,] 0.2208705547 ## [64,] 0.0542511792 ## [65,] -2.3191181852 ## [66,] -0.8689471694 ## [67,] -0.3917912659 ## [68,] -2.1207631375 ## [69,] 0.4568268338 ## [70,] -0.0690392057 ## [71,] 0.2208705547 ## [72,] -0.3806823613 ## [73,] -3.0761815722 ## [74,] -0.8102347566 ## [75,] -0.3213477347 ## [76,] -0.3859257067 ## [77,] 0.4409589977 ## [78,] 0.5912261232 ## [79,] 0.2367383908 ## [80,] 0.0225155070 ## [81,] 0.9879362186 ## [82,] 0.4139822570 ## [83,] 1.1710456440 ## [84,] -0.6060141497 ## [85,] -0.7186800439 ## [86,] -0.5414361777 ## [87,] 1.0425119138 ## [88,] 0.6769152768 ## [89,] 0.9996673370 ## [90,] 0.6551818815 ## [91,] 0.7626044303 ## [92,] -0.2462830719 ## [93,] 0.7097575766 ## [94,] -0.4616125833 ## [95,] -3.2047153025 ## [96,] 0.9668250372 ## [97,] 0.7038920174 ## [98,] -0.0690392057 ## [99,] -0.9387684869 ## [100,] 0.8106923524 ## [101,] -0.7615246206 ## [102,] -0.2304152358 ## [103,] 0.8224234708 ## [104,] -0.2093040543 ## [105,] 1.1710456440 ## [106,] -0.7134366985 ## [107,] 0.1082046605 ## [108,] 0.3981144209 ## [109,] 1.0853564905 ## [110,] 1.2138902208 ## [111,] 0.2426039500 ## [112,] 0.7097575766 ## [113,] 1.3424239511 ## [114,] 1.0853564905 ## [115,] 0.8165579116 ## [116,] 0.6980264582 ## [117,] 1.1710456440 ## [118,] 0.9996673370 ## [119,] 0.4726946699 ## [120,] 0.7626044303 ## [121,] -0.5790374090 ## [122,] -1.5666759298 ## [123,] 1.2567347975 ## [124,] 0.1938938141 ## [125,] -0.4716148602 ## [126,] -2.1148975783 ## [127,] 1.0525141907 ## [128,] -0.1817050998 ## [129,] -0.8325903657 ## [130,] -0.1118837824 ## [131,] 0.3770032395 ## [132,] -1.1705880482 ## [133,] 1.3424239511 ## [134,] 0.4198478162 ## [135,] -0.2245496766 ## [136,] 1.1282010673 ## [137,] 1.3424239511 ## [138,] 0.9879362186 ## [139,] 0.2208705547 ## [140,] 0.5912261232 ## [141,] 0.6880241813 ## [142,] 1.3424239511 ## [143,] -1.0890356124 ## [144,] 0.5912261232 ## [145,] 0.9996673370 ## [146,] -0.1553505730 ## [147,] -1.4974768262 ## [148,] -0.3161043893 ## [149,] -1.3361007960 ## [150,] 1.0853564905 ## [151,] 1.1710456440 ## [152,] 0.2367383908 ## [153,] 0.8652680475 ## [154,] 1.1710456440 ## [155,] -0.4446381196 ## [156,] -0.0531713696 ## [157,] -1.0039686727 ## [158,] 0.7626044303 ## [159,] -0.7351700938 ## [160,] 0.4568268338 ## [161,] 0.3770032395 ## [162,] 0.2156272093 ## [163,] 0.7795788940 ## [164,] -1.1588569298 ## [165,] 0.9996673370 ## [166,] -1.3361007960 ## [167,] 0.9996673370 ## [168,] 0.2954508037 ## [169,] 0.2367383908 ## [170,] 0.0594945246 ## [171,] -0.1717028229 ## [172,] 1.0425119138 ## [173,] 0.5266481512 ## [174,] -0.4563692380 ## [175,] -0.4669937287 ## [176,] -1.0790333355 ## [177,] 0.2789607538 ## [178,] 0.3711376803 ## [179,] 1.3424239511 ## [180,] -0.7562812753 ## [181,] -0.2245496766 ## [182,] -0.2573919765 ## [183,] 0.9568227602 ## [184,] -0.9822352774 ## [185,] -1.2234349019 ## [186,] 1.3424239511 ## [187,] 0.3711376803 ## [188,] -0.4505036788 ## [189,] -0.9387684869 ## [190,] 0.5055369697 ## [191,] 0.4626923930 ## [192,] -0.4241491519 ## [193,] -0.4241491519 ## [194,] 1.0425119138 ## [195,] -0.9933441820 ## [196,] -0.7345478800 ## [197,] 1.1282010673 ## [198,] 1.0425119138 ## [199,] 0.7097575766 ## [200,] 0.5266481512 ## [201,] -0.5426806053 ## [202,] 0.7626044303 ## [203,] -0.0320601881 ## [204,] 0.4081166978 ## [205,] 0.5912261232 ## [206,] -3.2047153025 ## [207,] 1.3424239511 ## [208,] 0.6451796046 ## [209,] 0.4568268338 ## [210,] 0.1134480059 ## [211,] -0.6329908904 ## [212,] -1.2938784331 ## [213,] -0.1388605231 ## [214,] 0.2854485268 ## [215,] -0.9874786228 ## [216,] -2.0450762609 ## [217,] 1.1710456440 ## [218,] 0.5325137104 ## [219,] 0.5325137104 ## [220,] -2.5820512049 ## [221,] -0.8959239101 ## [222,] 0.9568227602 ## [223,] 0.6610474407 ## [224,] -0.3161043893 ## [225,] -0.2838843033 ## [226,] 0.3013163629 ## [227,] -1.4276555087 ## [228,] 0.5853605641 ## [229,] -0.5473017369 ## [230,] 1.3424239511 ## [231,] -2.4048073387 ## [232,] -1.2504116425 ## [233,] 0.5483815465 ## [234,] -0.1817050998 ## [235,] -1.2932562193 ## [236,] -0.0590369288 ## [237,] 0.2854485268 ## [238,] -0.4781026332 ## [239,] -2.8989377060 ## [240,] -0.0109490066 ## [241,] 1.1710456440 ## [242,] 1.1710456440 ## [243,] 0.9996673370 ## [244,] 1.3424239511 ## [245,] 0.2308728316 ## [246,] -1.7023196468 ## [247,] -0.0050834474 ## [248,] 0.2308728316 ## [249,] -0.1875706590 ## [250,] 1.0425119138 ## attr(,&quot;scaled:center&quot;) ## EXP IMAGE QUAL VALUE SATISF ## -6.222800e-17 5.578871e-17 2.159384e-17 -7.310819e-17 -1.755263e-16 ## LOYAL ## -4.485301e-17 ## attr(,&quot;scaled:scale&quot;) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## 0.5231365 1.1889121 2.3487935 1.0512457 1.4909429 0.9860017 #Unstandardised solution # Function to get unstandardized LV unstandardized_LV &lt;- function(data, sempls_model){ # Save object to hold outer weights which will be scaled in loop outer_weights_resc &lt;- sempls_model$outer_weights # Find standard deviation of indicators std_indicators &lt;- unlist(lapply(data, FUN = sd)) for (lv in colnames(sempls_model$model$M)) { # Check estimation mode of LV mode &lt;- attr(sempls_model$model$blocks[[lv]], &quot;mode&quot;) if (mode == &quot;A&quot;) { tmp_indicator_names &lt;- sempls_model$model$measuremod[sempls_model$model$measuremod[,1]==lv,2] } else { tmp_indicator_names &lt;- sempls_model$model$measuremod[sempls_model$model$measuremod[,2]==lv,1] } tmp_weights &lt;- outer_weights_resc[tmp_indicator_names,lv] tmp_weights &lt;- tmp_weights/std_indicators[tmp_indicator_names] outer_weights_resc[tmp_indicator_names,lv] &lt;- tmp_weights/sum(tmp_weights) } # Data as matrix in order to perform matrix multiplication data_mat &lt;- as.matrix(data[,rownames(outer_weights_resc)]) unstandardized_LV &lt;- data_mat %*% outer_weights_resc return(unstandardized_LV) } # Unstandardized LV LV_unstand &lt;- unstandardized_LV(data = mobi, sempls_model = epsi) head(LV_unstand) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## [1,] 6.683309 5.553162 5.730157 2.595477 5.790953 5.915991 ## [2,] 9.683309 9.785231 9.741153 10.000000 9.299712 9.327926 ## [3,] 7.000000 6.131233 6.979907 7.000000 7.370261 6.232415 ## [4,] 7.095731 7.456748 7.361482 5.000000 10.000000 9.495944 ## [5,] 8.390344 8.411795 9.004820 6.000000 8.740522 8.275030 ## [6,] 8.806891 8.595372 9.043931 10.000000 7.370261 9.411935 plot(LV_unstand[,5:6]) pairs(LV_unstand,pch=19,cex=0.7,cex.axis=0.8,col.axis=&quot;gray70&quot;,gap=0.5) summary(LV_unstand) ## EXP IMAGE QUAL VALUE ## Min. : 3.187 Min. : 2.914 Min. : 3.199 Min. : 1.000 ## 1st Qu.: 6.683 1st Qu.: 6.686 1st Qu.: 7.152 1st Qu.: 5.579 ## Median : 7.525 Median : 7.538 Median : 7.788 Median : 7.000 ## Mean : 7.519 Mean : 7.483 Mean : 7.749 Mean : 6.609 ## 3rd Qu.: 8.299 3rd Qu.: 8.371 3rd Qu.: 8.658 3rd Qu.: 7.977 ## Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 ## SATISF LOYAL ## Min. : 3.461 Min. : 1.084 ## 1st Qu.: 6.811 1st Qu.: 6.304 ## Median : 7.640 Median : 7.820 ## Mean : 7.512 Mean : 7.368 ## 3rd Qu.: 8.385 3rd Qu.: 8.863 ## Max. :10.000 Max. :10.000 2.4.7 Revised code He made some changes, e.g., removing the question that was not relevant for the customer loyalty. In this setup the only difference is that I have made some modifications to the model formulation. I have deleted CUSL2 for the reasons mentioned above I have included IMAGE5 that I actually forgot to include in the first setup (it does not have any real influence I found out) I have modified the Quality construct. Now it only has 2 items So it is now a contruct that measures the perception of the service in an attempt to avoid it to be mixed up with SAT If you run this model, you will see that the problem with EXP still remains and the effect of EXP is still insignificant, so it could be an idea simply to drop EXP from the model. The construct IMAGE still have a relative low AVE, but the effects from IMAGE are now significant. library(boot) #Bootstrapping of standard deviations to outer loadings and path coefficients library(semPLS) library(psych) help(mobi) data(mobi) names(mobi) ## [1] &quot;CUEX1&quot; &quot;CUEX2&quot; &quot;CUEX3&quot; &quot;CUSA1&quot; &quot;CUSA2&quot; &quot;CUSA3&quot; &quot;CUSCO&quot; &quot;CUSL1&quot; &quot;CUSL2&quot; ## [10] &quot;CUSL3&quot; &quot;IMAG1&quot; &quot;IMAG2&quot; &quot;IMAG3&quot; &quot;IMAG4&quot; &quot;IMAG5&quot; &quot;PERQ1&quot; &quot;PERQ2&quot; &quot;PERQ3&quot; ## [19] &quot;PERQ4&quot; &quot;PERQ5&quot; &quot;PERQ6&quot; &quot;PERQ7&quot; &quot;PERV1&quot; &quot;PERV2&quot; #The dataset summary(mobi) ## CUEX1 CUEX2 CUEX3 CUSA1 ## Min. : 1.00 Min. : 1.000 Min. : 1.000 Min. : 4.000 ## 1st Qu.: 7.00 1st Qu.: 7.000 1st Qu.: 6.000 1st Qu.: 7.000 ## Median : 8.00 Median : 8.000 Median : 8.000 Median : 8.000 ## Mean : 7.58 Mean : 7.532 Mean : 7.424 Mean : 7.988 ## 3rd Qu.: 8.00 3rd Qu.: 9.000 3rd Qu.: 9.000 3rd Qu.: 9.000 ## Max. :10.00 Max. :10.000 Max. :10.000 Max. :10.000 ## CUSA2 CUSA3 CUSCO CUSL1 ## Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 1.000 ## 1st Qu.: 6.000 1st Qu.: 7.000 1st Qu.: 6.000 1st Qu.: 6.000 ## Median : 7.000 Median : 7.000 Median : 7.000 Median : 8.000 ## Mean : 7.128 Mean : 7.316 Mean : 7.068 Mean : 7.452 ## 3rd Qu.: 8.000 3rd Qu.: 8.000 3rd Qu.: 9.000 3rd Qu.:10.000 ## Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 ## CUSL2 CUSL3 IMAG1 IMAG2 ## Min. : 1.000 Min. : 1.000 Min. : 1.00 Min. : 1.00 ## 1st Qu.: 3.000 1st Qu.: 7.000 1st Qu.: 7.00 1st Qu.: 7.00 ## Median : 4.000 Median : 8.000 Median : 8.00 Median : 8.00 ## Mean : 4.988 Mean : 7.668 Mean : 7.64 Mean : 7.78 ## 3rd Qu.: 6.750 3rd Qu.:10.000 3rd Qu.: 9.00 3rd Qu.: 9.00 ## Max. :10.000 Max. :10.000 Max. :10.00 Max. :10.00 ## IMAG3 IMAG4 IMAG5 PERQ1 ## Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 2.000 ## 1st Qu.: 5.000 1st Qu.: 7.000 1st Qu.: 7.000 1st Qu.: 7.000 ## Median : 7.000 Median : 8.000 Median : 8.000 Median : 8.000 ## Mean : 6.744 Mean : 7.588 Mean : 7.932 Mean : 7.944 ## 3rd Qu.: 8.000 3rd Qu.: 9.000 3rd Qu.: 9.000 3rd Qu.: 9.000 ## Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 ## PERQ2 PERQ3 PERQ4 PERQ5 ## Min. : 1.000 Min. : 1.0 Min. : 1.000 Min. : 3.000 ## 1st Qu.: 6.000 1st Qu.: 7.0 1st Qu.: 7.000 1st Qu.: 7.000 ## Median : 7.000 Median : 8.0 Median : 8.000 Median : 8.000 ## Mean : 7.192 Mean : 7.7 Mean : 7.916 Mean : 7.872 ## 3rd Qu.: 8.000 3rd Qu.: 9.0 3rd Qu.: 9.000 3rd Qu.: 9.000 ## Max. :10.000 Max. :10.0 Max. :10.000 Max. :10.000 ## PERQ6 PERQ7 PERV1 PERV2 ## Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 1.000 ## 1st Qu.: 7.000 1st Qu.: 7.000 1st Qu.: 5.000 1st Qu.: 6.000 ## Median : 8.000 Median : 8.000 Median : 6.000 Median : 7.000 ## Mean : 7.776 Mean : 7.592 Mean : 6.156 Mean : 6.916 ## 3rd Qu.: 9.000 3rd Qu.: 9.000 3rd Qu.: 8.000 3rd Qu.: 8.000 ## Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 describe(mobi) #from library(psych) vars n mean sd median trimmed mad min max range skew kurtosis se CUEX1 1 250 7.580 1.621765 8 7.680 1.4826 1 10 9 -0.6632888 0.8289969 0.1025694 CUEX2 2 250 7.532 1.792716 8 7.665 1.4826 1 10 9 -0.7996766 1.4470764 0.1133813 CUEX3 3 250 7.424 2.102834 8 7.630 1.4826 1 10 9 -0.8489926 0.6154474 0.1329949 CUSA1 4 250 7.988 1.233671 8 8.030 1.4826 4 10 6 -0.2201919 0.1527890 0.0780242 CUSA2 5 250 7.128 1.765242 7 7.180 1.4826 1 10 9 -0.5471886 0.5579660 0.1116437 CUSA3 6 250 7.316 1.747099 7 7.405 1.4826 1 10 9 -0.6671235 0.9615690 0.1104962 CUSCO 7 250 7.068 2.274217 7 7.285 2.2239 1 10 9 -0.7099909 0.2384216 0.1438341 CUSL1 8 250 7.452 2.658565 8 7.860 2.9652 1 10 9 -0.9946527 0.0768705 0.1681424 CUSL2 9 250 4.988 2.839031 4 4.800 2.9652 1 10 9 0.5449058 -0.9063918 0.1795561 CUSL3 10 250 7.668 2.218437 8 7.955 2.9652 1 10 9 -1.0592863 0.8637662 0.1403063 IMAG1 11 250 7.640 1.699988 8 7.765 1.4826 1 10 9 -0.9014379 1.6887323 0.1075167 IMAG2 12 250 7.780 1.687302 8 7.930 1.4826 1 10 9 -0.7931040 0.9620206 0.1067143 IMAG3 13 250 6.744 2.131817 7 6.905 1.4826 1 10 9 -0.7930042 0.6570813 0.1348279 IMAG4 14 250 7.588 1.841690 8 7.755 1.4826 1 10 9 -0.9231357 1.3210435 0.1164787 IMAG5 15 250 7.932 1.557266 8 8.070 1.4826 1 10 9 -1.0694987 2.6817548 0.0984901 PERQ1 16 250 7.944 1.421600 8 8.040 1.4826 2 10 8 -0.7203890 1.1786363 0.0899099 PERQ2 17 250 7.192 1.891414 7 7.330 1.4826 1 10 9 -0.8186657 0.7287376 0.1196235 PERQ3 18 250 7.700 1.821888 8 7.885 1.4826 1 10 9 -0.9064463 0.8310284 0.1152263 PERQ4 19 250 7.916 1.651622 8 8.105 1.4826 1 10 9 -1.0704382 1.7901560 0.1044578 PERQ5 20 250 7.872 1.453294 8 7.970 1.4826 3 10 7 -0.6770313 0.5583053 0.0919144 PERQ6 21 250 7.776 1.629862 8 7.925 1.4826 1 10 9 -0.9743906 1.7870540 0.1030815 PERQ7 22 250 7.592 1.843674 8 7.775 1.4826 1 10 9 -0.9593571 1.0528448 0.1166042 PERV1 23 250 6.156 2.183284 6 6.260 1.4826 1 10 9 -0.3971992 -0.2142253 0.1380830 PERV2 24 250 6.916 1.842423 7 6.980 1.4826 1 10 9 -0.6445980 0.9714600 0.1165251 hist(mobi$CUSL1) hist(mobi$CUSL2) #The structural model EPSIsm &lt;- matrix(c(&quot;IMAGE&quot;, &quot;VALUE&quot;,&quot;EXP&quot;,&quot;VALUE&quot;,&quot;QUAL&quot;,&quot;VALUE&quot;,&quot;VALUE&quot;,&quot;SATISF&quot;,&quot;SATISF&quot;,&quot;LOYAL&quot;,&quot;IMAGE&quot;,&quot;LOYAL&quot;),byrow=TRUE,ncol=2) colnames(EPSIsm)=c(&quot;source&quot;,&quot;target&quot;) head(EPSIsm) ## source target ## [1,] &quot;IMAGE&quot; &quot;VALUE&quot; ## [2,] &quot;EXP&quot; &quot;VALUE&quot; ## [3,] &quot;QUAL&quot; &quot;VALUE&quot; ## [4,] &quot;VALUE&quot; &quot;SATISF&quot; ## [5,] &quot;SATISF&quot; &quot;LOYAL&quot; ## [6,] &quot;IMAGE&quot; &quot;LOYAL&quot; #The measurement model #IMAG5 has been added compared to the original setup (forgotten in original setup) EPSImm1 &lt;-matrix(c(&quot;IMAGE&quot;, &quot;IMAG1&quot;,&quot;IMAGE&quot;,&quot;IMAG2&quot;,&quot;IMAGE&quot;,&quot;IMAG3&quot;,&quot;IMAGE&quot;,&quot;IMAG4&quot;, &quot;IMAGE&quot;,&quot;IMAG5&quot;),byrow=TRUE,ncol=2) colnames(EPSImm1)=c(&quot;source&quot;,&quot;target&quot;) head(EPSImm1) ## source target ## [1,] &quot;IMAGE&quot; &quot;IMAG1&quot; ## [2,] &quot;IMAGE&quot; &quot;IMAG2&quot; ## [3,] &quot;IMAGE&quot; &quot;IMAG3&quot; ## [4,] &quot;IMAGE&quot; &quot;IMAG4&quot; ## [5,] &quot;IMAGE&quot; &quot;IMAG5&quot; EPSImm2 &lt;-matrix(c(&quot;EXP&quot;, &quot;CUEX1&quot;,&quot;EXP&quot;,&quot;CUEX2&quot;,&quot;EXP&quot;,&quot;CUEX3&quot;),byrow=TRUE,ncol=2) colnames(EPSImm2)=c(&quot;source&quot;,&quot;target&quot;) # &quot;Pure&quot; service quality EPSImm3 &lt;-matrix(c(&quot;QUAL&quot;,&quot;PERQ3&quot;,&quot;QUAL&quot;,&quot;PERQ4&quot;),byrow=TRUE,ncol=2) colnames(EPSImm3)=c(&quot;source&quot;,&quot;target&quot;) head(EPSImm3,7) ## source target ## [1,] &quot;QUAL&quot; &quot;PERQ3&quot; ## [2,] &quot;QUAL&quot; &quot;PERQ4&quot; EPSImm4 &lt;-matrix(c(&quot;VALUE&quot;, &quot;PERV1&quot;,&quot;VALUE&quot;,&quot;PERV2&quot;),byrow=TRUE,ncol=2) colnames(EPSImm4)=c(&quot;source&quot;,&quot;target&quot;) EPSImm5 &lt;-matrix(c(&quot;SATISF&quot;, &quot;CUSA1&quot;,&quot;SATISF&quot;,&quot;CUSA2&quot;,&quot;SATISF&quot;,&quot;CUSA3&quot;),byrow=TRUE,ncol=2) colnames(EPSImm5)=c(&quot;source&quot;,&quot;target&quot;) #Exclusion of CUSL2 EPSImm6 &lt;-matrix(c(&quot;LOYAL&quot;, &quot;CUSL1&quot;,&quot;LOYAL&quot;,&quot;CUSL3&quot;),byrow=TRUE,ncol=2) colnames(EPSImm6)=c(&quot;source&quot;,&quot;target&quot;) EPSImm &lt;-rbind(EPSImm1,EPSImm2,EPSImm3,EPSImm4,EPSImm5,EPSImm6) EPSImm ## source target ## [1,] &quot;IMAGE&quot; &quot;IMAG1&quot; ## [2,] &quot;IMAGE&quot; &quot;IMAG2&quot; ## [3,] &quot;IMAGE&quot; &quot;IMAG3&quot; ## [4,] &quot;IMAGE&quot; &quot;IMAG4&quot; ## [5,] &quot;IMAGE&quot; &quot;IMAG5&quot; ## [6,] &quot;EXP&quot; &quot;CUEX1&quot; ## [7,] &quot;EXP&quot; &quot;CUEX2&quot; ## [8,] &quot;EXP&quot; &quot;CUEX3&quot; ## [9,] &quot;QUAL&quot; &quot;PERQ3&quot; ## [10,] &quot;QUAL&quot; &quot;PERQ4&quot; ## [11,] &quot;VALUE&quot; &quot;PERV1&quot; ## [12,] &quot;VALUE&quot; &quot;PERV2&quot; ## [13,] &quot;SATISF&quot; &quot;CUSA1&quot; ## [14,] &quot;SATISF&quot; &quot;CUSA2&quot; ## [15,] &quot;SATISF&quot; &quot;CUSA3&quot; ## [16,] &quot;LOYAL&quot; &quot;CUSL1&quot; ## [17,] &quot;LOYAL&quot; &quot;CUSL3&quot; #The whole model EPSI &lt;- plsm(data=mobi,strucmod=EPSIsm,measuremod=EPSImm) EPSI[[&quot;D&quot;]] #The structural model (inner relations) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## EXP 0 0 0 1 0 0 ## IMAGE 0 0 0 1 0 1 ## QUAL 0 0 0 1 0 0 ## VALUE 0 0 0 0 1 0 ## SATISF 0 0 0 0 0 1 ## LOYAL 0 0 0 0 0 0 EPSI[[&quot;M&quot;]] #The measurement model ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 1 0 0 0 0 0 ## CUEX2 1 0 0 0 0 0 ## CUEX3 1 0 0 0 0 0 ## IMAG1 0 1 0 0 0 0 ## IMAG2 0 1 0 0 0 0 ## IMAG3 0 1 0 0 0 0 ## IMAG4 0 1 0 0 0 0 ## IMAG5 0 1 0 0 0 0 ## PERQ3 0 0 1 0 0 0 ## PERQ4 0 0 1 0 0 0 ## PERV1 0 0 0 1 0 0 ## PERV2 0 0 0 1 0 0 ## CUSA1 0 0 0 0 1 0 ## CUSA2 0 0 0 0 1 0 ## CUSA3 0 0 0 0 1 0 ## CUSL1 0 0 0 0 0 1 ## CUSL3 0 0 0 0 0 1 #Estimation of model epsi &lt;- sempls(model=EPSI,data=mobi) ## All 250 observations are valid. ## Converged after 5 iterations. ## Tolerance: 1e-07 ## Scheme: centroid epsi ## Path Estimate ## lam_1_1 EXP -&gt; CUEX1 0.79 ## lam_1_2 EXP -&gt; CUEX2 0.58 ## lam_1_3 EXP -&gt; CUEX3 0.68 ## lam_2_1 IMAGE -&gt; IMAG1 0.74 ## lam_2_2 IMAGE -&gt; IMAG2 0.57 ## lam_2_3 IMAGE -&gt; IMAG3 0.61 ## lam_2_4 IMAGE -&gt; IMAG4 0.79 ## lam_2_5 IMAGE -&gt; IMAG5 0.73 ## lam_3_1 QUAL -&gt; PERQ3 0.90 ## lam_3_2 QUAL -&gt; PERQ4 0.85 ## lam_4_1 VALUE -&gt; PERV1 0.90 ## lam_4_2 VALUE -&gt; PERV2 0.94 ## lam_5_1 SATISF -&gt; CUSA1 0.79 ## lam_5_2 SATISF -&gt; CUSA2 0.84 ## lam_5_3 SATISF -&gt; CUSA3 0.86 ## lam_6_1 LOYAL -&gt; CUSL1 0.84 ## lam_6_2 LOYAL -&gt; CUSL3 0.91 ## beta_1_4 EXP -&gt; VALUE 0.11 ## beta_2_4 IMAGE -&gt; VALUE 0.30 ## beta_3_4 QUAL -&gt; VALUE 0.26 ## beta_4_5 VALUE -&gt; SATISF 0.61 ## beta_2_6 IMAGE -&gt; LOYAL 0.21 ## beta_5_6 SATISF -&gt; LOYAL 0.51 # Evaluation of estimated model plsLoadings(epsi) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 0.79 . . . . . ## CUEX2 0.58 . . . . . ## CUEX3 0.68 . . . . . ## IMAG1 . 0.74 . . . . ## IMAG2 . 0.57 . . . . ## IMAG3 . 0.61 . . . . ## IMAG4 . 0.79 . . . . ## IMAG5 . 0.73 . . . . ## PERQ3 . . 0.90 . . . ## PERQ4 . . 0.85 . . . ## PERV1 . . . 0.90 . . ## PERV2 . . . 0.94 . . ## CUSA1 . . . . 0.79 . ## CUSA2 . . . . 0.84 . ## CUSA3 . . . . 0.86 . ## CUSL1 . . . . . 0.84 ## CUSL3 . . . . . 0.91 #Composite reliability dgrho(epsi) ## Dillon-Goldstein&#39;s rho reflective MVs ## EXP 0.73 3 ## IMAGE 0.82 5 ## QUAL 0.86 2 ## VALUE 0.92 2 ## SATISF 0.87 3 ## LOYAL 0.87 2 # AVE is average of communality for a construct communality(epsi) ## communality reflective MVs ## EXP 0.47 3 ## IMAGE 0.48 5 ## QUAL 0.76 2 ## VALUE 0.85 2 ## SATISF 0.69 3 ## LOYAL 0.77 2 ## ## Average communality: 0.63 ### check for discriminant validity using loadings. If the relative difference between a loading and a cross-loading # is more than 0.2 the the cross-loading will be printed l &lt;-plsLoadings(epsi) print(l, type=&quot;discriminant&quot;, cutoff=0.5, reldiff=0.2) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 0.79 . . . . . ## CUEX2 0.58 . . . . . ## CUEX3 0.68 . . . . . ## IMAG1 . 0.74 . . . . ## IMAG2 . 0.57 . . . . ## IMAG3 . 0.61 . . . . ## IMAG4 . 0.79 . . . . ## IMAG5 . 0.73 . . . . ## PERQ3 . . 0.90 . . . ## PERQ4 . . 0.85 . . . ## PERV1 . . . 0.90 . . ## PERV2 . . . 0.94 . . ## CUSA1 . . . . 0.79 . ## CUSA2 . . . . 0.84 . ## CUSA3 . . . . 0.86 . ## CUSL1 . . . . . 0.84 ## CUSL3 . . . . . 0.91 pC &lt;- pathCoeff(epsi) print(pC,abbreviate=TRUE,minlength=3) ## EXP IMA QUA VAL SAT LOY ## EXP . . . 0.11 . . ## IMA . . . 0.30 . 0.21 ## QUA . . . 0.26 . . ## VAL . . . . 0.61 . ## SAT . . . . . 0.51 ## LOY . . . . . . tE&lt;- totalEffects(epsi) print(tE,abbreviate=TRUE,minlength=3) ## EXP IMA QUA VAL SAT LOY ## EXP . . . 0.106 0.065 0.033 ## IMA . . . 0.296 0.180 0.298 ## QUA . . . 0.260 0.158 0.081 ## VAL . . . . 0.610 0.312 ## SAT . . . . . 0.512 ## LOY . . . . . . #R2 rSquared(epsi) ## R-squared ## EXP . ## IMAGE . ## QUAL . ## VALUE 0.32 ## SATISF 0.37 ## LOYAL 0.45 we still see that there are low \\(R^2\\) #Bootstrapping - estimation of standard errors set.seed(123) epsiBoot &lt;- bootsempls(epsi,nboot=500,start=&quot;ones&quot;,verbose=FALSE) epsiBoot ## Call: bootsempls(object = epsi, nboot = 500, start = &quot;ones&quot;, verbose = FALSE) ## ## Estimate Bias Std.Error ## EXP -&gt; CUEX1 0.787 -1.55e-02 0.07163 ## EXP -&gt; CUEX2 0.583 -1.91e-02 0.14742 ## EXP -&gt; CUEX3 0.682 -3.18e-03 0.10264 ## IMAGE -&gt; IMAG1 0.737 -4.02e-03 0.04051 ## IMAGE -&gt; IMAG2 0.569 -4.89e-03 0.06784 ## IMAGE -&gt; IMAG3 0.607 -2.51e-03 0.06196 ## IMAGE -&gt; IMAG4 0.790 -1.31e-03 0.03987 ## IMAGE -&gt; IMAG5 0.730 -2.54e-03 0.04754 ## QUAL -&gt; PERQ3 0.896 1.46e-05 0.02078 ## QUAL -&gt; PERQ4 0.846 -2.84e-03 0.03756 ## VALUE -&gt; PERV1 0.904 -2.08e-03 0.02118 ## VALUE -&gt; PERV2 0.938 5.94e-04 0.00715 ## SATISF -&gt; CUSA1 0.785 -3.18e-03 0.03452 ## SATISF -&gt; CUSA2 0.845 3.41e-04 0.02307 ## SATISF -&gt; CUSA3 0.864 1.32e-03 0.01645 ## LOYAL -&gt; CUSL1 0.837 -3.22e-03 0.03913 ## LOYAL -&gt; CUSL3 0.914 -6.55e-05 0.01073 ## EXP -&gt; VALUE 0.106 1.60e-02 0.08455 ## IMAGE -&gt; VALUE 0.296 7.13e-04 0.10320 ## QUAL -&gt; VALUE 0.260 -8.88e-03 0.09773 ## VALUE -&gt; SATISF 0.610 -1.23e-03 0.05526 ## IMAGE -&gt; LOYAL 0.206 1.08e-02 0.07371 ## SATISF -&gt; LOYAL 0.512 -6.88e-03 0.08686 We see that again the relationship between EXP and VALUE is still very low, perhaps one should drop the expectations. # Structural model coefficients parallelplot(epsiBoot,reflinesAt=0, alpha=0.8, varnames=attr(epsiBoot$t,&quot;path&quot;)[18:23]) #Constructing observations plsWeights(epsi) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 0.56 . . . . . ## CUEX2 0.34 . . . . . ## CUEX3 0.52 . . . . . ## IMAG1 . 0.29 . . . . ## IMAG2 . 0.23 . . . . ## IMAG3 . 0.25 . . . . ## IMAG4 . 0.36 . . . . ## IMAG5 . 0.30 . . . . ## PERQ3 . . 0.62 . . . ## PERQ4 . . 0.52 . . . ## PERV1 . . . 0.49 . . ## PERV2 . . . 0.60 . . ## CUSA1 . . . . 0.35 . ## CUSA2 . . . . 0.38 . ## CUSA3 . . . . 0.47 . ## CUSL1 . . . . . 0.48 ## CUSL3 . . . . . 0.65 #Standardised observations for (constructs) epsi$factor_scores ## EXP IMAGE QUAL VALUE SATISF ## [1,] -0.658301221 -1.949363685 -1.55684926 -2.195324010 -1.32184667 ## [2,] 1.704988931 1.630829659 1.44521531 1.855937388 1.37237187 ## [3,] -0.408794091 -1.010377934 -1.21411966 0.215032279 -0.10859055 ## [4,] -0.333710896 0.392506075 0.81475730 -0.878904460 1.90778446 ## [5,] 0.686617734 0.787436333 0.75975612 -0.331936091 0.94394863 ## [6,] 1.014608854 0.893700188 1.10248572 1.855937388 -0.10859055 ## [7,] -1.153914686 -1.010258477 0.15679870 -0.229933342 0.37332737 ## [8,] -2.790983560 0.446388894 -0.21343148 -0.229933342 -0.60811280 ## [9,] -0.408794091 -0.331570085 -0.87139007 -0.229933342 -0.39390118 ## [10,] -1.059932174 -0.264864511 0.12929811 -0.776901711 -1.35773701 ## [11,] -0.350652780 -0.076077609 0.12929811 0.215032279 0.24781908 ## [12,] 1.704988931 0.480052279 0.12929811 -0.229933342 0.10562107 ## [13,] 0.245744786 0.275410705 -0.21343148 -0.554418901 0.10562107 ## [14,] 1.954496061 1.957050821 1.44521531 0.539517838 0.94394863 ## [15,] -0.736785211 0.201958486 0.44452712 -1.546352891 -0.32280217 ## [16,] -0.603560704 0.759906758 0.78725671 -0.897381772 -0.03749154 ## [17,] -0.120044967 0.973228519 1.12998630 0.743523337 0.39093170 ## [18,] -1.102574963 -2.109522639 -0.89889066 0.215032279 -1.07242639 ## [19,] 0.877983552 0.949355974 0.78725671 0.437515089 0.92634429 ## [20,] -1.352082093 -0.909519517 -0.21343148 0.215032279 -0.16208522 ## [21,] -1.449465400 0.349903599 0.12929811 0.215032279 0.58753899 ## [22,] 0.126061369 -0.108959222 0.49952829 -0.878904460 -1.57194864 ## [23,] -0.791525728 -0.479284644 -0.21343148 -0.229933342 -0.92931377 ## [24,] -0.159286962 -0.035737557 0.12929811 1.308969018 0.37332737 ## [25,] -0.061903655 -0.614958218 -1.84457768 -1.323870081 -2.16085591 ## [26,] 0.761700930 1.957050821 1.44521531 0.762000648 1.47936122 ## [27,] 0.378969293 0.830269364 0.12929811 0.215032279 -0.53701379 ## [28,] 0.877983552 -2.399772575 -1.84457768 1.855937388 -0.53701379 ## [29,] -0.755684527 -0.227556375 -0.18593089 0.539517838 -0.10859055 ## [30,] 0.913824753 1.722398973 1.10248572 1.410971767 1.64007817 ## [31,] -0.564318709 0.092650436 0.44452712 1.086486208 0.65863800 ## [32,] -0.658301221 -0.479284644 -0.21343148 -0.878904460 -1.07242639 ## [33,] -1.027491768 1.028884304 -0.13092972 0.743523337 -0.28691183 ## [34,] 0.339727298 0.606667305 1.44521531 0.437515089 0.55233033 ## [35,] 0.997666969 -0.889950117 -1.84457768 -3.066777939 -2.85698545 ## [36,] 1.571764424 1.201866014 -0.44615872 0.437515089 -0.39390118 ## [37,] -0.101145650 -0.715207720 -0.24093206 -1.323870081 -1.46472636 ## [38,] -0.560917915 -0.505650167 -0.21343148 -0.656421650 -0.53701379 ## [39,] -0.061903655 0.208215673 1.10248572 1.308969018 0.99744330 ## [40,] -0.159286962 -0.343121612 -0.21343148 0.317035028 -0.10859055 ## [41,] 1.954496061 0.364487041 1.44521531 0.762000648 1.42586654 ## [42,] 0.476352599 0.429921387 0.47202770 0.437515089 0.42682204 ## [43,] -1.601589223 1.207880366 1.44521531 1.410971767 1.37237187 ## [44,] -0.061903655 -0.978487516 0.12929811 -1.101387271 -0.12619488 ## [45,] 1.069349371 1.722398973 1.44521531 1.410971767 1.90778446 ## [46,] 0.877983552 1.067463669 1.12998630 0.437515089 0.65863800 ## [47,] 0.686617734 0.134994008 0.47202770 -1.546352891 -0.32280217 ## [48,] -0.716442532 -0.283701618 -0.18593089 0.539517838 0.15911575 ## [49,] -0.680601332 -2.583372314 -2.58503803 -1.648355640 -1.91075394 ## [50,] -0.673799743 -1.133524987 1.10248572 -2.742292379 -1.30424234 ## [51,] 0.877983552 0.875471995 0.49952829 -0.452416152 0.80175061 ## [52,] -0.697543216 -0.110719909 -0.52866048 -0.554418901 -0.60811280 ## [53,] 0.054378967 -0.188977010 0.10179753 0.762000648 0.39093170 ## [54,] 1.072750165 -0.808052186 0.75975612 0.437515089 -0.10859055 ## [55,] -0.907808351 -0.344882299 -0.21343148 0.437515089 -0.32280217 ## [56,] 0.226845470 -0.556270516 0.15679870 0.437515089 -0.10859055 ## [57,] -0.542018598 -0.240490228 -1.50184808 -0.878904460 -2.30328684 ## [58,] 0.090220168 1.023848868 1.44521531 0.762000648 0.65863800 ## [59,] 1.072750165 1.048453705 0.44452712 1.086486208 1.47936122 ## [60,] -1.063332968 -0.283701618 -0.18593089 -0.331936091 -0.10859055 ## [61,] -0.829324361 -0.887527116 -1.18661908 -1.425872830 -1.58955297 ## [62,] -0.791525728 -0.328295335 0.15679870 0.215032279 0.17672008 ## [63,] 0.784001040 0.217261941 0.12929811 0.984483459 0.15911575 ## [64,] -0.506177398 -1.075203365 -2.84526586 -0.554418901 -0.82232442 ## [65,] 0.534493911 -0.607429802 0.12929811 -0.007450531 -0.66160747 ## [66,] 0.129462163 0.428160701 0.44452712 -1.221867332 -0.09098622 ## [67,] 0.761700930 -0.471093914 0.75975612 0.215032279 0.10562107 ## [68,] 0.378969293 -0.479284644 -0.18593089 -0.878904460 -2.76760043 ## [69,] 0.093620962 0.155591803 -0.21343148 -0.007450531 0.15911575 ## [70,] 0.187603475 -0.153552939 0.44452712 -0.331936091 -0.60811280 ## [71,] -0.159286962 0.368740707 0.44452712 0.215032279 0.44442638 ## [72,] 1.954496061 0.595617516 1.10248572 0.215032279 0.78323168 ## [73,] 0.512193800 0.845342264 1.44521531 -3.066777939 1.90778446 ## [74,] -1.001790863 -1.216414114 -1.21411966 0.215032279 -1.14352539 ## [75,] 1.260715189 -0.306302934 -0.18593089 0.215032279 -1.67917091 ## [76,] -0.519718488 -1.875775941 -0.87139007 -1.425872830 -0.87581910 ## [77,] 1.416239807 -0.287465826 0.12929811 0.762000648 -0.07270021 ## [78,] -1.637430424 -0.669342775 -1.55684926 -0.878904460 -0.37629685 ## [79,] -0.253269474 -0.031973349 -0.21343148 -0.331936091 -0.32280217 ## [80,] -0.217428273 -0.501885959 -0.21343148 0.215032279 -0.17968956 ## [81,] 0.975366859 0.659048341 1.10248572 0.215032279 0.58753899 ## [82,] -1.001790863 0.276681934 0.12929811 0.539517838 -0.48351912 ## [83,] 0.378969293 0.545486624 1.12998630 0.539517838 1.40826221 ## [84,] -0.658301221 -0.539194095 0.12929811 -0.007450531 -0.59050847 ## [85,] -0.003762344 -0.443860573 -0.18593089 -0.007450531 -0.37629685 ## [86,] -1.442663811 0.255841305 0.78725671 -0.127930592 0.03452207 ## [87,] 1.224873988 0.597378203 0.12929811 1.086486208 1.12295158 ## [88,] 1.108591366 0.775884807 0.78725671 1.308969018 0.42682204 ## [89,] 0.187603475 0.120776646 -0.18593089 0.539517838 0.65863800 ## [90,] -0.408794091 0.433685595 0.81475730 0.539517838 0.30222836 ## [91,] 0.378969293 1.009265426 1.10248572 0.762000648 0.67624233 ## [92,] -0.061903655 -0.289226513 -0.55616107 -0.007450531 0.10562107 ## [93,] 0.320827982 0.720229021 0.47202770 0.539517838 0.85524529 ## [94,] -0.813825839 -0.134226373 0.75975612 0.113029530 -0.17968956 ## [95,] -0.791525728 -2.705745954 -3.50322446 -1.425872830 -3.10640573 ## [96,] 0.531093116 1.457847949 0.49952829 1.410971767 0.90873996 ## [97,] -0.195128162 0.001081121 0.10179753 0.984483459 0.64103367 ## [98,] -0.350652780 -0.596610568 -0.52866048 0.215032279 -0.32280217 ## [99,] -1.099174169 -0.910008975 -0.52866048 -0.229933342 -0.10859055 ## [100,] 0.090220168 1.029616597 1.44521531 0.215032279 0.85524529 ## [101,] -0.195128162 -0.019150594 0.12929811 0.539517838 -0.53701379 ## [102,] 0.129462163 0.215501254 -0.18593089 0.215032279 0.10562107 ## [103,] 0.187603475 -0.351386109 -0.58366165 0.539517838 0.37332737 ## [104,] -0.658301221 -0.805505806 -0.50115990 -0.007450531 0.51552538 ## [105,] 0.284986781 1.139414105 0.44452712 0.762000648 0.72973700 ## [106,] -1.116116053 -1.415588490 0.15679870 -0.878904460 -1.35773701 ## [107,] 1.954496061 -0.663143286 -0.26843265 1.206966269 0.94394863 ## [108,] 0.032078857 0.271646498 0.12929811 -0.674898962 0.64103367 ## [109,] -0.516317693 1.086127920 -1.39184574 0.762000648 0.15911575 ## [110,] 0.570335111 0.407320072 0.12929811 0.539517838 1.47936122 ## [111,] -0.369552096 -0.806900413 -0.21343148 -0.229933342 0.39093170 ## [112,] 0.975366859 1.066681898 0.10179753 0.864003398 -0.05509587 ## [113,] 1.358098495 0.701391913 0.78725671 0.984483459 1.14055592 ## [114,] 0.877983552 0.992797919 1.44521531 0.762000648 1.47936122 ## [115,] 0.414810494 1.144449542 1.44521531 0.762000648 0.90873996 ## [116,] 0.220043881 0.778134951 -2.18730727 -1.546352891 -1.07242639 ## [117,] -1.751755614 1.013029633 0.78725671 1.855937388 1.33716321 ## [118,] 1.954496061 1.957050821 0.41702653 0.762000648 0.72973700 ## [119,] 1.571764424 0.854631366 1.12998630 1.308969018 0.64103367 ## [120,] 0.823243035 1.611087402 0.12929811 0.762000648 1.33716321 ## [121,] 1.224873988 0.657287654 0.44452712 0.539517838 0.28394235 ## [122,] 0.226845470 -0.826346435 -1.15911849 -1.870838450 -1.49993503 ## [123,] 1.455481802 1.957050821 1.44521531 1.855937388 1.90778446 ## [124,] -0.427693408 -1.723392025 -0.87139007 -0.554418901 -1.30424234 ## [125,] -0.639401905 -0.556270516 -2.47503569 -0.007450531 -0.53701379 ## [126,] -0.253269474 -2.846528731 0.10179753 -0.878904460 -0.67921181 ## [127,] 1.513623113 1.474924370 0.78725671 1.308969018 1.35476754 ## [128,] -0.849667039 -0.399266856 0.12929811 -0.554418901 0.10562107 ## [129,] -0.061903655 -0.575107624 0.15679870 -0.878904460 -0.87581910 ## [130,] -0.159286962 0.332827178 0.12929811 0.215032279 -0.10859055 ## [131,] 0.498652710 -0.844866942 -0.89889066 0.743523337 -0.16140354 ## [132,] 0.148361479 -0.921963912 -0.87139007 -1.425872830 -1.35773701 ## [133,] -2.025520287 -1.777287124 -2.15980668 -0.878904460 -1.64304764 ## [134,] 0.457453283 1.219921351 1.44521531 1.206966269 -0.19729389 ## [135,] -0.907808351 0.099327102 0.12929811 -0.007450531 -0.10859055 ## [136,] -0.827366929 0.346139391 0.15679870 0.762000648 0.37332737 ## [137,] 1.954496061 1.957050821 1.44521531 1.855937388 1.90778446 ## [138,] 0.129462163 0.079827680 0.81475730 0.317035028 0.94394863 ## [139,] 0.917225547 0.968193082 1.12998630 -0.470893463 1.64007817 ## [140,] 0.570335111 0.326812826 0.12929811 1.188488957 0.37332737 ## [141,] 0.284986781 1.470670704 1.12998630 0.335512340 0.92634429 ## [142,] 0.129462163 0.349903599 0.44452712 0.215032279 0.10562107 ## [143,] -1.757113841 -1.255482936 -0.87139007 -1.101387271 -1.92835827 ## [144,] 0.414810494 1.069956648 -0.21343148 -0.572896213 0.37332737 ## [145,] 1.954496061 1.957050821 1.44521531 1.855937388 1.90778446 ## [146,] 0.877983552 -0.925497566 0.78725671 1.308969018 0.94394863 ## [147,] -0.661702015 -1.678308851 -2.50253627 -1.203390020 -1.91075394 ## [148,] -1.695571735 -0.406552437 0.12929811 -0.007450531 0.10562107 ## [149,] -2.120946161 -1.455076793 -0.89889066 -0.878904460 -0.01920553 ## [150,] 0.913824753 0.311324234 -0.52866048 -0.229933342 -0.16208522 ## [151,] 0.877983552 1.025852389 1.12998630 0.762000648 0.44442638 ## [152,] 0.378969293 0.332827178 0.12929811 -0.229933342 0.37332737 ## [153,] -0.333710896 0.857490425 -0.89889066 -1.444350142 -0.48351912 ## [154,] 1.954496061 -0.809405673 1.44521531 0.743523337 0.81935494 ## [155,] 0.531093116 -0.060099559 0.47202770 -0.007450531 0.10562107 ## [156,] 0.975366859 0.215501254 0.12929811 -0.007450531 -0.17968956 ## [157,] 0.378969293 -0.554266995 -2.44753510 0.215032279 -0.10859055 ## [158,] -0.506177398 0.971957290 0.12929811 0.762000648 0.65863800 ## [159,] -1.813297720 -1.877536627 -0.55616107 -0.776901711 -0.66160747 ## [160,] 0.534493911 0.176921889 -0.52866048 0.335512340 0.15911575 ## [161,] 0.281585987 0.992797919 0.12929811 0.539517838 -0.10859055 ## [162,] 0.187603475 -0.245122254 0.12929811 -0.554418901 -0.12619488 ## [163,] 0.129462163 0.463465315 0.78725671 0.317035028 0.92634429 ## [164,] -0.600159910 -1.556436948 0.10179753 -0.211456030 -1.66133366 ## [165,] 0.913824753 0.146792158 -1.87207826 -2.844295129 -2.03626222 ## [166,] -0.473736991 0.643732606 0.07429694 -0.229933342 0.90873996 ## [167,] 1.260715189 1.453594283 1.44521531 0.215032279 0.37332737 ## [168,] 0.223444675 -0.958379180 0.12929811 0.317035028 -0.87581910 ## [169,] -1.546848706 0.563834274 0.12929811 0.094552218 -0.32280217 ## [170,] -0.022661660 -1.357439726 -1.18661908 -0.229933342 -0.32280217 ## [171,] -0.007163138 -0.847849379 -0.52866048 -0.331936091 -0.87581910 ## [172,] 0.877983552 -0.941706169 0.81475730 1.855937388 0.90873996 ## [173,] -0.408794091 0.059106508 0.44452712 0.539517838 0.15911575 ## [174,] 0.187603475 -0.883153994 -0.52866048 -0.878904460 -1.19702007 ## [175,] 0.628476423 1.353834238 0.78725671 0.094552218 1.15816025 ## [176,] 0.913824753 0.484305944 0.12929811 0.437515089 0.67624233 ## [177,] -0.061903655 0.558971695 0.78725671 1.855937388 1.12295158 ## [178,] 1.033508170 0.196664146 0.15679870 0.215032279 0.31983269 ## [179,] 0.339727298 -0.479284644 -0.52866048 0.215032279 0.53404432 ## [180,] 0.129462163 0.375660208 -2.24230844 -0.007450531 -0.10859055 ## [181,] 0.148361479 0.211737046 -0.18593089 -0.331936091 -0.10859055 ## [182,] -1.027491768 -0.516103322 -2.15980668 -1.852361139 -1.09003072 ## [183,] 1.380398606 1.573413186 0.44452712 0.094552218 0.15911575 ## [184,] 0.378969293 0.332827178 0.12929811 0.762000648 0.10562107 ## [185,] -1.254698787 -0.579534146 -1.15911849 -0.331936091 -1.66065197 ## [186,] 1.455481802 1.705322552 1.44521531 1.855937388 1.90778446 ## [187,] -0.603560704 -0.074806380 0.75975612 0.762000648 -0.05509587 ## [188,] -3.413082031 -0.986994847 -2.53003686 -3.066777939 -2.64277382 ## [189,] -1.254698787 -0.698863592 -1.18661908 -0.554418901 -0.87581910 ## [190,] 0.281585987 0.640211233 0.12929811 0.215032279 -0.10859055 ## [191,] 1.704988931 0.110985805 -1.10411732 -1.768835701 1.08706124 ## [192,] 0.512193800 1.957050821 1.44521531 1.188488957 0.83672635 ## [193,] 0.917225547 1.070446106 1.12998630 -0.007450531 0.80175061 ## [194,] 1.954496061 -0.155174179 1.10248572 1.531451828 0.87284962 ## [195,] -0.600159910 -0.175664797 -0.21343148 -0.776901711 -0.89342343 ## [196,] 0.573735906 -1.147446115 -0.21343148 -0.878904460 -1.10763505 ## [197,] 1.954496061 0.901117506 1.44521531 0.743523337 1.90778446 ## [198,] 1.954496061 0.875471995 0.75975612 1.855937388 1.37237187 ## [199,] 0.129462163 -0.188977010 -0.18593089 -0.554418901 -0.37629685 ## [200,] 1.166732677 0.447660123 0.10179753 -0.007450531 0.94394863 ## [201,] 0.378969293 -0.050810457 0.47202770 -0.229933342 0.42682204 ## [202,] -0.217428273 0.159356010 0.78725671 0.437515089 1.05185258 ## [203,] -0.311410785 -1.507038349 -0.81638890 -1.425872830 0.37332737 ## [204,] -0.888909034 -2.066932444 -0.87139007 -1.323870081 -1.35773701 ## [205,] -0.061903655 0.348805227 -0.52866048 -0.878904460 -0.05509587 ## [206,] -1.235799470 -1.712342236 0.15679870 0.215032279 -1.69654232 ## [207,] 0.570335111 0.585826676 0.78725671 1.308969018 0.35572304 ## [208,] -0.120044967 0.159845468 -0.52866048 0.864003398 1.10466557 ## [209,] -0.677200537 0.619370604 -0.21343148 0.215032279 -0.17968956 ## [210,] -0.736785211 -0.383045972 0.81475730 1.206966269 0.90873996 ## [211,] -0.159286962 -0.750755170 -1.21411966 0.215032279 -0.64400314 ## [212,] -0.408794091 0.371406543 -0.21343148 0.215032279 -0.39390118 ## [213,] -0.986292341 -0.538095723 -0.89889066 -0.999384521 -0.80472009 ## [214,] -0.849667039 -0.228045833 -0.87139007 1.086486208 0.15911575 ## [215,] -1.695571735 -1.021320546 -0.50115990 -0.109453281 -1.16112972 ## [216,] -2.483335119 -0.882895090 -1.24162025 -1.221867332 -0.21557990 ## [217,] -1.406822610 0.716464813 -0.18593089 0.762000648 -0.12619488 ## [218,] 0.975366859 0.780138472 0.44452712 -0.878904460 0.87284962 ## [219,] 0.320827982 0.025443123 0.12929811 -0.331936091 0.58753899 ## [220,] -3.212957192 -3.399750081 -3.50322446 -1.972841200 -2.90979844 ## [221,] -0.408794091 -1.013302673 0.75975612 -0.878904460 -0.71510215 ## [222,] 0.013179540 0.664083777 0.75975612 0.317035028 0.44442638 ## [223,] -0.061903655 0.837554945 1.10248572 0.215032279 0.37332737 ## [224,] -0.752283733 -0.052813978 -0.21343148 -1.323870081 -0.80472009 ## [225,] 0.324228776 -0.657128933 -0.52866048 -1.101387271 0.62342933 ## [226,] 0.148361479 0.838826173 0.12929811 -0.109453281 -0.10859055 ## [227,] -1.005191657 -0.686908654 -0.52866048 -2.093321261 -1.21462440 ## [228,] 0.262686670 1.375337182 0.44452712 0.437515089 0.85524529 ## [229,] -0.849667039 -0.406552437 -0.21343148 -1.870838450 -0.37629685 ## [230,] 1.954496061 1.957050821 1.44521531 1.855937388 1.90778446 ## [231,] -2.250769874 -1.003754667 -1.55684926 -1.101387271 -1.64304764 ## [232,] -0.369552096 -0.019150594 0.12929811 0.094552218 0.01600313 ## [233,] -0.178186278 0.662985405 1.10248572 1.308969018 0.87284962 ## [234,] -0.560917915 -0.483048852 -0.87139007 0.762000648 -0.39390118 ## [235,] -0.603560704 -2.545714167 -2.21480785 -0.999384521 -1.14352539 ## [236,] -0.907808351 -0.633429246 -0.52866048 0.215032279 -0.96452243 ## [237,] -0.888909034 -1.183849101 -0.87139007 -0.229933342 -0.32280217 ## [238,] -0.120044967 -0.534940430 -0.87139007 -0.776901711 -0.37629685 ## [239,] -0.022661660 -0.670005089 -2.18730727 0.113029530 -1.55434431 ## [240,] -0.755684527 -0.400538085 -0.18593089 -0.229933342 0.37332737 ## [241,] -0.875367944 1.380862076 1.44521531 0.437515089 1.62247383 ## [242,] 1.954496061 -2.415965110 1.10248572 -3.066777939 0.14082973 ## [243,] -1.367580615 0.717978876 0.75975612 -0.878904460 1.47936122 ## [244,] 1.455481802 1.470670704 0.81475730 0.094552218 0.69452834 ## [245,] -1.601589223 -0.981762266 0.49952829 -0.433938840 -0.60811280 ## [246,] -0.408794091 -0.982741182 -0.84388949 0.094552218 -1.71414665 ## [247,] 1.166732677 0.037603564 1.10248572 -0.878904460 0.65863800 ## [248,] 0.129462163 -0.192132304 -0.55616107 0.762000648 0.15911575 ## [249,] -0.600159910 -0.093643487 -0.55616107 -0.878904460 -0.53701379 ## [250,] 0.324228776 0.989033711 0.81475730 0.419037777 0.08801674 ## LOYAL ## [1,] -0.75409297 ## [2,] 1.14853353 ## [3,] -0.46039135 ## [4,] 1.14853353 ## [5,] 0.56113029 ## [6,] 1.14853353 ## [7,] 0.01526528 ## [8,] -0.34864473 ## [9,] -0.46039135 ## [10,] -0.02627295 ## [11,] 0.56113029 ## [12,] -1.22974959 ## [13,] 0.01526528 ## [14,] 1.14853353 ## [15,] 0.23875851 ## [16,] 0.37917528 ## [17,] 1.14853353 ## [18,] -2.18106284 ## [19,] 0.96657852 ## [20,] -0.39018296 ## [21,] 0.49092190 ## [22,] -1.00625636 ## [23,] -0.27843634 ## [24,] 0.49092190 ## [25,] -2.76846608 ## [26,] 1.14853353 ## [27,] -0.27843634 ## [28,] 0.56113029 ## [29,] 0.37917528 ## [30,] 1.14853353 ## [31,] 0.78462352 ## [32,] -1.52345121 ## [33,] 1.14853353 ## [34,] 0.56113029 ## [35,] -3.13237609 ## [36,] 0.01526528 ## [37,] -2.47476446 ## [38,] -0.20822796 ## [39,] 0.85483191 ## [40,] 0.85483191 ## [41,] 0.78462352 ## [42,] 0.37917528 ## [43,] 1.14853353 ## [44,] 0.12701189 ## [45,] 0.78462352 ## [46,] -2.04064606 ## [47,] -0.64234635 ## [48,] 0.19722028 ## [49,] -1.70540622 ## [50,] -0.82430136 ## [51,] 0.56113029 ## [52,] -0.46039135 ## [53,] 0.60266852 ## [54,] 0.19722028 ## [55,] -0.27843634 ## [56,] -0.27843634 ## [57,] -1.22974959 ## [58,] 0.19722028 ## [59,] 1.14853353 ## [60,] -0.86583958 ## [61,] -1.15954120 ## [62,] 0.19722028 ## [63,] 0.30896690 ## [64,] -0.02627295 ## [65,] -2.36301785 ## [66,] -0.75409297 ## [67,] -0.57213796 ## [68,] -2.25127123 ## [69,] 0.37917528 ## [70,] 0.01526528 ## [71,] 0.30896690 ## [72,] -0.34864473 ## [73,] -3.13237609 ## [74,] -0.86583958 ## [75,] -0.31997457 ## [76,] -0.39018296 ## [77,] 0.49092190 ## [78,] 0.56113029 ## [79,] 0.19722028 ## [80,] 0.19722028 ## [81,] 0.78462352 ## [82,] 0.37917528 ## [83,] 1.14853353 ## [84,] -0.57213796 ## [85,] -0.68388458 ## [86,] -0.50192958 ## [87,] 1.14853353 ## [88,] 0.56113029 ## [89,] 1.14853353 ## [90,] 0.49092190 ## [91,] 0.56113029 ## [92,] -0.16668973 ## [93,] 0.85483191 ## [94,] -0.68388458 ## [95,] -3.13237609 ## [96,] 0.85483191 ## [97,] 0.67287690 ## [98,] 0.01526528 ## [99,] -0.86583958 ## [100,] 0.60266852 ## [101,] -0.68388458 ## [102,] -0.27843634 ## [103,] 0.96657852 ## [104,] -0.34864473 ## [105,] 1.14853353 ## [106,] -0.64234635 ## [107,] 0.19722028 ## [108,] 0.49092190 ## [109,] 1.14853353 ## [110,] 1.14853353 ## [111,] 0.37917528 ## [112,] 0.85483191 ## [113,] 1.14853353 ## [114,] 1.14853353 ## [115,] 0.78462352 ## [116,] 0.49092190 ## [117,] 1.14853353 ## [118,] 1.14853353 ## [119,] 0.26742867 ## [120,] 0.56113029 ## [121,] -0.46039135 ## [122,] -1.49478105 ## [123,] 1.14853353 ## [124,] 0.19722028 ## [125,] -0.39018296 ## [126,] -2.06931623 ## [127,] 0.85483191 ## [128,] -0.09648134 ## [129,] -1.07646475 ## [130,] 0.01526528 ## [131,] 0.56113029 ## [132,] -1.41170460 ## [133,] 1.14853353 ## [134,] 0.56113029 ## [135,] -0.09648134 ## [136,] 1.14853353 ## [137,] 1.14853353 ## [138,] 0.78462352 ## [139,] 0.30896690 ## [140,] 0.56113029 ## [141,] 0.78462352 ## [142,] 1.14853353 ## [143,] -0.93604797 ## [144,] 0.56113029 ## [145,] 1.14853353 ## [146,] -0.12515150 ## [147,] -1.52345121 ## [148,] -0.27843634 ## [149,] -1.22974959 ## [150,] 1.14853353 ## [151,] 1.14853353 ## [152,] 0.19722028 ## [153,] 0.96657852 ## [154,] 1.14853353 ## [155,] -0.27843634 ## [156,] -0.09648134 ## [157,] -1.07646475 ## [158,] 0.56113029 ## [159,] -0.71255474 ## [160,] 0.37917528 ## [161,] 0.56113029 ## [162,] 0.26742867 ## [163,] 0.96657852 ## [164,] -1.04779459 ## [165,] 1.14853353 ## [166,] -1.22974959 ## [167,] 1.14853353 ## [168,] 0.08547366 ## [169,] 0.19722028 ## [170,] 0.01526528 ## [171,] -0.39018296 ## [172,] 1.14853353 ## [173,] 0.49092190 ## [174,] -0.64234635 ## [175,] -0.48906151 ## [176,] -1.22974959 ## [177,] 0.05680350 ## [178,] 0.37917528 ## [179,] 1.14853353 ## [180,] -0.64234635 ## [181,] -0.09648134 ## [182,] -0.39018296 ## [183,] 1.14853353 ## [184,] -1.00625636 ## [185,] -1.11800298 ## [186,] 1.14853353 ## [187,] 0.37917528 ## [188,] -0.46039135 ## [189,] -0.86583958 ## [190,] 0.56113029 ## [191,] 0.56113029 ## [192,] -0.48906151 ## [193,] -0.48906151 ## [194,] 1.14853353 ## [195,] -1.22974959 ## [196,] -0.57213796 ## [197,] 1.14853353 ## [198,] 1.14853353 ## [199,] 0.85483191 ## [200,] 0.49092190 ## [201,] -0.78276313 ## [202,] 0.56113029 ## [203,] -0.16668973 ## [204,] 0.19722028 ## [205,] 0.56113029 ## [206,] -3.13237609 ## [207,] 1.14853353 ## [208,] 0.78462352 ## [209,] 0.37917528 ## [210,] 0.23875851 ## [211,] -0.68388458 ## [212,] -1.37016637 ## [213,] -0.09648134 ## [214,] 0.37917528 ## [215,] -1.04779459 ## [216,] -1.95756961 ## [217,] 1.14853353 ## [218,] 0.67287690 ## [219,] 0.67287690 ## [220,] -2.54497285 ## [221,] -0.86583958 ## [222,] 1.14853353 ## [223,] 0.67287690 ## [224,] -0.27843634 ## [225,] -0.12515150 ## [226,] 0.26742867 ## [227,] -1.41170460 ## [228,] 0.37917528 ## [229,] -0.68388458 ## [230,] 1.14853353 ## [231,] -2.36301785 ## [232,] -1.22974959 ## [233,] 0.56113029 ## [234,] -0.09648134 ## [235,] -1.22974959 ## [236,] -0.27843634 ## [237,] 0.37917528 ## [238,] -0.71255474 ## [239,] -2.95042109 ## [240,] -0.23689812 ## [241,] 1.14853353 ## [242,] 1.14853353 ## [243,] 1.14853353 ## [244,] 1.14853353 ## [245,] 0.01526528 ## [246,] -1.95756961 ## [247,] -0.05494311 ## [248,] 0.01526528 ## [249,] -0.27843634 ## [250,] 1.14853353 ## attr(,&quot;scaled:center&quot;) ## EXP IMAGE QUAL VALUE SATISF ## -7.435719e-17 -3.844147e-17 -1.184608e-16 -4.979350e-17 -2.037259e-16 ## LOYAL ## -5.889733e-17 ## attr(,&quot;scaled:scale&quot;) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## 0.5230488 1.4692068 0.7579182 1.0143180 1.4925016 1.0023920 densityplot(epsi) densityplot(epsi,use=&quot;prediction&quot;) #Unstandardised solution # Function to get unstandardized LV unstandardized_LV &lt;- function(data, sempls_model){ # Save object to hold outer weights which will be scaled in loop outer_weights_resc &lt;- sempls_model$outer_weights # Find standard deviation of indicators std_indicators &lt;- unlist(lapply(data, FUN = sd)) for (lv in colnames(sempls_model$model$M)) { # Check estimation mode of LV mode &lt;- attr(sempls_model$model$blocks[[lv]], &quot;mode&quot;) if (mode == &quot;A&quot;) { tmp_indicator_names &lt;- sempls_model$model$measuremod[sempls_model$model$measuremod[,1]==lv,2] } else { tmp_indicator_names &lt;- sempls_model$model$measuremod[sempls_model$model$measuremod[,2]==lv,1] } tmp_weights &lt;- outer_weights_resc[tmp_indicator_names,lv] tmp_weights &lt;- tmp_weights/std_indicators[tmp_indicator_names] outer_weights_resc[tmp_indicator_names,lv] &lt;- tmp_weights/sum(tmp_weights) } # Data as matrix in order to perform matrix multiplication data_mat &lt;- as.matrix(data[,rownames(outer_weights_resc)]) unstandardized_LV &lt;- data_mat %*% outer_weights_resc return(unstandardized_LV) } # Unstandardized LV LV_unstand &lt;- unstandardized_LV(data = mobi, sempls_model = epsi) head(LV_unstand) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## [1,] 6.683271 5.189807 5.437305 2.593244 5.790523 6.000000 ## [2,] 9.683271 9.598305 10.000000 10.000000 9.302147 10.000000 ## [3,] 7.000000 6.346034 5.958203 7.000000 7.371872 6.617466 ## [4,] 7.095312 8.073486 9.041797 5.000000 10.000000 10.000000 ## [5,] 8.390534 8.559786 8.958203 6.000000 8.743744 8.765069 ## [6,] 8.806891 8.690635 9.479102 10.000000 7.371872 10.000000 plot(LV_unstand[,5:6]) pairs(LV_unstand,pch=19,cex=0.7,cex.axis=0.8,col.axis=&quot;gray70&quot;,gap=0.5) summary(LV_unstand) ## EXP IMAGE QUAL VALUE ## Min. : 3.186 Min. : 3.404 Min. : 2.479 Min. : 1.000 ## 1st Qu.: 6.683 1st Qu.: 6.835 1st Qu.: 7.000 1st Qu.: 5.568 ## Median : 7.525 Median : 7.650 Median : 8.000 Median : 7.000 ## Mean : 7.519 Mean : 7.590 Mean : 7.803 Mean : 6.607 ## 3rd Qu.: 8.299 3rd Qu.: 8.474 3rd Qu.: 8.990 3rd Qu.: 7.966 ## Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 ## SATISF LOYAL ## Min. : 3.465 Min. : 1.000 ## 1st Qu.: 6.813 1st Qu.: 6.419 ## Median : 7.640 Median : 8.000 ## Mean : 7.513 Mean : 7.585 ## 3rd Qu.: 8.389 3rd Qu.: 9.235 ## Max. :10.000 Max. :10.000 2.5 Segmentation (Clustering) 2.5.1 What is clustering Basically what we aim to do, is separate e.g., customers into a given amount of groups. We have the following measures: Between-cluster variation: this is from one center to other cluster centers, we want to maximize this. Within-cluster variation: this is the distance from the center of the clusters to the center. 2.5.2 Clustering techniques There are many techniques and no one size fits all. We are gogin to focus on: Hierarchical algorithms, and Partitioning algorithsm, and main focus on K-means, as this is the most widely used. K-means advantages and disadvantages Advantages: Relatively efficient Better than most hierarchical methods at handling noisy data and outliers Disadvantges Need to specify k, the number of clusters in advance In small samples, results depend strongly on initial cluster centers In principle k-means works only with continuous variables it will always create the amount of groups, hence it is a good idea to see if you can visualize the data 2.5.3 An example with HBAT 2.5.3.1 1. Background The dataset for this case study is from Hair et al.2010, Multivariate Data Analysis, Pearson Education. HBAT is a manufacturer of paper products who sells products to two market segments: the newsprint industry and the magazine industry.The current market is very competitive, so the manufacturer wants to understand how its customers perceive the company and make purchasing decisions, in order to enforce customers’ loyalty. The manufacturer commissioned a study asking its customers to complete a questionnaire on a secure website. In total,100 customers (purchasing managers from different firms) buying from HBAT completed the questionnaire. The data consist of three main pieces of information: A 1st type of information is available from HBAT’s data warehouse and includes information, such as: customer type in terms of length of purchase relationship(x1), industry type(x2), size of the customer(x3), region of the customer(x4) and distribution system(X5). The 2nd type of information is collected based on the online questionnaire and includes customers’ perceptions of HBAT ́s performance on 13 attributes using a continuous 0-10 line scale with 10 being “Excellent” and 0 being “Poor”. The 13 attributes are: X6 Product quality X7 E-commerce X8 Technical support X9 Complaint resolution X10 Advertising X11 Product line X12 Salesforce image X13 Competitive pricing X14 Warranty and claims X15 Packaging X16 Order and billing X17 Price flexibility X18 Delivery speed The 3rd type of information relates to purchase outcomes and business relationships: satisfaction with HBAT, future purchase intention, etc (X19-X22) whether the firm would consider a strategic alliance/partnership with HBAT (X23). 2.5.3.1.1 1.1. The primary objective is: to develop a taxonomy that segments the customers into groups with similar perceptions. Once identified, separate strategies with different appeals can be formulated for each segment. In addition to forming a taxonomy of customers that can be used for market segmentation, cluster analysis also facilitates data simplification (each segment is used to define the basic character of segment members) and identification of relationships (e.g. for estimating the impact of customers perceptions on sales in each segment, enabling the analyst to understant what uniquely impacts each segment rather than the sample as a whole). 2.5.3.2 2. The data (Data Understanding Phase) 2.5.3.2.1 2.1 Load data correctly library(foreign) HBAT &lt;- read.spss(file = &quot;Data/Clustering/HBAT.sav&quot;, to.data.frame=TRUE) 2.5.3.2.2 2.2 Check if the data is numerical or categorical str(HBAT) ## &#39;data.frame&#39;: 100 obs. of 24 variables: ## $ id : num 1 2 3 4 5 6 7 8 9 10 ... ## $ x1 : Factor w/ 3 levels &quot;Less than 1 year&quot;,..: 2 3 3 1 2 1 1 2 2 1 ... ## $ x2 : Factor w/ 2 levels &quot;Magazine industry&quot;,..: 1 2 1 2 1 2 2 1 2 1 ... ## $ x3 : Factor w/ 2 levels &quot;Small (0 to 499)&quot;,..: 2 1 2 2 2 1 2 2 2 2 ... ## $ x4 : Factor w/ 2 levels &quot;USA/North America&quot;,..: 2 1 2 2 1 2 2 2 2 2 ... ## $ x5 : Factor w/ 2 levels &quot;Indirect through broker&quot;,..: 2 1 2 1 2 1 1 1 1 1 ... ## $ x6 : num 8.5 8.2 9.2 6.4 9 6.5 6.9 6.2 5.8 6.4 ... ## $ x7 : num 3.9 2.7 3.4 3.3 3.4 2.8 3.7 3.3 3.6 4.5 ... ## $ x8 : num 2.5 5.1 5.6 7 5.2 3.1 5 3.9 5.1 5.1 ... ## $ x9 : num 5.9 7.2 5.6 3.7 4.6 4.1 2.6 4.8 6.7 6.1 ... ## $ x10: num 4.8 3.4 5.4 4.7 2.2 4 2.1 4.6 3.7 4.7 ... ## $ x11: num 4.9 7.9 7.4 4.7 6 4.3 2.3 3.6 5.9 5.7 ... ## $ x12: num 6 3.1 5.8 4.5 4.5 3.7 5.4 5.1 5.8 5.7 ... ## $ x13: num 6.8 5.3 4.5 8.8 6.8 8.5 8.9 6.9 9.3 8.4 ... ## $ x14: num 4.7 5.5 6.2 7 6.1 5.1 4.8 5.4 5.9 5.4 ... ## $ x15: num 4.3 4 4.6 3.6 4.5 9.5 2.5 4.8 4.4 5.3 ... ## $ x16: num 5 3.9 5.4 4.3 4.5 3.6 2.1 4.3 4.4 4.1 ... ## $ x17: num 5.1 4.3 4 4.1 3.5 4.7 4.2 6.3 6.1 5.8 ... ## $ x18: num 3.7 4.9 4.5 3 3.5 3.3 2 3.7 4.6 4.4 ... ## $ x19: num 8.2 5.7 8.9 4.8 7.1 4.7 5.7 6.3 7 5.5 ... ## $ x20: num 8 6.5 8.4 6 6.6 6.3 7.8 5.8 7.5 5.9 ... ## $ x21: num 8.4 7.5 9 7.2 9 6.1 7.2 7.7 8.2 6.7 ... ## $ x22: num 65.1 67.1 72.1 40.1 57.1 50.1 41.1 56.1 56.1 59.1 ... ## $ x23: Factor w/ 2 levels &quot;No, would not consider&quot;,..: 2 1 2 1 1 1 1 1 2 1 ... ## - attr(*, &quot;variable.labels&quot;)= Named chr [1:24] &quot;ID&quot; &quot;X1 - Customer Type&quot; &quot;X2 - Industry Type&quot; &quot;X3 - Firm Size&quot; ... ## ..- attr(*, &quot;names&quot;)= chr [1:24] &quot;id&quot; &quot;x1&quot; &quot;x2&quot; &quot;x3&quot; ... ## - attr(*, &quot;codepage&quot;)= int 65001 2.5.3.2.3 2.3 Missing treatment library(DataExplorer) plot_missing(HBAT) any(is.na.data.frame(HBAT)) ## [1] FALSE We will only work with the perceptions and hence not the factors. 2.5.3.2.4 2.4 Select relevant variables # Select variables aimed for clustering HBAT_CLU = HBAT[,c(7:19)] 2.5.3.2.5 2.5.Identify extreme outliers (the most dissimilar customers) There is no best single way to identify the outliers. Many methods are available. One option when the variables are all metric is: Calculate the difference between the observed value for a variable and the variable mean Square the difference Sum the squared differences to get a total deviations for that observation across all variables Take the square root Repeat this process for all observations Observations with the highest dissimilarity have the potential to be outliers Do not focus on the absolute value; rather, look for any value that is relatively large compare to others. diss = sqrt((HBAT_CLU$x6 - mean(HBAT_CLU$x6))^2 + (HBAT_CLU$x7 - mean(HBAT_CLU$x7))^2 + (HBAT_CLU$x8 - mean(HBAT_CLU$x8))^2 + (HBAT_CLU$x9-mean(HBAT_CLU$x9))^2 + (HBAT_CLU$x10 - mean(HBAT_CLU$x10))^2 + (HBAT_CLU$x11- mean(HBAT_CLU$x11))^2 + (HBAT_CLU$x12 - mean(HBAT_CLU$x12))^2 + (HBAT_CLU$x13 - mean(HBAT_CLU$x13))^2 + (HBAT_CLU$x14 - mean(HBAT_CLU$x14))^2 + (HBAT_CLU$x15 - mean(HBAT_CLU$x15))^2 + (HBAT_CLU$x16 - mean(HBAT_CLU$x16))^2 + (HBAT_CLU$x17- mean(HBAT_CLU$x17))^2 + (HBAT_CLU$x18 - mean(HBAT_CLU$x18))^2) # Add diss to the dataset HBAT$diss = diss # order by diss (decreasing order) newHBAT&lt;- HBAT[order(-diss),] # display the first 10% rows (the first 10 observations in our dataset) head(newHBAT[,c(1,25)], 10) id diss 87 87 6.928141 7 7 6.767181 92 92 6.334014 90 90 6.149157 6 6 6.053869 84 84 5.982937 22 22 5.978623 57 57 5.766622 48 48 5.498648 72 72 5.433832 # observe in the output that id 87 displays the largest diss value, meaning the highest average distance overall. However,it does not stand out over the others too much. In this dataset, we do not find any extreme outlier. 2.5.3.2.6 2.6 Standardize the variables (mean=0, std.dev=1) sd.data=scale(HBAT_CLU) 2.5.3.3 3. Hierarchical clustering (using variables) # Using complete, avg and single linkage + Euclidian distance hc.complete = hclust(dist(sd.data), method=&quot;complete&quot;) hc.average = hclust(dist(sd.data), method=&quot;average&quot;) hc.single = hclust(dist(sd.data), method=&quot;single&quot;) # Plot the dendograms par(mfrow=c(1,1)) plot(hc.complete, main=&quot;Complete linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;, ylab=&quot;&quot;) plot(hc.average, main=&quot;Average linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;, ylab=&quot;&quot;) plot(hc.single, main=&quot;Single linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;, ylab=&quot;&quot;) Based on the dendograms we observe: the choice of linkage type affects the results none of the linkage methods yield a balanced, attractive cluster solution one reason could be that many of the variables in our dataset are highly correlated multicolinearity (very highly correlated variables) can be an issue in cluster analysis in other words, the cluster analysis works best with uncorrelated features Therefore, we have two alternatives: Select the variables so that the remaining ones are not strongly correlated to one another Run a PCA to extract the most important principal components, and afterwards fit the cluster model using PCs vector scores, instead of the original variables. PCs are expected to retain most of the information in the dataset and by definition they are orthogonal (uncorrelated) with one another. We adopt this alternative below. we see that the complete linkage appear to yield the best result. 2.5.3.4 4. Hierarchical clustering (using PCs score vectors) #Run PCA HBAT.pca = prcomp(HBAT_CLU, scale = TRUE) HBAT.pca summary(HBAT.pca) screeplot(HBAT.pca, type=&quot;line&quot;) abline(h=1, col=&quot;red&quot;, lty= 3) ## Standard deviations (1, .., p=13): ## [1] 1.88867011 1.73137071 1.31836167 1.13455941 1.00261538 0.78652035 ## [7] 0.74258178 0.66857604 0.52984507 0.44800545 0.40767063 0.36202055 ## [13] 0.09489861 ## ## Rotation (n x k) = (13 x 13): ## PC1 PC2 PC3 PC4 PC5 PC6 ## x6 -0.008446162 -0.36209283 -0.04678935 0.47025543 0.05007347 0.053218125 ## x7 -0.244268729 0.25887463 0.35900707 0.34222834 -0.09769943 0.376718762 ## x8 -0.069090049 -0.27405908 0.52503623 -0.36487985 0.01424126 -0.052463334 ## x9 -0.457137214 -0.12307431 -0.20045850 -0.10240177 -0.08595970 0.056968447 ## x10 -0.255224876 0.20968349 0.18298044 0.32643625 0.12554223 -0.780692662 ## x11 -0.252450282 -0.42196215 -0.10369985 0.21751972 -0.03205501 0.163391859 ## x12 -0.290987483 0.27281337 0.35867289 0.29559280 -0.01309635 0.195905988 ## x13 0.008246314 0.43224128 -0.01407530 -0.15377746 0.02328483 0.358023704 ## x14 -0.128457735 -0.26796431 0.52277710 -0.32283288 0.13005779 0.001572649 ## x15 -0.065686377 0.01201495 -0.09504456 0.01527331 0.97011829 0.145935408 ## x16 -0.430501041 -0.10171830 -0.16835005 -0.14893939 -0.05661020 0.028760422 ## x17 -0.278546553 0.37514985 -0.14985400 -0.34574836 0.01358671 -0.154763215 ## x18 -0.478712405 -0.07283481 -0.21649677 -0.08389812 -0.03767846 0.020892771 ## PC7 PC8 PC9 PC10 PC11 PC12 ## x6 -0.66589654 0.33225521 -0.22462438 0.007874262 -0.18916547 0.004949852 ## x7 0.20788462 0.18096215 0.04333602 -0.568040361 -0.12566307 0.242746705 ## x8 -0.12857096 0.02143816 -0.36226254 -0.317860485 0.37815324 -0.340652073 ## x9 -0.01886806 -0.04489722 -0.30907596 0.143600911 0.42151255 0.646668513 ## x10 -0.13799303 -0.24702527 0.12615240 -0.132664225 0.08789281 0.064371162 ## x11 0.14409748 -0.51451471 0.07780447 -0.095834270 -0.07938639 -0.183829669 ## x12 0.09050524 0.06131073 -0.12450579 0.663204133 0.14586925 -0.308660927 ## x13 -0.62750296 -0.48310471 0.13005088 -0.074305149 0.10264536 -0.009554218 ## x14 -0.10712744 -0.07358489 0.29554570 0.264416777 -0.46117870 0.360900790 ## x15 0.10467064 0.04524136 -0.02892652 -0.056036609 0.08198456 -0.003969278 ## x16 -0.13766958 0.40015006 0.66839914 -0.055764820 0.26409100 -0.225868516 ## x17 -0.09410061 0.28163148 -0.30107123 -0.053262879 -0.37941812 -0.067534206 ## x18 0.06694700 -0.21116321 -0.20232546 -0.066810707 -0.38574841 -0.296113322 ## PC13 ## x6 -0.003255188 ## x7 0.024896668 ## x8 -0.007706804 ## x9 0.014147290 ## x10 0.012734529 ## x11 -0.575848184 ## x12 -0.049860958 ## x13 0.017852976 ## x14 0.013022281 ## x15 0.011434967 ## x16 -0.008058268 ## x17 -0.534199397 ## x18 0.615473481 ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 1.8887 1.7314 1.3184 1.13456 1.00262 0.78652 0.74258 ## Proportion of Variance 0.2744 0.2306 0.1337 0.09902 0.07733 0.04759 0.04242 ## Cumulative Proportion 0.2744 0.5050 0.6387 0.73769 0.81502 0.86261 0.90502 ## PC8 PC9 PC10 PC11 PC12 PC13 ## Standard deviation 0.66858 0.5298 0.44801 0.40767 0.36202 0.09490 ## Proportion of Variance 0.03438 0.0216 0.01544 0.01278 0.01008 0.00069 ## Cumulative Proportion 0.93941 0.9610 0.97644 0.98923 0.99931 1.00000 Based on the elbow, eigen values and % of variance explained criteria, we may decide to select the first 4 PCs as the most important (see Case Study PCA from last week for full details). Hierarchical clustering based on the first 4 PCs score vectors using Euclidian distance (dist) using linkage=complete, average, or single hc.complete= hclust(dist(HBAT.pca$x[,1:4]), method=&quot;complete&quot;) hc.average= hclust(dist(HBAT.pca$x[,1:4]), method=&quot;average&quot;) hc.single= hclust(dist(HBAT.pca$x[,1:4]), method=&quot;single&quot;) plot(hc.complete, main=&quot;Hierarchical cluster with complete linkage&quot;) plot(hc.average, main=&quot;Hierarchical cluster with average linkage&quot;) plot(hc.single, main=&quot;Hierarchical cluster with single linkage&quot;) Looking at the dendograms: Single linkage does not perform well again. Likewise, average linkage creates innapropriate clusters. Complete linkage creates more appropriate clusters. It comes out that 3-cluster is the most plausible solution. To visualize the dendogram with red borders around the 3 clusters uncomment the next lines: hc.complete &lt;- hclust(dist(HBAT.pca$x[,1:4]), method=“complete”) plot(hc.complete, main=“Hierarchical cluster with complete linkage”) rect.hclust(hc.complete , k = 3, border = “red”) Before proceeding with K-means, we can profile the original variables on the three clusters to confirm that the differences between the clusters are distinctive. # Save the clusters by cutting the tree into 3 clusters hc.clusters = cutree(hc.complete,3) # Add the cluster variable to my data HBAT_CLU$clusters = hc.clusters # Check size of the clusters table(HBAT_CLU$clusters) ## ## 1 2 3 ## 26 53 21 Run Anova to see if differences in means between clusters are significant and to define the characteristics of the clusters. The independent variable: cluster membership. The dependent variables: the 13 original variables. Does not appear to run The F-statistic from one-way ANOVA examine if there are sig. differences between the three clusters on the respective independent variable. The results indicate sig. differences with respect to all the variables, except x6, x13 and x15. We may examine the means for the significant variables and characterize the clusters. Examples: For some reason, this chunk cannot be knitted hence eval=FALSE We want to see one cluster being different than the others. In some examples we see that the box is pretty much overlapping with the others, hence that can also be an explanation of why a given cluster is not significant on some variables. # Run the same for: x14, x16, x17, x18 # Interpreting the clusters by looking at the extremes (highest and lowest means): # - Cluster 1 has a relatively lower mean on x9 and x11 variables than the other two clusters. This means that customers in this cluster are unhappy about complaint resolution (x9) and product line (x11) # - Cluster 2 has a relatively higher mean on x8 (Technical support) and x11(Product line), than the other two clusters. # - Cluster 3 has a relatively higher mean on x7(E-commerce perceptions), x9(complain resolution), x10(advertising), x12 (salesforce image), than the other two clusters. # - Overall, as expected, these results indicate that each of the three clusters exibit somewhat distinctive characteristics. # NOTE: the label assigned to clusters &quot;1&quot;, &quot;2&quot;, &quot;3&quot; can changes from one computer to another. 2.5.3.5 5. K-means In k-means clustering the analyst must specify the number of clusters to extract. But how can one know how many segments exist in the population? One possibility is to rely on theory. The analyst might expect a certain number of segments in the population investigated based on previous knowledge. Another possibility is to use the results from Hierarchical Clustering.There we identified 3 clusters.Thus, we run K-means clutering with K=3. set.seed(1) km.out= kmeans(HBAT.pca$x[,1:4], 3, nstart=20) km.out # evaluate this output km.clusters=km.out$cluster # saving the clusters ## K-means clustering with 3 clusters of sizes 39, 30, 31 ## ## Cluster means: ## PC1 PC2 PC3 PC4 ## 1 -0.3330352 -1.7369154 -0.08640935 0.26069725 ## 2 -1.6905084 1.3919404 0.05935694 -0.31601740 ## 3 2.0549557 0.8381126 0.05126634 -0.02215066 ## ## Clustering vector: ## [1] 2 1 1 3 1 3 3 3 2 2 1 3 2 1 2 1 2 2 2 2 3 1 1 1 2 2 1 3 1 3 1 3 3 2 3 3 1 ## [38] 1 2 2 3 1 2 2 1 2 1 2 1 1 3 1 1 3 2 1 2 1 1 1 1 3 3 3 3 2 2 2 3 2 2 3 2 1 ## [75] 2 1 2 1 1 3 1 1 3 3 1 3 3 1 1 2 1 3 1 1 3 1 3 3 2 3 ## ## Within cluster sum of squares by cluster: ## [1] 181.6540 162.0146 181.0767 ## (between_SS / total_SS = 44.7 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; We can compare k-means with hierarchical solution (if we want) using a cross-classification table: table(km.clusters, hc.clusters) ## hc.clusters ## km.clusters 1 2 3 ## 1 1 36 2 ## 2 4 7 19 ## 3 21 10 0 Perfect fit would appear if only one cell in each row or column of the table contained a value. The two methods produce quite different clusters (this is typical), but most of the observations in k-means are grouped with the same observations they cluster with in the hc.cluster solution. In practice the researchers rely on the K-means solution. We now interpret the meaning of the clusters by analysing the pattern of their cluster means (as we did before with hierarchical clustering) HBAT_CLU$clustersK = km.out$cluster summary(aov(HBAT_CLU$x6 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x7 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x8 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x9 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x10 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x11 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x12 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x13 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x14 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x15 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x16 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x17 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x18 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 55.28 55.28 39.33 9.68e-09 *** ## Residuals 98 137.73 1.41 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 0.05 0.0507 0.102 0.75 ## Residuals 98 48.53 0.4952 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 19.61 19.610 9.053 0.00333 ** ## Residuals 98 212.28 2.166 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 39.84 39.84 37.28 2.06e-08 *** ## Residuals 98 104.73 1.07 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 0.1 0.1036 0.081 0.777 ## Residuals 98 125.6 1.2819 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 92.64 92.64 115.5 &lt;2e-16 *** ## Residuals 98 78.63 0.80 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 0.0 0.000 0 0.999 ## Residuals 98 113.8 1.162 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 55.76 55.76 30.27 3.01e-07 *** ## Residuals 98 180.57 1.84 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 8.62 8.623 14.6 0.000234 *** ## Residuals 98 57.90 0.591 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 0.54 0.5365 0.239 0.626 ## Residuals 98 220.15 2.2465 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 22.81 22.810 35.71 3.71e-08 *** ## Residuals 98 62.60 0.639 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 4.92 4.924 3.47 0.0655 . ## Residuals 98 139.07 1.419 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 16.48 16.483 43.76 1.98e-09 *** ## Residuals 98 36.92 0.377 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 According to Anova F-test, the K-means clusters differ significantly based on x6,x7,x8,x10,x11,x12,x13,x17. As we did before for non-hierarchical clustering, interpretation of differences among clusters begins by looking for extreme mean values associated with each cluster. In other words, variable means that are the highest or lowest compared to other clusters are useful in the process of interpreting the clusters. We can take a look at the plots of the means: Example for x6: library(&quot;ggpubr&quot;) ggboxplot(HBAT_CLU, x = &quot;clustersK&quot;, y = &quot;x13&quot;, color = &quot;clustersK&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), ylab = &quot;x13&quot;, xlab = &quot;ClustersK&quot;) # To complement the descriptive analysis, we ask for a statistical differences between means based on TukeyHSD(Honestly Significant Difference) TukeyHSD(aov(HBAT_CLU$x6 ~ as.factor(HBAT_CLU$clustersK), data=HBAT_CLU[ ,1:13])) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = HBAT_CLU$x6 ~ as.factor(HBAT_CLU$clustersK), data = HBAT_CLU[, 1:13]) ## ## $`as.factor(HBAT_CLU$clustersK)` ## diff lwr upr p adj ## 2-1 -2.0446154 -2.6544070 -1.4348237 0.0000000 ## 3-1 -1.7028950 -2.3071019 -1.0986880 0.0000000 ## 3-2 0.3417204 -0.3013718 0.9848127 0.4184721 We see tat that cluster 1 appear to be different from the two other clusters. Therefore, we also see the Tukey HSF is also saying that there is difference between 1 - 2 and 1 - 3, although 2 - 3 are pretty much the same. Interpretation of the clusters: Cluster 1 has 39 observations and is most distiguished by a relatively higher mean on x6, and x?, x?…, than the other two clusters. Cluster 2 has 31 observations and is mostly distiguished by a relatively higher /lower mean on ….,than the other two clusters. Cluster 3 has 30 observations and is most distiguished by a relatively higher /lower mean on …., than the other two clusters. Using the pca3d library we represent the cluster solution into the first 2 PCs dimensions. This is another descriptive illustration of the clusters and their profile. For some reason this cannot be run when knitting library(pca3d) pca2d(HBAT.pca, biplot=TRUE, group=km.clusters, legend = &quot;topright&quot;, show.ellipses=TRUE, ellipse.ci=0.75, show.plane=TRUE, show.labels = TRUE) # See Case Study PCA for biplot interpretation. 2.5.3.6 6. Validation and profiling the clusters The analyst should perform tests to confirm the validity of the cluster solution while also ensuring the cluster solution has practical significance. 2.5.3.6.1 6.1. Assessing cluster stability The stability of the cluster solution can be evaluated by splitting the data (if sufficiently big), running two cluster analysis and comparing the results. 2.5.3.6.2 6.2. Assessing criterion (predictive) validity To assess predictive validity of the clusters, we can focus on other variables that have a theoretically based relationship to the clustering variables, but were not included (!) in the cluster analysis. Given this relationship, we should see significant differences in these variables across the clusters. If significant differences do exist, we can draw the conclusion that the clusters depict groups that have predictive validity. For this purpose, we consider four outcome measures from the HBAT, x19 Satisfaction x20 Likelihood to recommend x21 Likelihood to purchase x22 Purchase level # MANOVA allows us to compare simultaneusly the means of these continuous variables among the clusters manova.model &lt;- manova(cbind(x19,x20,x21,x22) ~ HBAT_CLU$clustersK, data = HBAT) summary(manova.model) ## Df Pillai approx F num Df den Df Pr(&gt;F) ## HBAT_CLU$clustersK 1 0.49108 22.918 4 95 2.831e-13 *** ## Residuals 98 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The overall F-test is significant at 10% level of significance =&gt; initial evidence that some these variables can be predicted by simply knowing to which cluster an HBAT customer belongs. # To check individual F-tests: summary.aov(manova.model) ## Response x19 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 48.930 48.930 52.292 1.064e-10 *** ## Residuals 98 91.698 0.936 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Response x20 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 30.296 30.2956 38.327 1.399e-08 *** ## Residuals 98 77.464 0.7905 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Response x21 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 16.370 16.3700 22.793 6.314e-06 *** ## Residuals 98 70.383 0.7182 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Response x22 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 3413.8 3413.8 76.745 5.898e-14 *** ## Residuals 98 4359.2 44.5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note the output reveals significant F-test for x20, x21 (at 10% level) and x22 Digging more into which specific means differ between the three clusters: HBAT$clustersK = HBAT_CLU$clustersK ggboxplot(HBAT, x = &quot;clustersK&quot;, y = &quot;x20&quot;, color = &quot;clustersK&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), ylab = &quot;x20&quot;, xlab = &quot;ClustersK&quot;) # For example we see that cluster 1 is the most likely to recommend and cluster 2 is the least likely. ggboxplot(HBAT, x = &quot;clustersK&quot;, y = &quot;x21&quot;, color = &quot;clustersK&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), ylab = &quot;x21&quot;, xlab = &quot;ClustersK&quot;) # Idem we see that cluster 1 is the most likely to purchase and cluster 2 is the least likely. ggboxplot(HBAT, x = &quot;clustersK&quot;, y = &quot;x22&quot;, color = &quot;clustersK&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), ylab = &quot;x22&quot;, xlab = &quot;ClustersK&quot;) # Idem we see that cluster 1 has the highest purchase level and cluster 2 is the lowest. Idem = the same as previous 2.5.3.6.3 6.3. Profiling the final cluster solution A final task can be to profile the clusters on a set of additional qualitative variables, not included in the cluster analysis. In this example, five characteristics of HBAT customers are available: length of purchase relationship(x1) industry type(x2) size of the customer(x3) region of the customer(x4) distribution system(X5) # Cross-classification table allows us to profile the clusters based on these categorical variables # Chi-square test library(MASS) tblx1 = table(HBAT$x1, HBAT_CLU$clustersK) tblx1 ## ## 1 2 3 ## Less than 1 year 2 5 25 ## 1 to 5 years 8 22 5 ## Over 5 years 29 3 1 chisq.test(tblx1) ## ## Pearson&#39;s Chi-squared test ## ## data: tblx1 ## X-squared = 85.369, df = 4, p-value &lt; 2.2e-16 Significant chi-square value reveals for example that x1 (customer type) tends to be associated with certain clusters. In my output observe that customers of “Over 5 years” tend to be concentrated in cluster 1 (recall these were customers that were also most likely to recommend and to purchase). Cluster 2 consists predominantly of customers of “less than 1 year” (they were the customers least likely to recommend and purchase in the future); and cluster 3 consists predominantly of customers between 1 to 5 years. Idem, we can evaluate x2, x3, x4 and x5. From these qualitative variables, distinctive profiles can be developed for each cluster. These profiles support the distinctiveness of the clusters on variables not used in the analysis at any prior point. 2.5.3.7 7. Final remarks The clustering techniques discussed (hierarchical and K-means) can be applied to numeric (continuous) variables. This course focuses exclusively on these two methods. For clustering with categorical variables, one option is to use Expectation Maximization algorithm, which yields probabilistic classes (clusters). Finally, if interested in clustering with both categorical and continous variables, refer to a new package in R called ‘clustMixType’ (k-Prototypes Clustering for Mixed Variable-Type Data) which can handle mixed data. 2.5.4 An example with McDonalds data The following is seperated into several sections. 2.5.4.1 Loading and exploring the data Step 1: Deciding to segment or not You are the manager of a fast food restaurant. The restaurant can take the position that it caters to the entire market. In such case, there is no need to understand systematic differences across market segments. Alternatively, the restaurant can take the position that, despite their market power, there is value in investigating systematic heterogeneity among customers annd harvest these differences with a differentiated marketing strategy. We go for the second alternative. Step 2: Specifying the ideal target segment Decide which are the key features that make a segment attractive? homogeneous substantially distinct large enough identifyiable reachable (channels of communication and distribution exist to aim at that target) In terms of attractiveness, the obvious choice would be a segment with positive perception of our restaurant, which frequently eats out, and likes fast food. But, the manager can also decide that they not only want to solidify position, but rather wish to learn more about the market segments which are currently not fond of fast food. Step 3: Collecting data Data originally from Dolnicar (2018) contains responses from 1453 adult Australian consumers relating to their perceptions of fast food restaurant with respect to different attributes (yummy, convenient, spicy, fattening greasy, fast, cheap, tasty, expensive, healthy, and disgusting). For each attribute, respondents provided either (Yes) or (No) response indicating whether the restaurant possesses this attribute. Note1: data here is categorical, not continuous. In addition, respondents indicated age, gender, visit frequency, and likeliness. Note2: Colecting additional data on dining out behavior and their use of information channels would allow the development of a more prosperous and more detailed description of our clusters. But this information is not available in the dataset. Step 4: Exploring data mcdonalds &lt;- read.csv (&quot;Data/Clustering/mcdonalds.csv&quot;) str(mcdonalds) ## &#39;data.frame&#39;: 1453 obs. of 15 variables: ## $ yummy : chr &quot;No&quot; &quot;Yes&quot; &quot;No&quot; &quot;Yes&quot; ... ## $ convenient : chr &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; ... ## $ spicy : chr &quot;No&quot; &quot;No&quot; &quot;Yes&quot; &quot;No&quot; ... ## $ fattening : chr &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; ... ## $ greasy : chr &quot;No&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; ... ## $ fast : chr &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; ... ## $ cheap : chr &quot;Yes&quot; &quot;Yes&quot; &quot;No&quot; &quot;Yes&quot; ... ## $ tasty : chr &quot;No&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; ... ## $ expensive : chr &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; &quot;No&quot; ... ## $ healthy : chr &quot;No&quot; &quot;No&quot; &quot;Yes&quot; &quot;No&quot; ... ## $ disgusting : chr &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;Yes&quot; ... ## $ Like : chr &quot;-3&quot; &quot;+2&quot; &quot;+1&quot; &quot;+4&quot; ... ## $ Age : int 61 51 62 69 49 55 56 23 58 32 ... ## $ VisitFrequency: chr &quot;Every three months&quot; &quot;Every three months&quot; &quot;Every three months&quot; &quot;Once a week&quot; ... ## $ Gender : chr &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ... We see that the categorical data is actually coded in words, this we need to transform. MD.x &lt;- as.matrix(mcdonalds[, 1:11]) MD.x &lt;- (MD.x == &quot;Yes&quot;) + 0 head(MD.x) ## yummy convenient spicy fattening greasy fast cheap tasty expensive healthy ## [1,] 0 1 0 1 0 1 1 0 1 0 ## [2,] 1 1 0 1 1 1 1 1 1 0 ## [3,] 0 1 1 1 1 1 0 1 1 1 ## [4,] 1 1 0 1 1 1 1 1 0 0 ## [5,] 0 1 0 1 1 1 1 0 0 1 ## [6,] 1 1 0 1 0 1 1 1 0 0 ## disgusting ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 1 ## [5,] 0 ## [6,] 0 Let us look at the colmeans. round(colMeans(MD.x), 2) %&gt;% barplot(ylim = c(0,1),col = &quot;darkgrey&quot;,main = &quot;Variable means&quot;) abline(h = seq(0,1,0.1),col = &quot;darkgrey&quot;,lty = 2) We see that few people rate the food to be spicy and most think that it is convenient, fast and fattening. We are going to run PCA to see variables are correlated. MD.pca &lt;- prcomp(MD.x) summary(MD.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 0.7570 0.6075 0.5046 0.3988 0.33741 0.3103 0.28970 ## Proportion of Variance 0.2994 0.1928 0.1331 0.0831 0.05948 0.0503 0.04385 ## Cumulative Proportion 0.2994 0.4922 0.6253 0.7084 0.76787 0.8182 0.86201 ## PC8 PC9 PC10 PC11 ## Standard deviation 0.27512 0.26525 0.24884 0.23690 ## Proportion of Variance 0.03955 0.03676 0.03235 0.02932 ## Cumulative Proportion 0.90156 0.93832 0.97068 1.00000 We see that the first three components reaches the 60% level. now lets look at the loadings print(MD.pca, digits = 1) ## Standard deviations (1, .., p=11): ## [1] 0.8 0.6 0.5 0.4 0.3 0.3 0.3 0.3 0.3 0.2 0.2 ## ## Rotation (n x k) = (11 x 11): ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 ## yummy 0.477 -0.36 0.30 -0.055 -0.308 0.17 -0.28 0.01 -0.572 0.110 ## convenient 0.155 -0.02 0.06 0.142 0.278 -0.35 -0.06 -0.11 0.018 0.666 ## spicy 0.006 -0.02 0.04 -0.198 0.071 -0.36 0.71 0.38 -0.400 0.076 ## fattening -0.116 0.03 0.32 0.354 -0.073 -0.41 -0.39 0.59 0.161 0.005 ## greasy -0.304 0.06 0.80 -0.254 0.361 0.21 0.04 -0.14 0.003 -0.009 ## fast 0.108 0.09 0.06 0.097 0.108 -0.59 -0.09 -0.63 -0.166 -0.240 ## cheap 0.337 0.61 0.15 -0.119 -0.129 -0.10 -0.04 0.14 -0.076 -0.428 ## tasty 0.472 -0.31 0.29 0.003 -0.211 -0.08 0.36 -0.07 0.639 -0.079 ## expensive -0.329 -0.60 -0.02 -0.068 -0.003 -0.26 -0.07 0.03 -0.067 -0.454 ## healthy 0.214 -0.08 -0.19 -0.763 0.288 -0.18 -0.35 0.18 0.186 0.038 ## disgusting -0.375 0.14 0.09 -0.370 -0.729 -0.21 -0.03 -0.17 0.072 0.290 ## PC11 ## yummy 0.045 ## convenient -0.542 ## spicy 0.142 ## fattening 0.251 ## greasy 0.002 ## fast 0.339 ## cheap -0.489 ## tasty 0.020 ## expensive -0.490 ## healthy 0.158 ## disgusting -0.041 We see component 1 is highly influenced by yummy and tasty, hence something about the taste. Then the second component is hihgly described by cheap and expensive, hence something about price. Component 3 is more about the fattening and greasy-ness. Lets plot the first two components. library(flexclust) plot(predict(MD.pca), col = &quot;grey&quot;) projAxes(MD.pca) Cheap and expensive play a key role in the evaluation of the restaurant disgusting, fattening and greasy point in the same direction (respondents who view the restauratnt fattening and disgusting are also likely to view it greasy) in the opposite direction are the positive attributes: fast, convenient, healthy, tasty, and yummy Notice how respondents (data points) are distributed: a group of respondents at the top, close to cheap a group of respondents at the bottom, close to expensive a group of respondents in thee middle Other preliminary remarks: Results indicate that some attributes are strognly related to one another and that price dimension may be critical in differentiating between groups of consumer Next, one can perform the segmentation using the most important Pc, as in Lecture 1. That was one approach. Dolnicar and Grun (2008) Challenging factor-cluster segmentation. proposes using Pc in the exploration phase only, and afterwards work with raw data. We follow this approach here. The clustering variables, in this case, are the dummy (0/1). Distance-based methods such as hierarchical and partitioning (e.g., k-means) work best with continuous variables. Still, in practice, K-means is often used with both categorical and continuous variables. 2.5.4.2 k-Means clustering First we are going to assess how many clusters. This will be based on Global stability-based data structure analysis, make boxplots for each clusters through bootstrapping to see cluster stability/reporducability, and Segment Stability, to assess the stability of given segments (clusters) STEP 5: Extracting segments We are going to use k-means clustering. set.seed(1234) MD.km28 &lt;- stepFlexclust(MD.x, 2:8 ,nrep = 10 #nrep = 10 random starts, k = 2:8 possible clusters ,verbose = FALSE) MD.km28 &lt;- relabel(MD.km28) # relabel segment numbers to be consistent across segmentation plot(MD.km28, xlab = &quot;number of segments&quot;) # scree plot to compare different solutions We see that the more clusters we have, the smaller is the within distance of the clusters, that makes sense, as each cluster is able to be more specific than in a fewer no. of cluster constellation. What are we looking for?: A big increase in heterogeneity. Although that is not visible. As we don’t see any bigger changes, we will use the approach Global stability-based data structure analysis. What is Global stability-based data structure analysis? Global Stability (Dolnicar, 2010) The fundamental idea is to conduct a large number of repeated computations with many bootstrap samples, with any chosen segmentation algorithm. If the resulting cluster is the same across repeated computations, it can be assumed that clusters exist in the data. The main steps draw bootstrap samples compute the Rand index inspect boxplots to assess the reproducibility of cluster solutions if Rand index close to 1 =&gt; existence of reproducible clusters if Rand index clode to 0 =&gt; no structure in the data set.seed(1234) MD.b28 &lt;- bootFlexclust(MD.x, 2:8, nrep = 10, nboot = 30) #She used 100 # global stability plot: plot(MD.b28, xlab = &quot;number of segments&quot;, ylab = &quot;adjusted Rand index&quot; ,main = &quot;Global Stability Plot&quot;) See the distribution of stability for each number of segments. Median in the black horizontal line. The higher the stability, the better. As noticed, 2- and 3-cluster solutions are pretty stable. However, they do not offer a differentiated view of the market 4-cluster solution emerges as the solution containing most market segments which can still be reasonably well replicated (median adjusted Rand index is above 0.7). Hence this argue for a 2 or 3 cluster model. Although before we take any conclusions, we can see what the segment stability suggests. What is segment stability? Segment Stability (Dolnicar, 2017) Global stability analysis explained before is based on a comparison of segmentation solutions with the same number of segments. Another way to explore that data before committing to the final market segmentation solution is to inspect how to segment memberships change each time an additional market segment is added, and to assess segment stability across solutions. For this we use the salsaplot :-) slsaplot(MD.km28) Interpretation: thick green lines indicate that many members of the segment to the left of the line move across to the segment on the right side of the line. For this data, we see that segment 2 in the two-segment solution (in the far left column of the plot) remains almost unchanged until the four-segment solution, then it starts losing members. (i.e. segment 2 displays high stability until 4 segment solution). similar with segment 1 in the two-segment solution. Segment 2 in the four-segment solution draws members from two segments. segment two might not be a good target segment because of the lack of stability. Similar segment 3 in the four-segment solution We dont want a cluster just to be splitting up and perhaps being clustered again etc. because that indicates that a model is unstable. Hence this argue for a 2, 3 or 4 cluster model. We select a four cluster model MD.k4 &lt;- MD.km28[[&quot;4&quot;]] MD.k4 ## kcca object of family &#39;kmeans&#39; ## ## call: ## stepFlexclust(x = MD.x, k = 4L, nrep = 10, verbose = FALSE) ## ## cluster sizes: ## ## 1 2 3 4 ## 334 457 237 425 Now we are looking at the segment stability for the 4 cluster model. MD.r4 &lt;- slswFlexclust(MD.x, MD.k4) plot(MD.r4, ylim = 0:1, xlab = &quot;segment number&quot;, ylab = &quot;segment stability&quot;) We see that cluster 1 nd 2 has the lowest stability. 2.5.4.3 Interpreting the clusters STEP 6: Profiling segments The core of the segmentation analysis is now complete; but now we need to understand what the 4-segment k-means solution means. In lecture 1 we used anova; now we use a graphical representation to explore the clusters characteristics. segment profile plot a trick to position similar attribute close to one another is to run hc on attributes (rather than consumers) before plotting MD.vclust &lt;- hclust(dist(t(MD.x))) barchart(MD.k4, shade = TRUE, which = rev(MD.vclust$order)) This plot is easy to managers to interpret. They can see the four segments with the corresponding size. The barplot shows the % of respondents - within each segment, who associate each perception with the fast-food restaurant The line with the dot is the % of respondents - in the entire sample, who associate each perception with the fast-food restaurant. The marker variables differ from the overall sample percentage either by more than 25% points in absolute terms or by more than 50% in relative terms. To understand the market, the manager has to do two things: to compare the bars for each segment with the horizontal line to see what makes each segment distinct from all consumers in the market compare the bars across segments to identify the difference between segments. *Yet another visualization # another helpful visualization for managers plot(MD.k4, project = MD.pca, data = MD.x, hull = FALSE, simlines = FALSE, xlab = &quot;principal component 1&quot;, ylab = &quot;principal component 2&quot;) projAxes(MD.pca) segment 1 and 4 see the restaurant cheap, with segment 1 holding some negative beliefs and segment 4 some positive beliefs segment 2 agrees that the restaurant is not cheap STEP 7: Describing segments analysis based on other variables not used in segmentation analysis Looking into the Like variable We show how it is liked and then see how that is reflected in the clusters. k4 &lt;- clusters(MD.k4) mosaicplot(table(k4, mcdonalds$Like), shade = TRUE, main = &quot;Relation between the clusters and Like&quot;, xlab = &quot;segment number&quot;) colored cells indicate the deviation of observed frequencies from the expected frequencies red means that that sentiment is rarely expresed by that segment blue means that that sentiment is much expresed by that segment Notice that the Y labels are fitted to cluster 1, so it is a bit hard to see. That means for instance cluster 2, there are not any people who loves it, but there is a lot of people who hates it. Looking into gender k4 &lt;- clusters(MD.k4) mosaicplot(table(k4, mcdonalds$Gender), shade = TRUE, main = &quot;&quot;, xlab = &quot;segment number&quot;) in my case, segment 2 contain significanty more men (blue box in the bottom of the second column) the rest have similar gender distribution (all white boxes) Looking into age # because age is metric boxplot(mcdonalds$Age ~ k4, varwidth = TRUE, notch = TRUE, xlab = &quot;segment number&quot;, ylab = &quot;age&quot;) the plot suggests significant differences in average age across segments cluster 1 is dominated by older people; cluster 4 is dominated by the younger people. altternatively, Anova test can be implemented. STEP 8: Selecting (the) target segment(s) mean segment values for other variables and segment evaluation plot library(dplyr) table(mcdonalds$VisitFrequency) #conver VisitFrequency to numeric before mcdonalds$VisitFrequency &lt;- as.factor(mcdonalds$VisitFrequency) # mcdonalds$VisitFrequency &lt;- recode(mcdonalds$VisitFrequency # ,&quot;Never&quot; =&quot;1&quot; # ,&quot;Once a year&quot;=&quot;2&quot; # ,&quot;Every three months&quot; = &quot;3&quot; # ,&quot;Once a month&quot; = &quot;4&quot; # ,&quot;Once a week&quot;=&quot;5&quot; # ,&quot;More than once a week&quot; = &quot;6&quot; # ) visit &lt;- tapply(as.numeric(mcdonalds$VisitFrequency), k4, mean) visit #Mean pr. cluster ## ## Every three months More than once a week Never ## 342 54 131 ## Once a month Once a week Once a year ## 439 235 252 ## 1 2 3 4 ## 3.841317 3.566740 3.523207 3.618824 We see that we have computed the mean for each cluster. Where cluster 4 is the lowest while cluster 1 the highest. Doing the same, but for Like table(mcdonalds$Like) ## ## -1 -2 -3 -4 +1 +2 ## 58 59 73 71 152 187 ## +3 +4 0 I hate it!-5 I love it!+5 ## 229 160 169 152 143 mcdonalds$Like &lt;- as.factor(mcdonalds$Like) # mcdonalds$Like&lt;- recode(mcdonalds$Like # , &quot;I hate it!-5&quot;=&quot;-5&quot; # , &quot;I love it!+5&quot; = &quot;5&quot;) # conver Like to numeric like &lt;- tapply(as.numeric(mcdonalds$Like), k4, mean) like ## 1 2 3 4 ## 5.961078 6.849015 7.949367 7.183529 We see the means again for each cluster. Doing the same, but for gender 1 = Female and 0 = Male female &lt;- tapply((mcdonalds$Gender == &quot;Female&quot;) + 0, k4, mean) # conver gender to numeric female ## 1 2 3 4 ## 0.5778443 0.4529540 0.6075949 0.5741176 # prop.table(table(mcdonalds$Gender, k4), 2) The same as above. Plotting visit frquency against like plot(visit, like ,cex = 8 * female #This is a clever way to control the size of the cluster! ,xlim = c(2, 4.5) ,ylim = c(-4, 4)) text(visit, like, 1:4) The plot obtained is a segment evaluation plot. Market segments 1 and 3 are located in the attractive quadrant of the plot. Members of those segments like the fast-food restaurant and visit it frequently. These segments need to be retained, and their need must be satisfied in the future. Segments 1 (in this run) is located in the least attractive position, they rarely eat there, making it unattractive as a potential market segment. they are, in general, older customers (median &gt; 50 yrs. old). Segment 2 does not currently perceive the restaurant in a positive way. Still, it is a viable market segment (average “Like” is close to zero (neutral)). Manager action could attempt to address the negative perceptions of this segment (they feel it is “expensive”) and reinforce positive perceptions (if any). As a result, the fast-food restaurant can enlarge its customer base. Overall the segment evaluation plot serves as a decision support tool for the management to discuss which of the four segments should be targeted. "],["recommender-systems-and-customer-targeting.html", "3 Recommender Systems and Customer Targeting 3.1 Association rule 3.2 Colaborative filtering 3.3 Latent Factor Models", " 3 Recommender Systems and Customer Targeting This group of lectures cover three different approaches to mining associations. Association rules Collaborative filtering Latent factor models Association rules We see that association rules looks at transactions and assess what products there appear to be on the same transaction, and thus creates association rules based on these. With these rules you can start making recommendations or generate ideas of what products should be marketed together. For instance one of the fun examples is where you see that beers and diapers often comes together. When you think about it, it is very logical, because people with small kids cant go out, but they might still enjoy a beer, thus they have to do it at home. Collaborative filtering Then we have collaborative filtering. Here we will work on two different approaches, item-based and user-based. User-based: we want to predict one users ratings of a given product/service based on similarity with other users. item-based: we want to predict one users ratings of a given product/service based on its similarity with other products, e.g., if one consistenly rate thai food high, then he may also like other thai food, this is in principle also what I have explored with the recommender system on the DSP. This is very much used for making recommender system. This was also very much applied in the Netflix competition. With this, we have the term cold start, that is when you have a new user, you must make some recommendations before you can use the contents, e.g., what you are doing at Netflix, so it starts knowing your preferences. Latent factor models The last approach is the latent factor model, this is just an extension of the user-based and item-based, where both scenarios are considered. This method assumes that we make some constructs, e.g., as in SEM, where the constructs are explained by different items (movies). Then it would be obvious to make constructs based on different genres. These constructs are not directly observable, hence the name latent. Within LFM cold start also applies. In general we see that this model gives the best predictions. That is also due to the possibility of regularizing user ratings, hence accounting for user specific biases. Notice that in the very end of this chapter a comparison between IBCF, UBCF, latent factor models and latent factor models accounting for user bias is shown. 3.1 Association rule Basically the method looks at what products that appear in the same transaction and the finds pairs, or items that come together. Terms: Items: these are the different products that can be bought, can also be a product group Itemset: this is the dataset containing a list of all of the items Syntatic Constrains: constraints involving restrictions on items that can appear in a rule. E.g., one may say, that we only want to evaluate one item (product) or an itemrange. Support Constraints: constraints the number of transactions in T that support a rule. Notice that in R there are default values for this. Minsupport: This is the threshold assigned to item pairs, used in procedure 1. + Large itemssets: Item combinations that meets the minsupport. This is denoted with I1,I2,…Ik. + Small itemssets: Item combinations that does not meet the minsupport Procedure: 1. Generate all combinations of items that have fractional transactinos support. Perhaps above a certain threshold. a. We group all combinations in two groups: i. Large itemsets. Examples: 1) Diaper and beer, 2) Milk, Bread --&gt; Eggs, Coke. ii. Small itemsets. For a large itemset generate all rules, that use items from the large itemsets (I1,I2,…Ik). We call all of these rules subsets (X) of Y. # https://www.datacamp.com/community/tutorials/market-basket-analysis-r #install and load package arules library(arules) library(arulesViz) library(tidyverse) library(readxl) library(knitr) library(ggplot2) library(lubridate) library(plyr) library(dplyr) library(DataExplorer) 3.1.1 Loading and data exploration First we load it #read excel into R dataframe retail &lt;- read_excel(&#39;Data/Recommender Systems/Online_Retail.xlsx&#39;) attach(retail) The data frame has rows with granularity of items, hence one item pr. line. Where one invoice may occur several times. We also have some other information. notice that stock code is an ID for the product. We see that some have some odd quantities, these are just plotted to see what is going on. plot(Quantity[StockCode == 23843]) plot(Quantity[StockCode == 23166]) 3.1.2 Data preprocessing complete.cases(data) will return a logical vector indicating which rows have no missing values. Then use the vector to get only rows that are complete using retail[,]. That is because we have NA’s. plot_missing(retail) We see that the customer ID is only there 75% of the time, also the description appear to be missing in some examples. Now we are going to remove these. retail &lt;- retail[complete.cases(retail), ] plot_missing(retail) Now we see that we remove approx. 140.000 observations, where we had 541.909 observations before and 406.829 observations after removing NA’s, corresponding with approx. 25% of the observations. 3.1.2.1 Encoding to correct data types Now we want to make the description a factor instead of character. #The following prints whole tables, although I just show the first couple of rows in a following chunk retail %&gt;% mutate(Description = as.factor(Description)) retail %&gt;% mutate(Country = as.factor(Country)) retail$Date &lt;- as.Date(retail$InvoiceDate) head(retail) InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country Date 536365 85123A WHITE HANGING HEART T-LIGHT HOLDER 6 2010-12-01 08:26:00 2.55 17850 United Kingdom 2010-12-01 536365 71053 WHITE METAL LANTERN 6 2010-12-01 08:26:00 3.39 17850 United Kingdom 2010-12-01 536365 84406B CREAM CUPID HEARTS COAT HANGER 8 2010-12-01 08:26:00 2.75 17850 United Kingdom 2010-12-01 536365 84029G KNITTED UNION FLAG HOT WATER BOTTLE 6 2010-12-01 08:26:00 3.39 17850 United Kingdom 2010-12-01 536365 84029E RED WOOLLY HOTTIE WHITE HEART. 6 2010-12-01 08:26:00 3.39 17850 United Kingdom 2010-12-01 536365 22752 SET 7 BABUSHKA NESTING BOXES 2 2010-12-01 08:26:00 7.65 17850 United Kingdom 2010-12-01 We also want a column with the date and one with the transaction time. #Extract time from InvoiceDate and store in another variable TransTime &lt;- format(retail$InvoiceDate,&quot;%H:%M:%S&quot;) #Convert and edit InvoiceNo into numeric InvoiceNo &lt;- as.numeric(as.character(retail$InvoiceNo)) #Bind new columns TransTime and InvoiceNo into dataframe retail retail &lt;- cbind(retail,TransTime) Now we can glimpse glimpse(retail) ## Rows: 406,829 ## Columns: 10 ## $ InvoiceNo &lt;chr&gt; &quot;536365&quot;, &quot;536365&quot;, &quot;536365&quot;, &quot;536365&quot;, &quot;536365&quot;, &quot;536365&quot;… ## $ StockCode &lt;chr&gt; &quot;85123A&quot;, &quot;71053&quot;, &quot;84406B&quot;, &quot;84029G&quot;, &quot;84029E&quot;, &quot;22752&quot;, … ## $ Description &lt;chr&gt; &quot;WHITE HANGING HEART T-LIGHT HOLDER&quot;, &quot;WHITE METAL LANTERN… ## $ Quantity &lt;dbl&gt; 6, 6, 8, 6, 6, 2, 6, 6, 6, 32, 6, 6, 8, 6, 6, 3, 2, 3, 3, … ## $ InvoiceDate &lt;dttm&gt; 2010-12-01 08:26:00, 2010-12-01 08:26:00, 2010-12-01 08:2… ## $ UnitPrice &lt;dbl&gt; 2.55, 3.39, 2.75, 3.39, 3.39, 7.65, 4.25, 1.85, 1.85, 1.69… ## $ CustomerID &lt;dbl&gt; 17850, 17850, 17850, 17850, 17850, 17850, 17850, 17850, 17… ## $ Country &lt;chr&gt; &quot;United Kingdom&quot;, &quot;United Kingdom&quot;, &quot;United Kingdom&quot;, &quot;Uni… ## $ Date &lt;date&gt; 2010-12-01, 2010-12-01, 2010-12-01, 2010-12-01, 2010-12-0… ## $ TransTime &lt;chr&gt; &quot;08:26:00&quot;, &quot;08:26:00&quot;, &quot;08:26:00&quot;, &quot;08:26:00&quot;, &quot;08:26:00&quot;… 3.1.2.2 Create a data frame with one row pr. invoice What you need to do is group data in the retail dataframe either by CustomerID, CustomerID, and Date or you can also group data using InvoiceNo and Date. We need this grouping and apply a function on it and store the output in another dataframe. This can be done by ddply. The following lines of code will combine all products from one InvoiceNo and date and combine all products from that InvoiceNo and date as one row, with each item, separated by “,”. library(plyr) transactionData &lt;- ddply(.data = retail ,.variables = c(&quot;InvoiceNo&quot;,&quot;Date&quot;) #Group by ,.fun = function(df1)paste(df1$Description #This concatenates the items ,collapse = &quot;,&quot;)) head(transactionData) InvoiceNo Date V1 536365 2010-12-01 WHITE HANGING HEART T-LIGHT HOLDER,WHITE METAL LANTERN,CREAM CUPID HEARTS COAT HANGER,KNITTED UNION FLAG HOT WATER BOTTLE,RED WOOLLY HOTTIE WHITE HEART.,SET 7 BABUSHKA NESTING BOXES,GLASS STAR FROSTED T-LIGHT HOLDER 536366 2010-12-01 HAND WARMER UNION JACK,HAND WARMER RED POLKA DOT 536367 2010-12-01 ASSORTED COLOUR BIRD ORNAMENT,POPPY’S PLAYHOUSE BEDROOM,POPPY’S PLAYHOUSE KITCHEN,FELTCRAFT PRINCESS CHARLOTTE DOLL,IVORY KNITTED MUG COSY,BOX OF 6 ASSORTED COLOUR TEASPOONS,BOX OF VINTAGE JIGSAW BLOCKS,BOX OF VINTAGE ALPHABET BLOCKS,HOME BUILDING BLOCK WORD,LOVE BUILDING BLOCK WORD,RECIPE BOX WITH METAL HEART,DOORMAT NEW ENGLAND 536368 2010-12-01 JAM MAKING SET WITH JARS,RED COAT RACK PARIS FASHION,YELLOW COAT RACK PARIS FASHION,BLUE COAT RACK PARIS FASHION 536369 2010-12-01 BATH BUILDING BLOCK WORD 536370 2010-12-01 ALARM CLOCK BAKELIKE PINK,ALARM CLOCK BAKELIKE RED,ALARM CLOCK BAKELIKE GREEN,PANDA AND BUNNIES STICKER SHEET,STARS GIFT TAPE,INFLATABLE POLITICAL GLOBE,VINTAGE HEADS AND TAILS CARD GAME,SET/2 RED RETROSPOT TEA TOWELS,ROUND SNACK BOXES SET OF4 WOODLAND,SPACEBOY LUNCH BOX,LUNCH BOX I LOVE LONDON,CIRCUS PARADE LUNCH BOX,CHARLOTTE BAG DOLLY GIRL DESIGN,RED TOADSTOOL LED NIGHT LIGHT,SET 2 TEA TOWELS I LOVE LONDON,VINTAGE SEASIDE JIGSAW PUZZLES,MINI JIGSAW CIRCUS PARADE,MINI JIGSAW SPACEBOY,MINI PAINT SET VINTAGE,POSTAGE So we see that now we have granularity on invoiceNo, and the the date and lastly the items are listed, where they are separated with a comma. Now we only want the items pr. invoice (basket), hence we remove the invoice InvoiceNo and Date. transactionData &lt;- as.data.frame(transactionData$V1) colnames(transactionData) &lt;- c(&quot;items&quot;) #Rename column to items transactionData[1,] ## [1] &quot;WHITE HANGING HEART T-LIGHT HOLDER,WHITE METAL LANTERN,CREAM CUPID HEARTS COAT HANGER,KNITTED UNION FLAG HOT WATER BOTTLE,RED WOOLLY HOTTIE WHITE HEART.,SET 7 BABUSHKA NESTING BOXES,GLASS STAR FROSTED T-LIGHT HOLDER&quot; We see that the dataframe now consists of only on column with all items separated with a comma. Now we are going to save it, and load it back into the environment. Saving the data write.csv(transactionData,&quot;Data/Recommender Systems/market_basket_transactions.csv&quot;, quote = FALSE, row.names = FALSE) 3.1.3 Loading transactions tr &lt;- read.transactions(&#39;market_basket_transactions.csv&#39;, format = &#39;basket&#39;, sep=&#39;,&#39;) summary(tr) ## transactions as itemMatrix in sparse format with ## 22191 rows (elements/itemsets/transactions) and ## 7876 columns (items) and a density of 0.001930725 ## ## most frequent items: ## WHITE HANGING HEART T-LIGHT HOLDER REGENCY CAKESTAND 3 TIER ## 1803 1709 ## JUMBO BAG RED RETROSPOT PARTY BUNTING ## 1460 1285 ## ASSORTED COLOUR BIRD ORNAMENT (Other) ## 1250 329938 ## ## element (itemset/transaction) length distribution: ## sizes ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 3598 1594 1141 908 861 758 696 676 663 593 624 537 516 531 551 522 ## 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ## 464 441 483 419 395 315 306 272 238 253 229 213 222 215 170 159 ## 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 ## 138 142 134 109 111 90 113 94 93 87 88 65 63 67 63 60 ## 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 ## 59 49 64 40 41 49 43 36 29 39 30 27 28 17 25 25 ## 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 ## 20 27 24 22 15 20 19 13 16 16 11 15 12 7 9 14 ## 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 ## 15 12 8 9 11 11 14 8 6 5 6 11 6 4 4 3 ## 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 ## 6 5 2 4 2 4 4 3 2 2 6 3 4 3 2 1 ## 113 114 116 117 118 120 121 122 123 125 126 127 131 132 133 134 ## 3 1 3 3 3 1 2 2 1 3 2 2 1 1 2 1 ## 140 141 142 143 145 146 147 150 154 157 168 171 177 178 180 202 ## 1 2 2 1 1 2 1 1 3 2 2 2 1 1 1 1 ## 204 228 236 249 250 285 320 400 419 ## 1 1 1 1 1 1 1 1 1 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 3.00 10.00 15.21 21.00 419.00 ## ## includes extended item information - examples: ## labels ## 1 1 HANGER ## 2 10 COLOUR SPACEBOY PEN ## 3 12 COLOURED PARTY BALLOONS We see that when we load it back into the environment using read.transactions() we create something similar to a data term matrix, this is merely a sparse matrix, where we see that we have 7.876 items and 22.191 baskets/invoices. Hence each row is just ‘ticked-off’ if the basket contains the product. We see the summary specify that we have 22.191 transactions and 7.876 items, the most frequent is ‘white hanging heeart t-light holder’. We see that the largest basket has 419 units. Where basket sizes of 1 unit is the most typical. Then the mean = 15 products and the median 10 products. 3.1.4 Plotting Now we want to plot the most frequent products. # Create an item frequency plot for the top 20 items library(RColorBrewer) itemFrequencyPlot(tr ,topN = 20 ,type = &quot;absolute&quot; ,col = brewer.pal(8,&#39;Pastel2&#39;) ,main = &quot;Absolute Item Frequency Plot&quot;) We see that top combinations. E.g., more than 1.500 baskets where the top combination occurs. Now we can look at it in a relative way instead. itemFrequencyPlot(tr ,topN = 20 ,type = &quot;relative&quot; ,col = brewer.pal(8,&#39;Pastel2&#39;) ,main = &quot;Relative Item Frequency Plot&quot;) This is basically the same, just written in percentage. 3.1.5 Apriori calculation We will now start looking at association rules. To do so, we apply the apriori principle. We set the following: supp = minimum support, where we see that the combinations must occur in at least 0.1% of the baskets conf = the describes the conditional relationship between the left-hand and right-hand side. Hence we want a relationship that is greater than 80%, as we want to avoid random pairs. You can have a rate combination of items (low support) althought the confidence is high, if these pairs always come together. maxlen = the maximum length of items in a basket. # Min Support as 0.001, confidence as 0.8. association.rules &lt;- apriori(data = tr ,parameter = list(supp = 0.001 #we only see rules where the support is at least 0.001 ,conf = 0.8 #The confidence should be at least 0.8 ,maxlen = 10 #We only want ot look at a max length of 10 items pr. basket ) ) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.8 0.1 1 none FALSE TRUE 5 0.001 1 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 22 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[7876 item(s), 22191 transaction(s)] done [0.11s]. ## sorting and recoding items ... [2324 item(s)] done [0.01s]. ## creating transaction tree ... done [0.01s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 done [0.27s]. ## writing ... [49122 rule(s)] done [0.03s]. ## creating S4 object ... done [0.03s]. The parameters can be seen as constraints, to lower the amount of rules that we are going to make Extra note on support. We only want to see combinations that occurs 1 out of 1000 times, hence supp = 0.001 Now we have calculated association rules lets call the summary. summary(association.rules) ## set of 49122 rules ## ## rule length distribution (lhs + rhs):sizes ## 2 3 4 5 6 7 8 9 10 ## 105 2111 6854 16424 14855 6102 1937 613 121 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 5.000 5.000 5.499 6.000 10.000 ## ## summary of quality measures: ## support confidence coverage lift ## Min. :0.001036 Min. :0.8000 Min. :0.001036 Min. : 9.846 ## 1st Qu.:0.001082 1st Qu.:0.8333 1st Qu.:0.001262 1st Qu.: 22.237 ## Median :0.001262 Median :0.8788 Median :0.001442 Median : 28.760 ## Mean :0.001417 Mean :0.8849 Mean :0.001609 Mean : 64.589 ## 3rd Qu.:0.001532 3rd Qu.:0.9259 3rd Qu.:0.001712 3rd Qu.: 69.200 ## Max. :0.015997 Max. :1.0000 Max. :0.019107 Max. :715.839 ## count ## Min. : 23.00 ## 1st Qu.: 24.00 ## Median : 28.00 ## Mean : 31.45 ## 3rd Qu.: 34.00 ## Max. :355.00 ## ## mining info: ## data ntransactions support confidence ## tr 22191 0.001 0.8 We see some basic descriptions, e.g., we have the most combinations around 5 and 6. We see that the lower end and upper end has very few. My guess would be that for a basket combination of 10 to not be random, there are only few things that are naturally there. In the other end (e.g., with length of 2), then there are many products that comes in pairs, but it is also influenced by a lot of randomness. Hence we are ruling out a lot of combinations with the confidence = 80%, if we were to decrease the confidence, then we could have found more association rules. Now we can inspect the first 10 combinations inspect(association.rules[1:10]) ## lhs rhs support confidence coverage lift count ## [1] {WOBBLY CHICKEN} =&gt; {DECORATION} 0.001261773 1.0000000 0.001261773 443.8200 28 ## [2] {WOBBLY CHICKEN} =&gt; {METAL} 0.001261773 1.0000000 0.001261773 443.8200 28 ## [3] {DECOUPAGE} =&gt; {GREETING CARD} 0.001036456 1.0000000 0.001036456 389.3158 23 ## [4] {BILLBOARD FONTS DESIGN} =&gt; {WRAP} 0.001306836 1.0000000 0.001306836 715.8387 29 ## [5] {WRAP} =&gt; {BILLBOARD FONTS DESIGN} 0.001306836 0.9354839 0.001396963 715.8387 29 ## [6] {ENAMEL PINK TEA CONTAINER} =&gt; {ENAMEL PINK COFFEE CONTAINER} 0.001396963 0.8157895 0.001712406 385.1741 31 ## [7] {WOBBLY RABBIT} =&gt; {DECORATION} 0.001532153 1.0000000 0.001532153 443.8200 34 ## [8] {WOBBLY RABBIT} =&gt; {METAL} 0.001532153 1.0000000 0.001532153 443.8200 34 ## [9] {ART LIGHTS} =&gt; {FUNK MONKEY} 0.001712406 1.0000000 0.001712406 583.9737 38 ## [10] {FUNK MONKEY} =&gt; {ART LIGHTS} 0.001712406 1.0000000 0.001712406 583.9737 38 We see that the confidence is one in many examples, that is for instance because when you have bought wobbly chicken, the you also buy decoration and for this pair it also goes the other way around (actually cant remember why, we probably checked). Lets take the first row as an example: We have wobbly chicken in the basket, then you will also buy decoration. Support = this pair occurs in 0.0013% of the times. Confidence = this is 1, meaning when we have WC then the decoration will always be there Coverage = that is how often the lhs appear in total Lift = an indication of how much it is boosting the sale. We see that when we have a lift of 444, it means that we are more than 400 times boosting the sale of decoration given a sale of wobbly chicken. If you had a lift of 1, then you would have equal probability of buying e.g., decoration given wobbly chicken is in the basket. On the other side, lets say that lift is less than 1, then you prone to not buying. See the equation in the following. We see an example where the coverage is not one, that is because when you buy one, then you do not always buy the other. That is calculated by: \\[\\frac{support}{coverage} = confidence\\] We see that: Support = \\(\\frac{TotalInvoices}{AbsoluteNo.OfTransactions} = Support\\) Coverage = \\(\\frac{Support}{Confidence} = Coverage\\) Lift = \\(\\frac{P(A|B)}{P(A)} = Lift\\), we see with lift, that we actually need to find out how the pairs appear. See an example in the following. Lift example Manual calculation of support, coverage and confidence and lift We are going to do three things: Count no. of transactions with the left hand side combination Count the no. of transactions where the right hand side combination is within the transactions found in step 1. Divide support by coverage to find confidence. If 1 = the right hand side combination is always in the basket when the left hand side is, when e.g., 0.5 instead, then the right hand side is only half of the time in the basket when the left hand side is. #We are going to look at the pair woobly chicken and decoration left &lt;- &quot;WRAP&quot; right &lt;- &quot;BILLBOARD FONTS DESIGN&quot; #Find row number of a given item idx &lt;- which(t(t(as.vector(tr@itemInfo))) == left) table(tr@data[idx,]) ## ## FALSE TRUE ## 22160 31 coverage = as.vector(table(tr@data[idx,])[2]/ncol(tr@data)) idx &lt;- which(t(t(as.vector(tr@itemInfo))) == left) idx_right &lt;- which(t(t(as.vector(tr@itemInfo))) == right) #which(tr@data[idx,] == TRUE) #which(tr@data[idx_right,] == TRUE) right_side &lt;- table(which(tr@data[idx,] == TRUE) %in% which(tr@data[idx_right,] == TRUE)) right_side #Inteprete to see if there are only T, F or both. ## ## FALSE TRUE ## 2 29 #We see that decoration is always in the basket when wobbly chicken is. support = as.vector(right_side[2]/ncol(tr@data)) support ## [1] 0.001306836 confidence = support / coverage confidence #Hence a very stron relationship from woobly chicken to decoration ## [1] 0.9354839 lift p_WC_given_d &lt;- 28/28 #because they are both there 28 times. p_WC &lt;- 50/22191 #The support of WC p_WC_given_d/p_WC #444 ## [1] 443.82 Now we can inspect the rules after they are sorted sortedRules &lt;- sort(association.rules,by=&quot;lift&quot;,decreasing=TRUE) inspect(sortedRules[c(1:10)]) Notice that bookdown gave an error so I just manually inserted the output Sorted Rules Output The conclusions are the same, it is just sorted now. The LIFT reflects how often the LHS is bought given the RHS is in the basket. 3.1.5.1 Another example where maxlen = 3 Here we only want to look at combinations with up to three items. shorter.association.rules &lt;- apriori(tr, parameter = list(supp=0.001 #must occur in 1 in 1000 ,conf=0.8 #We need to pairs to be at least 80% of the time together ,maxlen=3)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.8 0.1 1 none FALSE TRUE 5 0.001 1 ## maxlen target ext ## 3 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 22 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[7876 item(s), 22191 transaction(s)] done [0.10s]. ## sorting and recoding items ... [2324 item(s)] done [0.01s]. ## creating transaction tree ... done [0.01s]. ## checking subsets of size 1 2 3 done [0.13s]. ## writing ... [2216 rule(s)] done [0.02s]. ## creating S4 object ... done [0.01s]. Notice that the RHS and LHS summarize up to 3, where before it could go all the way up to 10. 3.1.5.2 Filtering on right hand side. An example with the item {METAL} We want to see what is purchased before buying metal. #For example, to find what customers buy before buying &#39;METAL&#39; run the following line of code metal.association.rules &lt;- apriori(data = tr ,parameter = list(supp=0.001, conf=0.8) ,appearance = list(default=&quot;lhs&quot;,rhs=&quot;METAL&quot;)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.8 0.1 1 none FALSE TRUE 5 0.001 1 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 22 ## ## set item appearances ...[1 item(s)] done [0.00s]. ## set transactions ...[7876 item(s), 22191 transaction(s)] done [0.12s]. ## sorting and recoding items ... [2324 item(s)] done [0.01s]. ## creating transaction tree ... done [0.01s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 done [0.31s]. ## writing ... [5 rule(s)] done [0.03s]. ## creating S4 object ... done [0.01s]. inspect(metal.association.rules) ## lhs rhs support confidence coverage ## [1] {WOBBLY CHICKEN} =&gt; {METAL} 0.001261773 1 0.001261773 ## [2] {WOBBLY RABBIT} =&gt; {METAL} 0.001532153 1 0.001532153 ## [3] {DECORATION} =&gt; {METAL} 0.002253166 1 0.002253166 ## [4] {DECORATION,WOBBLY CHICKEN} =&gt; {METAL} 0.001261773 1 0.001261773 ## [5] {DECORATION,WOBBLY RABBIT} =&gt; {METAL} 0.001532153 1 0.001532153 ## lift count ## [1] 443.82 28 ## [2] 443.82 34 ## [3] 443.82 50 ## [4] 443.82 28 ## [5] 443.82 34 Now we see the combinations where the metal is the right hand side. Where we which products that leads to purchase of metal. 3.1.5.3 Filtering on left hand side. An example with the item {METAL} We can do the same for the left hand side. metal.association.rules &lt;- apriori(data = tr ,parameter = list(supp=0.001, conf=0.8) ,appearance = list(lhs=&quot;METAL&quot;,default=&quot;rhs&quot;)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.8 0.1 1 none FALSE TRUE 5 0.001 1 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 22 ## ## set item appearances ...[1 item(s)] done [0.00s]. ## set transactions ...[7876 item(s), 22191 transaction(s)] done [0.11s]. ## sorting and recoding items ... [2324 item(s)] done [0.01s]. ## creating transaction tree ... done [0.01s]. ## checking subsets of size 1 2 done [0.00s]. ## writing ... [1 rule(s)] done [0.00s]. ## creating S4 object ... done [0.01s]. inspect(head(metal.association.rules)) ## lhs rhs support confidence coverage lift count ## [1] {METAL} =&gt; {DECORATION} 0.002253166 1 0.002253166 443.82 50 Where this is showing what products metal leads to buying. We see only one item that is followed by buying metal. 3.1.6 Plotting association rules We want to get all where the confidence is greater than 40%. #This is just showed as an example, so we can do the filtering association.rules &lt;- apriori(data = tr ,parameter = list(supp=0.001, conf=0.3,maxlen=10)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.3 0.1 1 none FALSE TRUE 5 0.001 1 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 22 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[7876 item(s), 22191 transaction(s)] done [0.11s]. ## sorting and recoding items ... [2324 item(s)] done [0.01s]. ## creating transaction tree ... done [0.01s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 done [0.27s]. ## writing ... [180603 rule(s)] done [0.05s]. ## creating S4 object ... done [0.05s]. #Subsetting subRules &lt;- association.rules[quality(association.rules)$confidence&gt;0.4] We see that the higher the confidence the higher the lift, so there appears to be a strong relationship between these. We can also show it in the following. #Plot SubRules plot(subRules) We see that usually the lift is the greatest in the top left corner, where the confidence is high and support is low. This makes sense, as you cannot lift (boost) the sales of another product, if their relationship is already rather high. Now we can plot the length of the rule with respect to confidence #The order is the number of items in the rule plot(subRules,method=&quot;two-key plot&quot;) The following is supposed to be an interactive plot. But does not appear to work, missing a library. # plotly_arules(subRules) Now we can plot some interactive plot, where one can explore the rules. top10subRules &lt;- head(subRules, n = 10, by = &quot;confidence&quot;) plot(top10subRules, method = &quot;graph&quot;, engine = &quot;htmlwidget&quot;) rm(list = ls()) 3.2 Colaborative filtering As mentioned in the beginning of the chapter, we see that one can make recommendations based on similarity with other users and or similarity between products. 3.2.1 Loading and formatting data library(recommenderlab) library(tidyverse) data(&quot;MovieLense&quot;) # Data is given in realRatingMatrix format ; Optimized to store sparse matrices class(MovieLense) str(MovieLense,vec.len=2) #not as we normally reference list elements by \\\\$ but \\\\@ ## [1] &quot;realRatingMatrix&quot; ## attr(,&quot;package&quot;) ## [1] &quot;recommenderlab&quot; ## Formal class &#39;realRatingMatrix&#39; [package &quot;recommenderlab&quot;] with 2 slots ## ..@ data :Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots ## .. .. ..@ i : int [1:99392] 0 1 4 5 9 ... ## .. .. ..@ p : int [1:1665] 0 452 583 673 882 ... ## .. .. ..@ Dim : int [1:2] 943 1664 ## .. .. ..@ Dimnames:List of 2 ## .. .. .. ..$ : chr [1:943] &quot;1&quot; &quot;2&quot; ... ## .. .. .. ..$ : chr [1:1664] &quot;Toy Story (1995)&quot; &quot;GoldenEye (1995)&quot; ... ## .. .. ..@ x : num [1:99392] 5 4 4 4 4 ... ## .. .. ..@ factors : list() ## ..@ normalize: NULL the following are the different methods that we can use. methods(class = class(MovieLense)) # methods applicable to this class ## [1] [ [&lt;- binarize ## [4] calcPredictionAccuracy coerce colCounts ## [7] colMeans colSds colSums ## [10] denormalize dim dimnames ## [13] dimnames&lt;- dissimilarity evaluationScheme ## [16] getData.frame getList getNormalize ## [19] getRatingMatrix getRatings getTopNLists ## [22] hasRating image nratings ## [25] Recommender removeKnownRatings rowCounts ## [28] rowMeans rowSds rowSums ## [31] sample show ## see &#39;?methods&#39; for accessing help and source code We see the dimensions of our matrix with users and the movies. dim(MovieLense) ## [1] 943 1664 We see that there 1.664 movies and 943 users. 3.2.1.1 Loading metadata that gets loaded with main dataset This is not applied in the exercise, but just shown moviemeta &lt;- MovieLenseMeta class(moviemeta) ## [1] &quot;data.frame&quot; We see that it is a data.frame. It has the following column names: colnames(moviemeta) ## [1] &quot;title&quot; &quot;year&quot; &quot;url&quot; &quot;unknown&quot; &quot;Action&quot; ## [6] &quot;Adventure&quot; &quot;Animation&quot; &quot;Children&#39;s&quot; &quot;Comedy&quot; &quot;Crime&quot; ## [11] &quot;Documentary&quot; &quot;Drama&quot; &quot;Fantasy&quot; &quot;Film-Noir&quot; &quot;Horror&quot; ## [16] &quot;Musical&quot; &quot;Mystery&quot; &quot;Romance&quot; &quot;Sci-Fi&quot; &quot;Thriller&quot; ## [21] &quot;War&quot; &quot;Western&quot; We see that the genre for each movie is presented. 3.2.2 Data Exploration We can see the topics for each movie. Only used for exploration purposes. #What do we know about the films? library(pander) pander(head(moviemeta,2),caption = &quot;First few Rows within Movie Meta Data &quot;) First few Rows within Movie Meta Data (continued below) title year Toy Story (1995) 1995 GoldenEye (1995) 1995 Table continues below url unknown Action http://us.imdb.com/M/title-exact?Toy%20Story%20(1995) 0 0 http://us.imdb.com/M/title-exact?GoldenEye%20(1995) 0 1 Table continues below Adventure Animation Children’s Comedy Crime Documentary Drama 0 1 1 1 0 0 0 1 0 0 0 0 0 0 Table continues below Fantasy Film-Noir Horror Musical Mystery Romance Sci-Fi Thriller 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 War Western 0 0 0 0 We can also interpret the ratings for a given customer ## look at the first few ratings of the first user head(as(MovieLense[1,], &quot;list&quot;)[[1]]) ## Toy Story (1995) ## 5 ## GoldenEye (1995) ## 3 ## Four Rooms (1995) ## 4 ## Get Shorty (1995) ## 3 ## Copycat (1995) ## 3 ## Shanghai Triad (Yao a yao yao dao waipo qiao) (1995) ## 5 3.2.2.1 Plotting ## number of ratings per user hist(rowCounts(MovieLense)) We see that most has reviewed less than 100 movies. Therefore we also see in the following plot, that most movies have less than 100 reviews, where we have a long tail with some movies that have very few ratings. ## number of ratings per movie hist(colCounts(MovieLense)) Now we can look at which movies that are top 10, based on number of times watched. #Top 10 movies movie_watched &lt;- data.frame( movie_name = names(colCounts(MovieLense)), watched_times = colCounts(MovieLense) ) top_ten_movies &lt;- movie_watched[order(movie_watched$watched_times, decreasing = TRUE), ][1:10, ] ggplot(top_ten_movies) + aes(x=movie_name, y=watched_times) + geom_bar(stat = &quot;identity&quot;,fill = &quot;firebrick4&quot;, color = &quot;dodgerblue2&quot;) + xlab(&quot;Movie Tile&quot;) + ylab(&quot;Count&quot;) + theme(axis.text = element_text(angle = 40, hjust = 1)) Looking at the overall data, we see that the mean rating is 3.5 where the median is actually 4, which seem pretty high. summary(getRatings(MovieLense)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 3.00 4.00 3.53 4.00 5.00 This we can also plot. data.frame(ratings = getRatings(MovieLense)) %&gt;% ggplot(aes(ratings)) + geom_bar(width = 0.75)+ labs(title = &#39;MovieLense Ratings Distribution&#39;) 3.2.3 What models can we use? Here we can see some of the recommender models that are available. #Different recommender models recommender_models &lt;- recommenderRegistry$get_entries(dataType=&quot;realRatingMatrix&quot;) names(recommender_models) ## [1] &quot;HYBRID_realRatingMatrix&quot; &quot;ALS_realRatingMatrix&quot; ## [3] &quot;ALS_implicit_realRatingMatrix&quot; &quot;IBCF_realRatingMatrix&quot; ## [5] &quot;LIBMF_realRatingMatrix&quot; &quot;POPULAR_realRatingMatrix&quot; ## [7] &quot;RANDOM_realRatingMatrix&quot; &quot;RERECOMMEND_realRatingMatrix&quot; ## [9] &quot;SVD_realRatingMatrix&quot; &quot;SVDF_realRatingMatrix&quot; ## [11] &quot;UBCF_realRatingMatrix&quot; #recommenderRegistry$get_entries(dataType=&quot;realRatingMatrix&quot;) #If run, you will see the parameters We will focus on: UBCF IBCF SVD ALS SVDF 3.2.4 Training and test set We want movies with at least 30 ratings and at least 100 users for each items. We see that the following approach is actually pretty clever, where we see that we filter on rows and columns by making the logical directly in the subsetting. #Training and test set At least 30 items evaluated or at least 100 users for each item rates &lt;- MovieLense[rowCounts(MovieLense) &gt; 30 ,colCounts(MovieLense) &gt; 100] rates1 &lt;- rates[rowCounts(rates) &gt; 30,] #This is included again, to make sure that we have accounted for both Now we can split the data into train and test data. We randomly define the which_train vector that is TRUE for users in the training set and FALSE for the others. Will set the probability in the training set as 80% which_train &lt;- sample(x = c(TRUE, FALSE), size = nrow(rates1), replace = TRUE, prob = c(0.8, 0.2)) #Random sampling # Define the training and the test sets recc_data_train &lt;- rates1[which_train, ] recc_data_test &lt;- rates1[!which_train, ] My guess is that recc is for recommender. 3.2.5 A small example with IBCF (item based) Fitting the model # Let&#39;s build the recommender IBCF - cosine: recc_model &lt;- Recommender(data = recc_data_train #On the train data ,method = &quot;IBCF&quot; #The col. filt. model ,parameter = list(k = 30) #No. of neighbors ) We have now created a IBCF Recommender Model Predicting recommendations on test data We want 5 recommendations for each user, based on the test data set. n_recommended &lt;- 5 recc_predicted &lt;- predict(object = recc_model ,newdata = recc_data_test ,n = n_recommended) # This is the recommendation for the first user recc_predicted@items[[1]] ## [1] 239 296 330 314 227 These are the recommendations for the given user. Now we can look for the movei name, which we are recommending. Now we want to convert the numbers into the actual movie names. # Now let&#39;s define a list with the recommendations for each user recc_matrix &lt;- lapply(recc_predicted@items, function(x){ colnames(rates)[x] }) # Let&#39;s take a look the recommendations for the first four users: recc_matrix[1:4] ## $`1` ## [1] &quot;Highlander (1986)&quot; &quot;Stand by Me (1986)&quot; &quot;Con Air (1997)&quot; ## [4] &quot;Ransom (1996)&quot; &quot;Batman (1989)&quot; ## ## $`8` ## [1] &quot;Net, The (1995)&quot; &quot;Ed Wood (1994)&quot; &quot;Lion King, The (1994)&quot; ## [4] &quot;Time to Kill, A (1996)&quot; &quot;Air Force One (1997)&quot; ## ## $`11` ## [1] &quot;Jurassic Park (1993)&quot; &quot;Carrie (1976)&quot; ## [3] &quot;Peacemaker, The (1997)&quot; &quot;Wizard of Oz, The (1939)&quot; ## [5] &quot;Stand by Me (1986)&quot; ## ## $`14` ## [1] &quot;Hoop Dreams (1994)&quot; &quot;Searching for Bobby Fischer (1993)&quot; ## [3] &quot;Sting, The (1973)&quot; &quot;Young Frankenstein (1974)&quot; ## [5] &quot;Edge, The (1997)&quot; Then we see recommendations for four persons. 3.2.6 UBCF - User based collaborative based The method computes the similarity between users with cosine 3.2.6.1 Fitting the model # Let&#39;s build a recommender model leaving the parameters to their defaults. recc_model &lt;- Recommender(data = recc_data_train ,method = &quot;UBCF&quot;) A UBCF recommender has now been created 3.2.6.2 Predicting (making) recommendations n_recommended &lt;- 5 recc_predicted &lt;- predict(object = recc_model ,newdata = recc_data_test ,n = n_recommended) # Let&#39;s define a list with the recommendations to the test set users. recc_matrix &lt;- sapply(recc_predicted@items, function(x) { colnames(rates)[x] }) # Again, let&#39;s look at the first four users recc_matrix[1:4] ## [1] &quot;In the Name of the Father (1993)&quot; ## [2] &quot;Like Water For Chocolate (Como agua para chocolate) (1992)&quot; ## [3] &quot;Shine (1996)&quot; ## [4] &quot;Glory (1989)&quot; Now we get recommendations based on user based filtering. 3.2.7 Cross validation The purpose is to make a more stable model through cross validation. The cross validation, that we set up here, is going to be used in the following code, for estimating models. We can split the data into some chunks, take a chunk out as the test set, and evaluate the accuracy. Then we can do the same with each other chunk and compute the average accuracy. Here we construct the evaluation model n_fold &lt;- 4 rating_threshold &lt;- 4 # threshold at which we consider the item to be good items_to_keep &lt;- 20 # given=20 means that while testing the model use only 20 randomly picked ratings from every user to predict the unknown ratings # in the test set the known data set has the ratings specified by given and the unknown data set the remaining ratings used for validation eval_sets &lt;- evaluationScheme(data = rates1 ,method = &quot;cross-validation&quot; ,k = n_fold, given = items_to_keep ,goodRating = rating_threshold) size_sets &lt;-sapply(eval_sets@runsTrain, length) size_sets ## [1] 459 459 459 459 3.2.8 IBCF - Item based collaborative based 3.2.8.1 Fitting the model (using CV) model_to_evaluate &lt;- &quot;IBCF&quot; model_parameters &lt;- NULL # we use the standard settings eval_recommender &lt;-Recommender(data = getData(eval_sets, &quot;train&quot;) ,method = model_to_evaluate ,parameter = model_parameters) 3.2.8.2 Making predictions The IBCF can recommend new items and predict their ratings. In order to build the model, we need to specify how many items we want to recommend, for example, 5. items_to_recommend &lt;- 5 # We can build the matrix with the predicted ratings using the predict function: eval_prediction &lt;- predict(object = eval_recommender ,newdata = getData(eval_sets, &quot;known&quot;) ,n = items_to_recommend ,type = &quot;ratings&quot;) 3.2.8.3 Performance metrics Per user By using the calcPredictionAccuracy, we can calculate: the Root mean square error (RMSE), Mean squared error (MSE), and the Mean absolute error (MAE). We want to compare the results on each user, hence we take the head. In the following we are also plotting a histogram. eval_accuracy &lt;- calcPredictionAccuracy( x = eval_prediction ,data = getData(eval_sets, &quot;unknown&quot;) ,byUser = TRUE #Because, how good does the model work for each user ) # This is a small sample of the results for the Prediction and Accuracy head(eval_accuracy) ## RMSE MSE MAE ## 1 1.497832 2.243502 1.1888625 ## 12 1.035603 1.072474 0.6004122 ## 14 1.290864 1.666331 0.9981729 ## 15 1.612897 2.601437 1.2568863 ## 20 1.436871 2.064599 1.2632984 ## 32 1.287141 1.656732 1.0935495 # Now, let&#39;s take a look at the RMSE by each user qplot(eval_accuracy[,&quot;RMSE&quot;]) + geom_histogram(binwidth = 0.1) + ggtitle(&quot;Distribution of the RMSE by user&quot;) We see that the typical RMSE is between 1 and 1.5. Overall The following is the average for all users # However, we need to evaluate the model as a whole, so we will set the byUser to False eval_accuracy &lt;- calcPredictionAccuracy( x = eval_prediction ,data = getData(eval_sets, &quot;unknown&quot;) ,byUser = FALSE #We want it for the whole model ) eval_accuracy #for IBCF ## RMSE MSE MAE ## 1.2865540 1.6552211 0.9776134 The mean absolute error is just above 1, hence one rating off. Lets us take a look at the confusionmatrix to see how it looks there. Notice that we specificy n, that is because we want predictions for different number of predictions. # Confusion matrix good threshold =4 results &lt;- evaluate(x = eval_sets #Using the Cross validation ,method = model_to_evaluate ,n = seq(10, 100, 10)) #We make from 10 to 100 recommendations ## IBCF run fold/sample [model time/prediction time] ## 1 [0.148sec/0.037sec] ## 2 [0.149sec/0.037sec] ## 3 [0.142sec/0.036sec] ## 4 [0.156sec/0.037sec] results object is an evaluationResults object containing the results of the evaluation. Each element of the list corresponds to a different split of the k-fold. Let’s look at the first element #Results for each CV getConfusionMatrix(results)[[1]] #the first cross validation, 2 would be the second CV etc. ## TP FP FN TN N precision recall TPR ## [1,] 1.750000 8.25000 43.30769 258.6923 312 0.1750000 0.03764419 0.03764419 ## [2,] 3.320513 16.67949 41.73718 250.2628 312 0.1660256 0.07092946 0.07092946 ## [3,] 4.980769 25.01923 40.07692 241.9231 312 0.1660256 0.10898860 0.10898860 ## [4,] 6.262821 33.73718 38.79487 233.2051 312 0.1565705 0.13940092 0.13940092 ## [5,] 7.705128 42.29487 37.35256 224.6474 312 0.1541026 0.17067432 0.17067432 ## [6,] 9.076923 50.92308 35.98077 216.0192 312 0.1512821 0.19979843 0.19979843 ## [7,] 10.416667 59.58333 34.64103 207.3590 312 0.1488095 0.23239186 0.23239186 ## [8,] 11.666667 68.33333 33.39103 198.6090 312 0.1458333 0.26205976 0.26205976 ## [9,] 12.967949 77.03205 32.08974 189.9103 312 0.1440883 0.28825533 0.28825533 ## [10,] 14.198718 85.80128 30.85897 181.1410 312 0.1419872 0.31749146 0.31749146 ## FPR n ## [1,] 0.03084949 10 ## [2,] 0.06239415 20 ## [3,] 0.09371258 30 ## [4,] 0.12651359 40 ## [5,] 0.15869446 50 ## [6,] 0.19111871 60 ## [7,] 0.22358664 70 ## [8,] 0.25637477 80 ## [9,] 0.28901715 90 ## [10,] 0.32195177 100 #getConfusionMatrix(results)[[2]] #the first cross validation, 2 would be the second CV etc. #getConfusionMatrix(results)[[3]] #the first cross validation, 2 would be the second CV etc. #getConfusionMatrix(results)[[4]] #the first cross validation, 2 would be the second CV etc. We see that the more recommendations we make, the ‘better’ does the results get, although making 100 recommendations also means that the FP is also increasing, hence we start recommending something, where it did not turn out to be purchased. Hence it is a trade-off. In this case, look at the first four columns True Positives (TP): These are recommended items that have been purchased. False Positives (FP): These are recommended items that haven’t been purchased False Negatives (FN): These are not recommended items that have been purchased. True Negatives (TN): These are not recommended items that haven’t been purchased. Manually making the ROC curve # If we want to take account of all the splits at the same time, we can just sum up the indices: columns_to_sum &lt;- c(&quot;TP&quot;, &quot;FP&quot;, &quot;FN&quot;, &quot;TN&quot;) indices_summed &lt;- Reduce(&quot;+&quot;, getConfusionMatrix(results))[, columns_to_sum] TPR &lt;- indices_summed[,&quot;TP&quot;]/(indices_summed[,&quot;TP&quot;] + indices_summed[,&quot;FN&quot;]) FPR &lt;- indices_summed[,&quot;FP&quot;]/(indices_summed[,&quot;FP&quot;] + indices_summed[,&quot;TN&quot;]) indices_summed &lt;- cbind(indices_summed,TPR,FPR) indices_summed ## TP FP FN TN TPR FPR ## [1,] 7.051282 32.94872 174.0897 1033.9103 0.03892703 0.03088386 ## [2,] 13.416667 66.58333 167.7244 1000.2756 0.07406752 0.06241062 ## [3,] 19.666667 100.33333 161.4744 966.5256 0.10857102 0.09404554 ## [4,] 25.141026 134.85897 156.0000 932.0000 0.13879255 0.12640750 ## [5,] 30.705128 169.29487 150.4359 897.5641 0.16950952 0.15868533 ## [6,] 35.858974 204.14103 145.2821 862.7179 0.19796164 0.19134771 ## [7,] 41.224359 238.77564 139.9167 828.0833 0.22758157 0.22381181 ## [8,] 46.384615 273.61538 134.7564 793.2436 0.25606908 0.25646818 ## [9,] 51.615385 308.36538 129.5256 758.4936 0.28494586 0.28904044 ## [10,] 56.891026 343.02564 124.2500 723.8333 0.31407035 0.32152857 plot(x = TPR &lt;- indices_summed[,&quot;FPR&quot;],y = TPR &lt;- indices_summed[,&quot;TPR&quot;],type = &quot;b&quot; ,cex = 0 #To remove dots. ,xlab = &quot;FPR&quot; ,ylab = &quot;TPR&quot; ) text(x = TPR &lt;- indices_summed[,&quot;FPR&quot;],y = TPR &lt;- indices_summed[,&quot;TPR&quot;],labels = seq(10,100,10)) title(&quot;ROC&quot;) 3.2.9 Building ROC curve and precision / recall rate Building an ROC curve. Will need these factors True Positive Rate (TPR): Percentage of purchased items that have been recommended. TP/(TP + FN) False Positive Rate (FPR): Percentage of not purchased items that have been recommended. FP/(FP + TN) plot(results, annotate = TRUE, main = &quot;ROC curve&quot;) Now we can also make a precision / recall rate We can also look at the accuracy metrics as well: Precision: Percentage of recommended items that have been purchased. FP/(TP + FP) Recall: Percentage of purchased items that have been recommended. TP/(TP + FN) = True Positive Rate plot(results, &quot;prec/rec&quot;, annotate = TRUE, main = &quot;Precision-Recall&quot;) The plots above gives an indication of how many recommendations to make. Notice that it is based upon the results object that we are making, there we specify the intervals for which we are going to recommend. 3.2.10 Comparing UBCF and IBCF let us see how the models compare #Comparing models models_to_evaluate &lt;- list(IBCF_cos = list(name = &quot;IBCF&quot;, param = list(method = &quot;cosine&quot;)), #Cosine correl. coef. IBCF_cor = list(name = &quot;IBCF&quot;, param = list(method = &quot;pearson&quot;)), #Pearson correl. coef. UBCF_cos = list(name = &quot;UBCF&quot;, param = list(method = &quot;cosine&quot;)), UBCF_cor = list(name = &quot;UBCF&quot;, param = list(method = &quot;pearson&quot;)), random = list(name = &quot;RANDOM&quot;, param = NULL)) # In order to evaluate the models, we need to test them, varying the number of items. n_recommendations &lt;- c(1,5,seq(10,100,10)) # Now let&#39;s run and evaluate the models list_results &lt;- evaluate(x = eval_sets, method = models_to_evaluate, n = n_recommendations) ## IBCF run fold/sample [model time/prediction time] ## 1 [0.146sec/0.038sec] ## 2 [0.145sec/0.036sec] ## 3 [0.154sec/0.036sec] ## 4 [0.155sec/0.102sec] ## IBCF run fold/sample [model time/prediction time] ## 1 [0.242sec/0.036sec] ## 2 [0.232sec/0.036sec] ## 3 [0.233sec/0.037sec] ## 4 [0.227sec/0.036sec] ## UBCF run fold/sample [model time/prediction time] ## 1 [0.004sec/0.237sec] ## 2 [0.003sec/0.305sec] ## 3 [0.004sec/0.239sec] ## 4 [0.005sec/0.298sec] ## UBCF run fold/sample [model time/prediction time] ## 1 [0.004sec/0.268sec] ## 2 [0.003sec/0.339sec] ## 3 [0.005sec/0.26sec] ## 4 [0.004sec/0.328sec] ## RANDOM run fold/sample [model time/prediction time] ## 1 [0sec/0.041sec] ## 2 [0.001sec/0.046sec] ## 3 [0.001sec/0.039sec] ## 4 [0.001sec/0.04sec] Now we can plot the roc curves and the and the precision - recall rate. plot(list_results, annotate = 1, legend = &quot;topleft&quot;) title(&quot;ROC curve&quot;) plot(list_results, &quot;prec/rec&quot;, annotate = 1, legend = &quot;bottomright&quot;, ylim = c(0,0.4)) title(&quot;Precision-recall&quot;) We see that the IBCF Pearson correlation coefficient gets the best result. We also see that the random recommendations is almost as good as some of the other methods. Other evaluation criteria Coverage - what we have seen earlier Diversity and novelty - To avoid monotone lists, discover new families of items Serendipity - Unexpected and surprisng items might be valuable Familiarity - GIve the user the impression of understanding his/her needs Biases - Do we just end up recommended the best movies? hence there is a great bias. rm(list = ls()) 3.3 Latent Factor Models This is a snippet from the introduction in the chapter: The last approach is the latent factor model, this is just an extension of the user-based and item-based, where both scenarios are considered. This method assumes that we make some constructs, e.g., as in SEM, where the constructs are explained by different items (movies). Then it would be obvious to make constructs based on different genres. These constructs are not directly observable, hence the name latent. Within LFM cold start also applies How is correlation measured? We see that the dot product (inner product) between items and users, here we are able to interpret the projection of these from the center to see how similar/dissimilar they are. From the image we can see that Gus and the guy in the top right is more or less perpendicular, so they do not share much interest on the y axis, although on the x axis they appear to be equally far out, hence the dot product appears to be able to reflect similarity between perople. . Hence to make these projections we need to find to matrices - namely U and V. To estimate the values for these to matrices, we have used one approach, the Signular Value Decomposition (SVD). Notice that this approach demands that we have a full matrix, as we often have sparse matrices, is that it replace missing values with the column means. This introduce a lot of bias, but makes the model work. For this method we cannot really avoid it, because each movie is never reviewed by all users. Notice that for this method we are able to account for bias for ratings, e.g., one person may often give relatively higher ratings than other, hence we can regularize this to get rid of some bias. library(recommenderlab) library(tidyverse) data(MovieLense) class(MovieLense) ## [1] &quot;realRatingMatrix&quot; ## attr(,&quot;package&quot;) ## [1] &quot;recommenderlab&quot; help(MovieLense) dim(MovieLense) ## [1] 943 1664 Select only the users who have rated at least 50 movies or movies that had been rated more than 100 times. As if they did not rate more than this it starts getting difficult recommeding movies for them. (ratings_movies &lt;- MovieLense[rowCounts(MovieLense) &gt; 50, colCounts(MovieLense) &gt; 100]) ## 560 x 332 rating matrix of class &#39;realRatingMatrix&#39; with 55298 ratings. # use the minimum number of items purchased by any user to decide item number to keep min(rowCounts(ratings_movies)) n_fold &lt;- 4 items_to_keep &lt;- 15 #randomly chosen items from a given responded rating_threshold &lt;- 3 #A good rating is three or higher ## [1] 18 In the following we include SVD-Funk, there is a bunch of tuning parameters, these are: k = number of topics gamma = Learning parameter / learning rate lambda = The regularization Notice that there are default values, although we choose to set k to 50. # Use k-fold to validate models eval_sets &lt;- evaluationScheme(data = ratings_movies ,method = &quot;cross-validation&quot; ,k = n_fold ,given = items_to_keep ,goodRating = rating_threshold) #We want to comapre different approaches models &lt;- list( IBCF = list(name = &quot;IBCF&quot;,param = list(method = &quot;cosine&quot;)), UBCF = list(name = &quot;UBCF&quot;, param = list(method = &quot;pearson&quot;)), SVD = list(name = &quot;SVD&quot;, param = list(k = 50)), #50 concepts included SVDF = list(name = &quot;SVDF&quot;, param = list(k = 50)) #50 concepts included ) notice that the SVDF takes some time to compute. That is beacuse it has to go through all the known ratings and construct the U and V matrix. # varying the number of items we want to recommend to users n_rec &lt;- c(1, 5, seq(10, 100, 10)) # evaluating the recommendations results &lt;- evaluate(x = eval_sets, method = models, n= n_rec) ## IBCF run fold/sample [model time/prediction time] ## 1 [0.142sec/0.036sec] ## 2 [0.138sec/0.033sec] ## 3 [0.137sec/0.031sec] ## 4 [0.14sec/0.033sec] ## UBCF run fold/sample [model time/prediction time] ## 1 [0.004sec/0.415sec] ## 2 [0.003sec/0.226sec] ## 3 [0.004sec/0.232sec] ## 4 [0.003sec/0.228sec] ## SVD run fold/sample [model time/prediction time] ## 1 [0.128sec/0.044sec] ## 2 [0.12sec/0.034sec] ## 3 [0.132sec/0.034sec] ## 4 [0.123sec/0.035sec] ## SVDF run fold/sample [model time/prediction time] ## 1 [52.911sec/12.98sec] ## 2 [54.063sec/12.735sec] ## 3 [51.292sec/12.718sec] ## 4 [56.732sec/13.261sec] # extract the related average confusion matrices (avg_matrices &lt;- lapply(results, avg)) plot(results, annotate=TRUE) plot(results, &quot;prec/rec&quot;, annotate = TRUE, main = &quot;Precision-Recall&quot;) ## $IBCF ## TP FP FN TN N precision recall ## [1,] 0.3303571 0.6696429 72.75357 243.2464 317 0.3303571 0.004568692 ## [2,] 1.5160714 3.4839286 71.56786 240.4321 317 0.3032143 0.020634324 ## [3,] 2.8767857 7.1232143 70.20714 236.7929 317 0.2876786 0.040088734 ## [4,] 5.3482143 14.6517857 67.73571 229.2643 317 0.2674107 0.074195302 ## [5,] 7.7696429 22.2303571 65.31429 221.6857 317 0.2589881 0.108197019 ## [6,] 10.0553571 29.9446429 63.02857 213.9714 317 0.2513839 0.138813338 ## [7,] 12.2535714 37.7464286 60.83036 206.1696 317 0.2450714 0.169588086 ## [8,] 14.4321429 45.5678571 58.65179 198.3482 317 0.2405357 0.200229136 ## [9,] 16.5714286 53.4160714 56.51250 190.5000 317 0.2367549 0.229422172 ## [10,] 18.7178571 61.2267857 54.36607 182.6893 317 0.2340674 0.257042328 ## [11,] 20.8678571 69.0125000 52.21607 174.9036 317 0.2320829 0.286646823 ## [12,] 22.9678571 76.8339286 50.11607 167.0821 317 0.2300182 0.314631273 ## TPR FPR n ## [1,] 0.004568692 0.002698827 1 ## [2,] 0.020634324 0.014071290 5 ## [3,] 0.040088734 0.028856182 10 ## [4,] 0.074195302 0.059630384 20 ## [5,] 0.108197019 0.090767888 30 ## [6,] 0.138813338 0.122424915 40 ## [7,] 0.169588086 0.154621771 50 ## [8,] 0.200229136 0.186736794 60 ## [9,] 0.229422172 0.219029327 70 ## [10,] 0.257042328 0.251059370 80 ## [11,] 0.286646823 0.283069688 90 ## [12,] 0.314631273 0.315243075 100 ## ## $UBCF ## TP FP FN TN N precision recall ## [1,] 0.200000 0.800000 72.88393 243.1161 317 0.2000000 0.003102085 ## [2,] 1.089286 3.910714 71.99464 240.0054 317 0.2178571 0.015381877 ## [3,] 2.339286 7.660714 70.74464 236.2554 317 0.2339286 0.033106915 ## [4,] 5.080357 14.919643 68.00357 228.9964 317 0.2540179 0.071419240 ## [5,] 7.907143 22.092857 65.17679 221.8232 317 0.2635714 0.111028558 ## [6,] 10.662500 29.337500 62.42143 214.5786 317 0.2665625 0.149532758 ## [7,] 13.430357 36.569643 59.65357 207.3464 317 0.2686071 0.188680995 ## [8,] 16.210714 43.789286 56.87321 200.1268 317 0.2701786 0.226556760 ## [9,] 18.898214 51.101786 54.18571 192.8143 317 0.2699745 0.263241745 ## [10,] 21.560714 58.439286 51.52321 185.4768 317 0.2695089 0.298798935 ## [11,] 24.225000 65.775000 48.85893 178.1411 317 0.2691667 0.334021810 ## [12,] 26.925000 73.075000 46.15893 170.8411 317 0.2692500 0.370086522 ## TPR FPR n ## [1,] 0.003102085 0.003318243 1 ## [2,] 0.015381877 0.016126021 5 ## [3,] 0.033106915 0.031489110 10 ## [4,] 0.071419240 0.061072796 20 ## [5,] 0.111028558 0.090076120 30 ## [6,] 0.149532758 0.119608890 40 ## [7,] 0.188680995 0.149044295 50 ## [8,] 0.226556760 0.178323728 60 ## [9,] 0.263241745 0.208088165 70 ## [10,] 0.298798935 0.237866098 80 ## [11,] 0.334021810 0.267605469 90 ## [12,] 0.370086522 0.297205841 100 ## ## $SVD ## TP FP FN TN N precision recall ## [1,] 0.450000 0.550000 72.63393 243.3661 317 0.4500000 0.006932589 ## [2,] 2.092857 2.907143 70.99107 241.0089 317 0.4185714 0.032002768 ## [3,] 3.908929 6.091071 69.17500 237.8250 317 0.3908929 0.058777921 ## [4,] 7.021429 12.978571 66.06250 230.9375 317 0.3510714 0.102528131 ## [5,] 9.905357 20.094643 63.17857 223.8214 317 0.3301786 0.143496673 ## [6,] 12.557143 27.442857 60.52679 216.4732 317 0.3139286 0.180218464 ## [7,] 15.187500 34.812500 57.89643 209.1036 317 0.3037500 0.217443813 ## [8,] 17.698214 42.301786 55.38571 201.6143 317 0.2949702 0.252858144 ## [9,] 19.951786 50.048214 53.13214 193.8679 317 0.2850255 0.283024508 ## [10,] 22.310714 57.689286 50.77321 186.2268 317 0.2788839 0.313963244 ## [11,] 24.585714 65.414286 48.49821 178.5018 317 0.2731746 0.345222131 ## [12,] 26.835714 73.164286 46.24821 170.7518 317 0.2683571 0.377419316 ## TPR FPR n ## [1,] 0.006932589 0.00217082 1 ## [2,] 0.032002768 0.01154666 5 ## [3,] 0.058777921 0.02428955 10 ## [4,] 0.102528131 0.05221222 20 ## [5,] 0.143496673 0.08114971 30 ## [6,] 0.180218464 0.11096119 40 ## [7,] 0.217443813 0.14107822 50 ## [8,] 0.252858144 0.17165752 60 ## [9,] 0.283024508 0.20347839 70 ## [10,] 0.313963244 0.23468178 80 ## [11,] 0.345222131 0.26641002 90 ## [12,] 0.377419316 0.29820879 100 ## ## $SVDF ## TP FP FN TN N precision recall ## [1,] 0.4642857 0.5357143 72.61964 243.3804 317 0.4642857 0.006798236 ## [2,] 2.0607143 2.9392857 71.02321 240.9768 317 0.4121429 0.029918990 ## [3,] 3.9517857 6.0482143 69.13214 237.8679 317 0.3951786 0.057327356 ## [4,] 7.1517857 12.8482143 65.93214 231.0679 317 0.3575893 0.101596714 ## [5,] 10.1982143 19.8017857 62.88571 224.1143 317 0.3399405 0.143170359 ## [6,] 13.1017857 26.8982143 59.98214 217.0179 317 0.3275446 0.181778879 ## [7,] 15.8625000 34.1375000 57.22143 209.7786 317 0.3172500 0.219532352 ## [8,] 18.7410714 41.2589286 54.34286 202.6571 317 0.3123512 0.257835571 ## [9,] 21.4839286 48.5160714 51.60000 195.4000 317 0.3069133 0.294144039 ## [10,] 24.2910714 55.7089286 48.79286 188.2071 317 0.3036384 0.331652928 ## [11,] 27.0857143 62.9142857 45.99821 181.0018 317 0.3009524 0.369219330 ## [12,] 29.7785714 70.2214286 43.30536 173.6946 317 0.2977857 0.404098654 ## TPR FPR n ## [1,] 0.006798236 0.002107731 1 ## [2,] 0.029918990 0.011670172 5 ## [3,] 0.057327356 0.024138018 10 ## [4,] 0.101596714 0.051502000 20 ## [5,] 0.143170359 0.079566522 30 ## [6,] 0.181778879 0.108251975 40 ## [7,] 0.219532352 0.137661019 50 ## [8,] 0.257835571 0.166475776 60 ## [9,] 0.294144039 0.195882123 70 ## [10,] 0.331652928 0.225024274 80 ## [11,] 0.369219330 0.254149429 90 ## [12,] 0.404098654 0.283631574 100 We will see from the plot how the different models perform. We see that the SVDF appear to have the best performance. 3.3.1 Fitting the model recommender_ibcf &lt;- Recommender(data = getData(eval_sets, &quot;train&quot;), method = &quot;IBCF&quot;,parameter = list(method = &quot;cosine&quot;)) recommender_ubcf &lt;- Recommender(data = getData(eval_sets, &quot;train&quot;), method = &quot;UBCF&quot;,parameter = list(method = &quot;pearson&quot;)) recommender_svd &lt;- Recommender(data = getData(eval_sets, &quot;train&quot;), method = &quot;SVD&quot;,parameter = list(k=50)) recommender_svdf &lt;- Recommender(data = getData(eval_sets, &quot;train&quot;), method = &quot;SVDF&quot;,parameter = list(k=50)) items_to_recommend &lt;- 10 3.3.2 Making predictions #Produce predictions based upon the known dataset eval_prediction_ibcf &lt;- predict(object = recommender_ibcf ,newdata = getData(eval_sets, &quot;known&quot;) ,n = items_to_recommend ,type = &quot;ratings&quot;) eval_prediction_ubcf &lt;- predict(object = recommender_ubcf ,newdata = getData(eval_sets, &quot;known&quot;) ,n = items_to_recommend ,type = &quot;ratings&quot;) eval_prediction_svd &lt;- predict(object = recommender_svd ,newdata = getData(eval_sets, &quot;known&quot;) ,n = items_to_recommend ,type = &quot;ratings&quot;) eval_prediction_svdf &lt;- predict(object = recommender_svdf ,newdata = getData(eval_sets, &quot;known&quot;) ,n = items_to_recommend ,type = &quot;ratings&quot;) Now we want to compare the different models on the unknown data. ######################RANDOM###################### #UBCF eval_accuracy_ubcf &lt;- calcPredictionAccuracy( x = eval_prediction_ubcf, data = getData(eval_sets, &quot;unknown&quot;), byUser = F) eval_accuracy_ubcf_user &lt;- calcPredictionAccuracy( x = eval_prediction_ubcf, data = getData(eval_sets, &quot;unknown&quot;), byUser = TRUE) print(&quot;UBCF&quot;) head(eval_accuracy_ubcf_user) #IBCF eval_accuracy_ibcf &lt;- calcPredictionAccuracy( x = eval_prediction_ibcf, data = getData(eval_sets, &quot;unknown&quot;), byUser = F) eval_accuracy_ibcf_user &lt;- calcPredictionAccuracy( x = eval_prediction_ibcf, data = getData(eval_sets, &quot;unknown&quot;), byUser = TRUE) print(&quot;IBCF&quot;) head(eval_accuracy_ibcf_user) #SVD eval_accuracy_svd &lt;- calcPredictionAccuracy( x = eval_prediction_svd, data = getData(eval_sets, &quot;unknown&quot;), byUser = F) eval_accuracy_svd_user &lt;- calcPredictionAccuracy( x = eval_prediction_svd, data = getData(eval_sets, &quot;unknown&quot;), byUser = TRUE) print(&quot;SVD&quot;) head(eval_accuracy_svd_user) #SVDF eval_accuracy_svdf &lt;- calcPredictionAccuracy( x = eval_prediction_svdf, data = getData(eval_sets, &quot;unknown&quot;), byUser = F) eval_accuracy_svdf_user &lt;- calcPredictionAccuracy( x = eval_prediction_svdf, data = getData(eval_sets, &quot;unknown&quot;), byUser = TRUE) print(&quot;SVDF&quot;) head(eval_accuracy_svdf_user) ## [1] &quot;UBCF&quot; ## RMSE MSE MAE ## 5 1.3368297 1.7871136 1.1166378 ## 8 1.0496554 1.1017764 0.8630784 ## 10 0.6969517 0.4857417 0.5665927 ## 24 0.6808052 0.4634957 0.5013154 ## 26 0.7600131 0.5776199 0.5963810 ## 37 1.1899559 1.4159951 0.9527017 ## [1] &quot;IBCF&quot; ## RMSE MSE MAE ## 5 1.7149052 2.9408998 1.4097262 ## 8 1.3435709 1.8051829 0.8648377 ## 10 0.7288311 0.5311948 0.5706625 ## 24 1.0849837 1.1771897 0.7312437 ## 26 0.8459852 0.7156910 0.5825797 ## 37 1.4891927 2.2176948 1.1190274 ## [1] &quot;SVD&quot; ## RMSE MSE MAE ## 5 1.2528478 1.5696277 1.0431439 ## 8 1.0341382 1.0694418 0.8087618 ## 10 0.5723975 0.3276389 0.5225907 ## 24 0.7489073 0.5608622 0.6738023 ## 26 0.8562948 0.7332408 0.6644771 ## 37 1.0759907 1.1577561 0.9287478 ## [1] &quot;SVDF&quot; ## RMSE MSE MAE ## 5 1.1476491 1.3170984 0.9137723 ## 8 0.7923530 0.6278232 0.6298550 ## 10 0.5562461 0.3094097 0.4909283 ## 24 0.6734846 0.4535816 0.5358979 ## 26 0.7109840 0.5054983 0.5710451 ## 37 1.0980914 1.2058048 0.8323201 We can then assess the RMSE for each model. eval_accuracy_ubcf eval_accuracy_ibcf eval_accuracy_svd eval_accuracy_svdf ## RMSE MSE MAE ## 1.0437604 1.0894357 0.8148581 ## RMSE MSE MAE ## 1.2996132 1.6889945 0.9752173 ## RMSE MSE MAE ## 0.9606769 0.9229001 0.7682577 ## RMSE MSE MAE ## 0.8972149 0.8049946 0.6967501 We see that the SVDF appear to do the best. "],["probabilistic-models-for-customer-analytics.html", "4 Probabilistic Models for Customer Analytics 4.1 Bayesian Networks 4.2 Application 1: Explaining and Predicting Customer Retention in a Virtual Community 4.3 Application 2: Identifying populations with Positive Expected Lift in Profit (ELP) and targeting 4.4 Application 3: Product Recommendation using Probabilistic Models (BN) 4.5 Latent models for cross selling and acquisition sequence", " 4 Probabilistic Models for Customer Analytics 4.1 Bayesian Networks Why do we use bayesian network analysis? We have seen EFA, CFA and lastly SEM, which are structural models where you assume relationships among variables. Bayesian network is applied to identify causality in the data. Notice that Bayesian Networks ONLY deals with categorical data, hence if you are given a continous dataset, then you want to encode it as factors before modeling, e.g., in ordered categories, non-ordered, or as binary. 4.1.1 Why Bayesian? We apply the bayes theorem to update probabilities in light of new evidence. A quick summary: \\[P(A|B) = P(A)\\frac{P(B|A)}{P(B)}\\] Where we see A is the outcome given event B happening. This means: \\(P(A)\\) = prior, i.e., unconditional probability, hence not accounting for event B. \\(P(A|B)\\) = the posterior, i.e., conditional probaiblity, here we account for event A happening given event B is happening. \\(P(B)\\) = Plausible cause, i.e., the predictor. \\(P(B|A)\\) = The likelihood, i.e., conditional probability of B given that event A occurs. We should notice that this assumes that there is a dependency between the variables. As if there were no dependence then event A happening given event B happening is not changed at all, for instance wind gusts in Denmark and temperature in Denmark must have a dependency on each other, e.g., when it is very windy it is probable that the temperature will decrease. Although when it is windy in Australia one may not infer a relationship between these two variables. This could also be written with \\(P(A|B) = P(Y)\\), as event B has no effect at all. 4.1.1.1 Many names for Bayesian Networks Causal probabilistic networks Probabilistic graphical models Bayesian Belief networks Causal networks Directed graphs Probabilistic expert systems Influence diagrams 4.1.2 Terms and explanatory examples Whe have the following terms: DAG: Directed Acyclic Graph: meaning that we have graphs with directed arrows that cannot lead to cycles. Within the structure we have the following terms: Nodes = variable Arcs = links = relationships (the arrows) Path = A path where you are only able to directions of the arrows. Chain = This is path, but you are not constrained to following the directions. A cycle = that would be that a path is starting and able to end in the same node. in BN we are not able to have these instances. Connected graph = Where all nodes are connected Empty graph = A graph with nodes, but not with arcs. Multivariate analysis: Where the graph is the means and the variable relationships the ends. Bayesian Networks is a stochastic data-mining technique. It applies different elements to construct the model. We see in the following picture, that there is a relationship from Job down to Credit. These we call the parent and the child (root and non-root). Hence it is clear the the direction is also from Job to Credit. Simple example of a Bayesian network It can be seen in the model that the prior \\(P(A) = 0.915\\), hence a probability of approx. 91% being that you have a job. Then we see the different kinds of probabilities of B and non B (credit) given a person having a job or not having a job. We see a more extensive example here: Extensive example of a Bayesian network Notice that the decendents are effetcs of the given variables, e.g., that a given season influence the probability of how it is going to rain or sprinkle etc. In general we should be able to say, that a guy has a broken leg and then we can calculate backwards what the probabilities of the different variables being occurent. Recall that each connected variables (and adjacent) have a conditional probability between them, meaning that we can take the same example as above: Example: We know a guy has broken his leg, we can now calculate backwards and use the probabilities for the certain events to get an idea of the whole situation. Hence it is probable that it was slippery, due to rain or sprinkles and ultimately due to the season, hence the information flows. On the other hand, if we find out that it was in fact slippery, hence we impose evidence, i.e., control for slippery, we see that broken leg and (sprinkle,rain) is completely independent of each other, as we know that it was slippery, hence no more information is needed. 4.1.2.1 Types of connections We have three types of connections: Serial connection, Explaining away (discounting), Common cause (divergent) I have summarized the approaches in the following visualization and following by an example for each. Examples of connections Serial connection example:We have a scenario where wet is the cause of slippery, and slippery is the cause of breaking legs. Lets say that we know that a person has a broken leg, hence we can update the probability for it being slippery and then if it was wet. this will also increase probabilities for B (slippery) and C (Wet) Now lets say that we also know that it was slippery any new information about C (broken leg condition) is not changing A. And changes in A (the wet condition) will not affect C (broken leg condition) because these are now known as we found evidence, e.g., if we find out that it was not wet, but we still know that it was slippery (I guess that would also require some other relationship to B) Explaining away example: We have a scenario where both having covid and a cold is the cause of dry coughing, hence dry coughing being the children while the others the parents. Notice that A and B is completely independent. Lets say that a person has symptoms C and we later find out that the person has covid (A). Hence we set probability of A = 100 (or in practive it would probably be 99%) then the probability of B will decrease as it is now less likely that the person is also having a could, because we have found one cause for the symptoms. Common cause example: We have a scenario where we have children that are related to the same parents. We now know that the house is burning, this will increase the probability that of B (forgetting stove) is also higher. This leads with the probability of food being overcooked is also increasing. Now lets say that we get the information that you did in fact forget the stove. Hence any new information in A (about burning house) will not change anything to the probability of C (overcooked meal) 4.1.2.2 Variable selection based on a Bayesian Network We know that a Bayesian network can be used to indentify causality and hence dependencies among variables, given that we apply an algorithm for this (more about this in a later section), we can now start inferring important variables based on this. But how to do this? Recall that parents are the causes while the children being the outcome. Let us say that we have a child, which we want to analyze, e.g., a burning house, and we want to explore the causes. What would be natural to do? One could take the following variables: The child’s parents’ The child’s children. Other parents of the child. This is called the the Markov Blanket. Hence one is able to find the Markov Blanket of a variable, meaning identifying variables that you must find information about (hence control for, i.e., evidence) making the child conditional independent of all other variables. Meaning that any change in any given non-related variable will not affect the variable that we want to look into. Therefore, Bayesian networks in combination with Markov Blanket method is a strong combination for variable selection. Although one must be aware that, the Markov Blanket model build on the Markov property: For each variable X in a graph, X is conditionally independent of the set of all nondescendents given the set of all its parents That means that all we need to do is check for a nodes parents, if we can find evidance on these, then the given node is not dependent on all non descendents. In laymans words it can be said, that if a node is not a descendent (can be found by following the arc directions), then it is a nondescendent. The following draws an example. library(ggdag) #Generating the relationships dag &lt;- dagitty::dagitty(&quot;dag { X -&gt; Y -&gt; W X -&gt; Z -&gt; W X -&gt; Z -&gt; V }&quot; ) tidy_dag &lt;- tidy_dagitty(dag) #Adding information on the color tidy_dag$data &lt;- dplyr::mutate(tidy_dag$data, colour = ifelse(tidy_dag$data$name == &quot;Z&quot; ,&quot;Reference point&quot; ,ifelse(tidy_dag$data$name == &quot;X&quot;,&quot;Parent&quot; ,ifelse(tidy_dag$data$name %in% c(&quot;V&quot;,&quot;W&quot;),&quot;Descendent&quot;,&quot;Non descendents&quot;)))) #Making the plot tidy_dag %&gt;% ggplot(aes( x = x, y = y, xend = xend, yend = yend )) + geom_dag_point(aes(colour = colour)) + geom_dag_edges() + geom_dag_text() + theme_dag() We that for Z, V and W are its descendents. We see that X is its parent and Y being its ancestor i.e., nondescendent. We can make Z independent (hence it being conditionally independent of Y) of Y, if we just control for X as it is its parent. This mean that the graph must include all relevant variables, hence you cannot have hidden causes. In such as case we cannot say that it is a complete Bayesian Network, although the model may still hold and can be used for prediction. In general what we see here, is that this is no different that we have previously seen in machine learning, that you can search for the true model, although truth is utopia that can only very rarely if ever be achieved. I am actually a bit in doubt whether this is actually a true statement I make here, perhaps one could perhaps just say that when you have found the Markov Blanket in your model, then changes in all variables that become conditional independent cannot have an effect on the variable under evaluation if their ‘state’ changes, e.g., we impose evidence. 4.1.3 Independence We have three different scenarios: No dependence between two variables. Dependence between two variables. Conditional independence between two variables. This is the example with the broken leg, where we can make two non adjacent nodes in a serial correlation independent if we control for a given variable, that is why the name conditional independence, because variables can become independent, if check for the variable inbetween, because then you don’t need to know the ancestor (parent of the parent). See a visual example of these. Dependence examples Notice that in the last scenario we see that the A is no longer depend on B because we account for C, hence we have a conditional indepedency, where we see the chain reaction, that A is dependent on C which is dependent on B. With Bayesian networks we are able to graph this and find the dependencies and independencies and also the directions hence the causality among the variables. Therefore, we see that constrained based algorithms explores this, where score-based algorithms it primarily focusing on making good predictions. 4.1.4 Methods for identifying the network There are many different methods, although we are going to focus on two different schools: 4.1.4.1 1. Algorithms focused on causality discovery We are going to work with constrained based algorithms. Basically what this is doing is: 1. Assess data 2. Identify conditional independence tests 3. Create arcs (arrows, i.e., dependencies) based on the dependencies we find in step two. Therefore, this approach is heavily relying on the principles found the section above, regarding independence. Algorithms: There are different approaches, we are going to apply the Grow-Shrink (gs). Limitations: 1) This approach requires sufficient data to learn conditional independencies; 2) we manually have set the significance levels, hence we introduce bias. Hence we want the graph to be stable, e.g., if we introduce new data we should not see that the model changes. 4.1.4.2 2. Algorithms focused on prediction We are going to work with score-based algorithms. This is developed after the method above, hence it is also building on their algorithms. The aim for this algorithm is to make predictions, and hence not primarily identify causalities. It works in the following way: All possible DAGs (Directed Acyclic Graphs) are drawed. Evaluate and score all DAGs based on different information criteria, e.g., AIC and BIC. Choose the DAG with the highest score. We are going to work with the algorithm Hill Climbing (hs), which is a greedy search algorithm, hence acting according to the procedure above. The reason that this is not used for discovering causality, is that for instance the HS approach is looking at the model on an overall level and not on each ARC, hence we cannot rely on actual causality in the ARC, although it we can by coincidence find actual causalities. 4.1.5 Real life application There are not many applications for Bayesian Networks within Customer Analytics, but there are some examples. Although it is becoming more used. But examples are: Medical diagnostics Mechanical diagnostics Weather prediction Stock market prediction Bank services Insurances Computer science for robotic vision And other examples, all though they are limited, see the slide We see that we can use this to find the directions between the nodes and we are able to infer the probabilities between the nodes and scenarios. Example from the slides: We can use the network to simulate situations, f.eks. if we have some information on customers and we want to know if we should send out mails/commercials to the given customer. Hence we can apply a BN to see how characteristics of a given customer affects him making a purchase or not. Then we can introduce a variable about marketing or not to see how this affects the outcome. Naturally, this requires that we have data to support this. Assume that we have the data from the example, then we can calculate the expected Lift in profit for a customer by simulating whether we market or not, e.g., finding out whehter it is worth to market or not by looking at the probabilities of making a purchase. 4.2 Application 1: Explaining and Predicting Customer Retention in a Virtual Community Getting the structure Building the structure manually. Here we manually decide the arrows and hence the directions. Learning the structure from data. Here we are going to learn the arcs based on actual data. Model evaluation Making inference 4.2.1 1. Getting the structure 4.2.1.1 A. Building the structure and parameters manually There are three different approaches 4.2.1.1.1 Approaches to make the network Three approaches: we are going to see three different approaches to making this. Use set.arc() Construct a matrix, with two columns, from and to Manually write the conditional probabilties. First we will build the graph based on a given structure. Notice that this corresponds with the graph in the slides: 4.2.1.1.1.1 (i) First approach library(bnlearn) # Create an empty graph dag &lt;- empty.graph(nodes = c(&quot;Fuse&quot;,&quot;Plea&quot;,&quot;Atti&quot;,&quot;Comm&quot;)) dag ## ## Random/Generated Bayesian network ## ## model: ## [Fuse][Plea][Atti][Comm] ## nodes: 4 ## arcs: 0 ## undirected arcs: 0 ## directed arcs: 0 ## average markov blanket size: 0.00 ## average neighbourhood size: 0.00 ## average branching factor: 0.00 ## ## generation algorithm: Empty We see that the arcs are empty. Now we can start building the structure, hence adding arrows. # Add the arcs that encode the direct dependencies between variables dag &lt;- set.arc(dag, from = &quot;Fuse&quot;, to = &quot;Atti&quot;) dag &lt;- set.arc(dag, from = &quot;Plea&quot;, to = &quot;Atti&quot;) dag &lt;- set.arc(dag, from = &quot;Fuse&quot;, to = &quot;Comm&quot;) dag &lt;- set.arc(dag, from = &quot;Plea&quot;, to = &quot;Comm&quot;) dag &lt;- set.arc(dag, from = &quot;Atti&quot;, to = &quot;Comm&quot;) # Print the DAG dag ## ## Random/Generated Bayesian network ## ## model: ## [Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti] ## nodes: 4 ## arcs: 5 ## undirected arcs: 0 ## directed arcs: 5 ## average markov blanket size: 3.00 ## average neighbourhood size: 2.50 ## average branching factor: 1.25 ## ## generation algorithm: Empty we see the directed graph here. Now we can get the model string, also show the nodes. # Direct dependencies are listed for each variable: modelstring(dag) ## [1] &quot;[Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti]&quot; Notice that this is written as the conditional probabilities. We see that Fuse and Plea are parent nodes. We see that attitude is based on Functional usefulness and Pleasure Now we can explore it visually. # Explore the elements of the graphical network nodes(dag) arcs(dag) plot(dag) ## [1] &quot;Fuse&quot; &quot;Plea&quot; &quot;Atti&quot; &quot;Comm&quot; ## from to ## [1,] &quot;Fuse&quot; &quot;Atti&quot; ## [2,] &quot;Plea&quot; &quot;Atti&quot; ## [3,] &quot;Fuse&quot; &quot;Comm&quot; ## [4,] &quot;Plea&quot; &quot;Comm&quot; ## [5,] &quot;Atti&quot; &quot;Comm&quot; Notice that throughout the code, the Rgraphviz package is applied to better looking plots, although I have not been able to install it. Although it is not necessary. # Optional # library(Rgraphviz) # graphviz.plot(dag) 4.2.1.1.1.2 (ii) Second approach Another way to build a large network from scratch is to define the nodes and create a matrix to set the whole arc set at once: dag2 &lt;- empty.graph(nodes = c(&quot;Fuse&quot;,&quot;Plea&quot;,&quot;Atti&quot;,&quot;Comm&quot;)) arcs(dag2) = matrix (c(&quot;Fuse&quot;, &quot;Atti&quot;, &quot;Plea&quot;, &quot;Atti&quot;, &quot;Fuse&quot;, &quot;Comm&quot;, &quot;Plea&quot;, &quot;Comm&quot;, &quot;Atti&quot;, &quot;Comm&quot;), byrow = TRUE, ncol = 2, dimnames = list (NULL, c(&quot;from&quot;, &quot;to&quot;))) plot(dag2) We see that we get the exact same plot. 4.2.1.1.1.3 (iii) Third approach An even easier way to build the DAG when we know the structure: Notice that this is the output that we have seen from the first approach. When we are having a simple network it is doable, but with larger networks it gets more difficult. dag3 &lt;- model2network(&quot;[Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti]&quot;) plot(dag3) # graphviz.plot (dag3) We see that they are the same. Although we can also use a function to check for this. # Compare dags all.equal(dag, dag2) all.equal(dag, dag3) ## [1] TRUE ## [1] TRUE We see that all methods are the same. 4.2.1.1.2 Loading/entering data We see that we are manually entering the matrices: # Introducing the parameters manually Fuse.lv &lt;- c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;) Plea.lv &lt;- c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;) Atti.lv &lt;- c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;) Comm.lv &lt;- c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;) Fuse.prob &lt;- array(c(0.02, 0.26, 0.72), dim = 3, dimnames = list(Fuse = Fuse.lv)) Fuse.prob Plea.prob &lt;- array(c(0.01, 0.55, 0.44), dim = 3, dimnames= list(Plea = Plea.lv)) Plea.prob Atti.prob &lt;- array(c(0.99, 0.01, 0.00, 0.00, 0.67, 0.33, 0.01, 0.99, 0.00, 0.34, 0.33, 0.33, 0.00, 0.79, 0.21, 0.00, 0.40, 0.60, 0.99, 0.01, 0.00, 0.00, 0.47, 0.53, 0.00, 0.09, 0.91), dim = c(3, 3, 3), dimnames = list(Atti = Atti.lv, Plea = Plea.lv, Fuse = Fuse.lv)) Atti.prob ## Fuse ## Low Med High ## 0.02 0.26 0.72 ## Plea ## Low Med High ## 0.01 0.55 0.44 ## , , Fuse = Low ## ## Plea ## Atti Low Med High ## Low 0.99 0.00 0.01 ## Med 0.01 0.67 0.99 ## High 0.00 0.33 0.00 ## ## , , Fuse = Med ## ## Plea ## Atti Low Med High ## Low 0.34 0.00 0.0 ## Med 0.33 0.79 0.4 ## High 0.33 0.21 0.6 ## ## , , Fuse = High ## ## Plea ## Atti Low Med High ## Low 0.99 0.00 0.00 ## Med 0.01 0.47 0.09 ## High 0.00 0.53 0.91 We see that all these are probabilities for the given situations, just as we have seen previously with naive bayes. Comm.prob &lt;- array(c(0.00, 1.00, 0.00, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.00, 1.00, 0.00, 1.00, 0.00, 0.00, 0.34, 0.33, 0.33, 0.00, 1.00, 0.00, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.00, 0.98, 0.02, 0.00, 0.83, 0.17, 0.34, 0.33, 0.33, 0.00, 0.33, 0.67, 0.00, 0.44, 0.56, 1.00, 0.00, 0.00, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.00, 0.84, 0.16, 0.00, 0.71, 0.29, 0.34, 0.33, 0.33, 0.00, 0.40, 0.60, 0.00, 0.10, 0.90), dim = c(3, 3, 3, 3), dimnames= list(Comm = Comm.lv, Atti = Atti.lv, Plea = Plea.lv, Fuse = Fuse.lv)) Comm.prob ## , , Plea = Low, Fuse = Low ## ## Atti ## Comm Low Med High ## Low 0 0.34 0.34 ## Med 1 0.33 0.33 ## High 0 0.33 0.33 ## ## , , Plea = Med, Fuse = Low ## ## Atti ## Comm Low Med High ## Low 0.34 0 1 ## Med 0.33 1 0 ## High 0.33 0 0 ## ## , , Plea = High, Fuse = Low ## ## Atti ## Comm Low Med High ## Low 0.34 0 0.34 ## Med 0.33 1 0.33 ## High 0.33 0 0.33 ## ## , , Plea = Low, Fuse = Med ## ## Atti ## Comm Low Med High ## Low 0.34 0.34 0.34 ## Med 0.33 0.33 0.33 ## High 0.33 0.33 0.33 ## ## , , Plea = Med, Fuse = Med ## ## Atti ## Comm Low Med High ## Low 0.34 0.00 0.00 ## Med 0.33 0.98 0.83 ## High 0.33 0.02 0.17 ## ## , , Plea = High, Fuse = Med ## ## Atti ## Comm Low Med High ## Low 0.34 0.00 0.00 ## Med 0.33 0.33 0.44 ## High 0.33 0.67 0.56 ## ## , , Plea = Low, Fuse = High ## ## Atti ## Comm Low Med High ## Low 1 0.34 0.34 ## Med 0 0.33 0.33 ## High 0 0.33 0.33 ## ## , , Plea = Med, Fuse = High ## ## Atti ## Comm Low Med High ## Low 0.34 0.00 0.00 ## Med 0.33 0.84 0.71 ## High 0.33 0.16 0.29 ## ## , , Plea = High, Fuse = High ## ## Atti ## Comm Low Med High ## Low 0.34 0.0 0.0 ## Med 0.33 0.4 0.1 ## High 0.33 0.6 0.9 Now where we have defined the numbers, we want to combine the matrices and the graphical structure. 4.2.1.1.3 Conditional probability table (CPT) Now we are going to combine the probabilities into a list # Relate the CPT to the labels cpt &lt;- list(Fuse = Fuse.prob, Plea = Plea.prob, Atti = Atti.prob, Comm = Comm.prob) # Relate the DAG and CPT and define a fully-specified BN bn &lt;- custom.fit(dag, cpt) bn ## ## Bayesian network parameters ## ## Parameters of node Fuse (multinomial distribution) ## ## Conditional probability table: ## Fuse ## Low Med High ## 0.02 0.26 0.72 ## ## Parameters of node Plea (multinomial distribution) ## ## Conditional probability table: ## Plea ## Low Med High ## 0.01 0.55 0.44 ## ## Parameters of node Atti (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = Low ## ## Fuse ## Atti Low Med High ## Low 0.99 0.34 0.99 ## Med 0.01 0.33 0.01 ## High 0.00 0.33 0.00 ## ## , , Plea = Med ## ## Fuse ## Atti Low Med High ## Low 0.00 0.00 0.00 ## Med 0.67 0.79 0.47 ## High 0.33 0.21 0.53 ## ## , , Plea = High ## ## Fuse ## Atti Low Med High ## Low 0.01 0.00 0.00 ## Med 0.99 0.40 0.09 ## High 0.00 0.60 0.91 ## ## ## Parameters of node Comm (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = Low, Atti = Low ## ## Fuse ## Comm Low Med High ## Low 0.00 0.34 1.00 ## Med 1.00 0.33 0.00 ## High 0.00 0.33 0.00 ## ## , , Plea = Med, Atti = Low ## ## Fuse ## Comm Low Med High ## Low 0.34 0.34 0.34 ## Med 0.33 0.33 0.33 ## High 0.33 0.33 0.33 ## ## , , Plea = High, Atti = Low ## ## Fuse ## Comm Low Med High ## Low 0.34 0.34 0.34 ## Med 0.33 0.33 0.33 ## High 0.33 0.33 0.33 ## ## , , Plea = Low, Atti = Med ## ## Fuse ## Comm Low Med High ## Low 0.34 0.34 0.34 ## Med 0.33 0.33 0.33 ## High 0.33 0.33 0.33 ## ## , , Plea = Med, Atti = Med ## ## Fuse ## Comm Low Med High ## Low 0.00 0.00 0.00 ## Med 1.00 0.98 0.84 ## High 0.00 0.02 0.16 ## ## , , Plea = High, Atti = Med ## ## Fuse ## Comm Low Med High ## Low 0.00 0.00 0.00 ## Med 1.00 0.33 0.40 ## High 0.00 0.67 0.60 ## ## , , Plea = Low, Atti = High ## ## Fuse ## Comm Low Med High ## Low 0.34 0.34 0.34 ## Med 0.33 0.33 0.33 ## High 0.33 0.33 0.33 ## ## , , Plea = Med, Atti = High ## ## Fuse ## Comm Low Med High ## Low 1.00 0.00 0.00 ## Med 0.00 0.83 0.71 ## High 0.00 0.17 0.29 ## ## , , Plea = High, Atti = High ## ## Fuse ## Comm Low Med High ## Low 0.34 0.00 0.00 ## Med 0.33 0.44 0.10 ## High 0.33 0.56 0.90 4.2.1.2 B. Learning the structure and parameters from observational data We have seen in the previous section, that we are able to manually set the directions. We can also use a constrained based algorithms to learn the links based on historical data. retention &lt;- read.csv(&quot;Data/Probabilistic Models for Customer Analytics/retention.csv&quot; ,header = T ,colClasses = &quot;factor&quot;) retention_test &lt;- read.csv(&quot;Data/Probabilistic Models for Customer Analytics/retention_test.csv&quot;, header = T, colClasses = &quot;factor&quot; ) head(retention) str(retention) Fuse Plea Atti Comm High High High High Med High High Med High Med High High Low Med High Low High Med Med Med High Med High Med ## &#39;data.frame&#39;: 5000 obs. of 4 variables: ## $ Fuse: Factor w/ 3 levels &quot;High&quot;,&quot;Low&quot;,&quot;Med&quot;: 1 3 1 2 1 1 1 1 1 1 ... ## $ Plea: Factor w/ 3 levels &quot;High&quot;,&quot;Low&quot;,&quot;Med&quot;: 1 1 3 3 3 3 1 1 3 1 ... ## $ Atti: Factor w/ 3 levels &quot;High&quot;,&quot;Low&quot;,&quot;Med&quot;: 1 1 1 1 3 1 1 1 3 1 ... ## $ Comm: Factor w/ 3 levels &quot;High&quot;,&quot;Low&quot;,&quot;Med&quot;: 1 3 1 2 3 3 1 1 3 1 ... We see that we have four vectors with three factor levels, high, low and medium. Notice that the data must be factors. Although she does mention that one can also have all numeric data, we did not explore this. bn are particularly designed for categorical variables, continuous variable require to be discretisized if all variables are continuous, a Gausian bn can be built (not discussed here). 4.2.1.2.1 Grow-shrink (gs) - Learning a structure using a constrained-based algorithm 2.1 Learning a structure using a constrained-based algorithm “grow-shrink (gs)”, with conditional independence test chi-squared Constrained-based alg. do not work with missing data bn.gs &lt;- gs(retention #The data ,alpha = 0.05 #The signifance level, for deciding if there is a relationship ,test =&quot;x2&quot;) # alternative test =&quot;mi&quot; plot(bn.gs, main = &quot;Grow shrink_X2&quot;) We see that this is the model that the model suggests, purely on the data. We see that it could not draw the direction of the Communicating and Attitude, but the rest, it was able to identify the arrows. Hence we must set the arrow ourselves. Then how do we set the arrow? A rule of thumb, is chronological order, if it cannot be applied, derive it theoretically The following is just an example with graphviz but not mandatory. #graphviz.plot (bn.gs, main = &quot;Grow shrink_X2&quot;) # notice that in the constrained-based alg some links are undirected. # this occurs because the algorithm cannot establish the direction of &quot;causality&quot;. 4.2.1.2.1.1 Identifying undirectred links Notice that when we do not manually set the links, then there is a risk of having variables where the technique is not able to identify directions. As we cannot have bidrectional links, we much search for these. If we already have knowledge about what relationships that can and cannot be there, we can blacklist and whitelist these. Blacklisting is about prohibiting a relationship while whitelisting a relationship if it should be there. Notice that you must specify the directions correctly, if a relationship just cannot be there, then you blacklist both directions. Now we can look into which links that the algorithm could not define. We can see the undirected arcs in the following. undirected.arcs(bn.gs) ## from to ## [1,] &quot;Atti&quot; &quot;Comm&quot; ## [2,] &quot;Comm&quot; &quot;Atti&quot; We need to set the direction of the undirected arcs to be able, this is done in the following. Setting directions manually Now we can set the relationship manually. bn.gs1 &lt;- set.arc(bn.gs, from = &quot;Atti&quot;, to = &quot;Comm&quot;) # to learn the parameters from observational data plot(bn.gs1, main = &quot;Grow Shrink_&quot;) #graphviz.plot(bn.gs1, main = &quot;Grow Shrink&quot;) Now we see that all variables are directed. Blacklisting and whitelisting relationships In the following we blacklist a relationship, hence we see that there can be no relationship from commitment to attitude, but not the other way around. Hence if we know such, it could be a good idea to provide that information. Therefore we are now going to imposing restrictions to the algorithm based on previous knowledge + blacklist() and whitelist(): e.g., if Comm cannot theoretically determine Atti, but Atti can determine Comm bn.gs &lt;- gs(retention, blacklist = c(&quot;Comm&quot;, &quot;Atti&quot;)) # graphviz.plot(bn.gs, main = &quot;Grow Shrink_ with restriction 1&quot;) Now we can blacklist one more relationship if arc Atti - Comm should not be there at all blacklist &lt;- data.frame(from = c(&quot;Comm&quot;, &quot;Atti&quot;), to = c(&quot;Atti&quot;, &quot;Comm&quot;)) blacklist bn.gsb &lt;- gs(retention, blacklist = blacklist) plot(bn.gsb, main = &quot;Grow Shrink_with restriction 2&quot;) # graphviz.plot(bn.gsb, main = &quot;Grow Shrink_with restrictions&quot;) from to Comm Atti Atti Comm Now we see that the link between Comm and Atti is not there. We can also allow for a relationship, that is done by white listing. if theoretically the arc Atti -&gt; Comm should be there bn.gsw &lt;- gs(retention, whitelist = c(&quot;Atti&quot;, &quot;Comm&quot;)) plot(bn.gsw, main = &quot;Grow Shrink_with restriction 3&quot;) # graphviz.plot(bn.gsw, main = &quot;Grow Shrink_with restriction 3&quot;) We see that the arrow is not there again, going from attitude to commitment, because we whitelisted that relationship. 4.2.1.2.2 Hill-Climbing (hc) - Learning the structure using a score-based algorithm Hill-Climbing (hc) greedy search, this is made for prediction. Here we are using an information criteria to predict the relationships and choose on the IC. bn.hc &lt;- hc(retention, score = &quot;bde&quot;) plot (bn.hc, main = &quot;Hill Climbing_BDe&quot;) #graphviz.plot (bn.hc, main = &quot;Hill Climbing_BDe&quot;) bn.hc &lt;- hc(retention, score = &quot;bic&quot;) plot (bn.hc, main = &quot;Hill Climbing_BIC&quot;) #graphviz.plot (bn.hc, main = &quot;Hill Climbing_BIC&quot;) We see that this suggests two different structures. 4.2.1.2.3 Fitting the model to the data (learning the probabilities) Notice that in the first section with the manual method, we manually made the conditional probabilities. We are going to learn the model parameters using the Grow-Shrink model that was created earlier. Bayesian network model I chose bn.gs1. bn.mle &lt;- bn.fit(bn.gs1, data = retention, method = &quot;mle&quot;) bn.mle #One by one bn.mle$Fuse #Parent bn.mle$Plea #Parent bn.mle$Atti bn.mle$Comm # Other useful functions # set.arc(net, from = &quot;A&quot;, to = &quot;T&quot;) # drop.arc(net, from=&quot;A&quot;, to=&quot;&quot;T) # e.g. newnet = drop.arc(net, from = &quot;T&quot;, to = &quot;A&quot;) # Test for the conditional independence between variables # ci.test(&quot;T&quot;, &quot;E&quot;, c(&quot;O&quot;, &quot;R&quot;), test = &quot;x2&quot;, data = data) ## ## Bayesian network parameters ## ## Parameters of node Fuse (multinomial distribution) ## ## Conditional probability table: ## High Low Med ## 0.7206 0.0212 0.2582 ## ## Parameters of node Plea (multinomial distribution) ## ## Conditional probability table: ## High Low Med ## 0.4346 0.0094 0.5560 ## ## Parameters of node Atti (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = High ## ## Fuse ## Atti High Low Med ## High 0.91332059 0.00000000 0.58676208 ## Low 0.00000000 0.02222222 0.00000000 ## Med 0.08667941 0.97777778 0.41323792 ## ## , , Plea = Low ## ## Fuse ## Atti High Low Med ## High 0.00000000 0.00000000 0.36363636 ## Low 1.00000000 1.00000000 0.45454545 ## Med 0.00000000 0.00000000 0.18181818 ## ## , , Plea = Med ## ## Fuse ## Atti High Low Med ## High 0.52576288 0.28333333 0.19694868 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.47423712 0.71666667 0.80305132 ## ## ## Parameters of node Comm (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = High, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.89602233 0.56402439 ## Low 0.00000000 0.00000000 ## Med 0.10397767 0.43597561 ## ## , , Plea = Low, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.50000000 ## Low 0.25000000 ## Med 0.25000000 ## ## , , Plea = Med, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.30066603 0.00000000 0.11267606 ## Low 0.00000000 1.00000000 0.00000000 ## Med 0.69933397 0.00000000 0.88732394 ## ## , , Plea = High, Atti = Low ## ## Fuse ## Comm High Low Med ## High 1.00000000 ## Low 0.00000000 ## Med 0.00000000 ## ## , , Plea = Low, Atti = Low ## ## Fuse ## Comm High Low Med ## High 0.00000000 0.00000000 0.00000000 ## Low 1.00000000 0.00000000 0.40000000 ## Med 0.00000000 1.00000000 0.60000000 ## ## , , Plea = Med, Atti = Low ## ## Fuse ## Comm High Low Med ## High ## Low ## Med ## ## , , Plea = High, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.61029412 0.00000000 0.67532468 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.38970588 1.00000000 0.32467532 ## ## , , Plea = Low, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.00000000 ## Low 0.50000000 ## Med 0.50000000 ## ## , , Plea = Med, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.17405063 0.00000000 0.01899827 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.82594937 1.00000000 0.98100173 ## ## ## ## Parameters of node Fuse (multinomial distribution) ## ## Conditional probability table: ## High Low Med ## 0.7206 0.0212 0.2582 ## ## Parameters of node Plea (multinomial distribution) ## ## Conditional probability table: ## High Low Med ## 0.4346 0.0094 0.5560 ## ## Parameters of node Atti (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = High ## ## Fuse ## Atti High Low Med ## High 0.91332059 0.00000000 0.58676208 ## Low 0.00000000 0.02222222 0.00000000 ## Med 0.08667941 0.97777778 0.41323792 ## ## , , Plea = Low ## ## Fuse ## Atti High Low Med ## High 0.00000000 0.00000000 0.36363636 ## Low 1.00000000 1.00000000 0.45454545 ## Med 0.00000000 0.00000000 0.18181818 ## ## , , Plea = Med ## ## Fuse ## Atti High Low Med ## High 0.52576288 0.28333333 0.19694868 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.47423712 0.71666667 0.80305132 ## ## ## Parameters of node Comm (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = High, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.89602233 0.56402439 ## Low 0.00000000 0.00000000 ## Med 0.10397767 0.43597561 ## ## , , Plea = Low, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.50000000 ## Low 0.25000000 ## Med 0.25000000 ## ## , , Plea = Med, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.30066603 0.00000000 0.11267606 ## Low 0.00000000 1.00000000 0.00000000 ## Med 0.69933397 0.00000000 0.88732394 ## ## , , Plea = High, Atti = Low ## ## Fuse ## Comm High Low Med ## High 1.00000000 ## Low 0.00000000 ## Med 0.00000000 ## ## , , Plea = Low, Atti = Low ## ## Fuse ## Comm High Low Med ## High 0.00000000 0.00000000 0.00000000 ## Low 1.00000000 0.00000000 0.40000000 ## Med 0.00000000 1.00000000 0.60000000 ## ## , , Plea = Med, Atti = Low ## ## Fuse ## Comm High Low Med ## High ## Low ## Med ## ## , , Plea = High, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.61029412 0.00000000 0.67532468 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.38970588 1.00000000 0.32467532 ## ## , , Plea = Low, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.00000000 ## Low 0.50000000 ## Med 0.50000000 ## ## , , Plea = Med, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.17405063 0.00000000 0.01899827 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.82594937 1.00000000 0.98100173 4.2.1.2.4 Alternatives to Grow-Shrink and HC These are others algorithms to constructing the Bayesian Network. # other constraint-based algorithms have been developed bn2 &lt;- iamb(retention, alpha = 0.05, test =&quot;mi&quot;) #graphviz.plot (bn2, main = &quot;Iamb1_mi&quot; ) bn3 &lt;- fast.iamb(retention, alpha = 0.05, test =&quot;mi&quot;) #graphviz.plot (bn3, main = &quot;FastIamb_mi&quot;) bn4 &lt;- inter.iamb(retention, alpha = 0.05, test =&quot;mi&quot; ) #graphviz.plot (bn4, main = &quot;InterIamb_mi&quot;) # in the optimal case, all will return the same graph 4.2.2 2. Model evaluation Now we are going to test how good the model is. 4.2.2.1 I. Metrics of model complexity nodes(bn.mle) arcs(bn.mle) bn.mle plot(bn.gs1) ## [1] &quot;Fuse&quot; &quot;Plea&quot; &quot;Atti&quot; &quot;Comm&quot; ## from to ## [1,] &quot;Fuse&quot; &quot;Atti&quot; ## [2,] &quot;Fuse&quot; &quot;Comm&quot; ## [3,] &quot;Plea&quot; &quot;Atti&quot; ## [4,] &quot;Plea&quot; &quot;Comm&quot; ## [5,] &quot;Atti&quot; &quot;Comm&quot; ## ## Bayesian network parameters ## ## Parameters of node Fuse (multinomial distribution) ## ## Conditional probability table: ## High Low Med ## 0.7206 0.0212 0.2582 ## ## Parameters of node Plea (multinomial distribution) ## ## Conditional probability table: ## High Low Med ## 0.4346 0.0094 0.5560 ## ## Parameters of node Atti (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = High ## ## Fuse ## Atti High Low Med ## High 0.91332059 0.00000000 0.58676208 ## Low 0.00000000 0.02222222 0.00000000 ## Med 0.08667941 0.97777778 0.41323792 ## ## , , Plea = Low ## ## Fuse ## Atti High Low Med ## High 0.00000000 0.00000000 0.36363636 ## Low 1.00000000 1.00000000 0.45454545 ## Med 0.00000000 0.00000000 0.18181818 ## ## , , Plea = Med ## ## Fuse ## Atti High Low Med ## High 0.52576288 0.28333333 0.19694868 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.47423712 0.71666667 0.80305132 ## ## ## Parameters of node Comm (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = High, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.89602233 0.56402439 ## Low 0.00000000 0.00000000 ## Med 0.10397767 0.43597561 ## ## , , Plea = Low, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.50000000 ## Low 0.25000000 ## Med 0.25000000 ## ## , , Plea = Med, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.30066603 0.00000000 0.11267606 ## Low 0.00000000 1.00000000 0.00000000 ## Med 0.69933397 0.00000000 0.88732394 ## ## , , Plea = High, Atti = Low ## ## Fuse ## Comm High Low Med ## High 1.00000000 ## Low 0.00000000 ## Med 0.00000000 ## ## , , Plea = Low, Atti = Low ## ## Fuse ## Comm High Low Med ## High 0.00000000 0.00000000 0.00000000 ## Low 1.00000000 0.00000000 0.40000000 ## Med 0.00000000 1.00000000 0.60000000 ## ## , , Plea = Med, Atti = Low ## ## Fuse ## Comm High Low Med ## High ## Low ## Med ## ## , , Plea = High, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.61029412 0.00000000 0.67532468 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.38970588 1.00000000 0.32467532 ## ## , , Plea = Low, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.00000000 ## Low 0.50000000 ## Med 0.50000000 ## ## , , Plea = Med, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.17405063 0.00000000 0.01899827 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.82594937 1.00000000 0.98100173 We see that the model is rather simple, not many variables and links. 4.2.2.2 II. Metrics of model sensitivity Test if any two nodes are d-separated, meaning that information for one variables to another flows or not. For this I refer back to the conditional independence. In the following we see that we just compare two nodes, although one could also add the scenario of imposing evidence (hence controlling for a given variable). dsep(bn.mle, x = &quot;Plea&quot;, y = &quot;Fuse&quot;) dsep(bn.mle, x = &quot;Plea&quot;, y = &quot;Comm&quot;) ## [1] TRUE ## [1] FALSE We see that Pleasure and functional usefulness is not dependent on each other, although pleasure and communication is not separated, hence dependent. 4.2.2.3 III. Evaluate the arc.strength() 4.2.2.3.1 For the growth shrink Now we want to see how strong the relationship is between the nodes. Normally we have beta values as normally, hence we can look at: p-values: with criterion = “x2” or “mi” (mutual information), the output reports the p-value for the test. The lower the p-value, the stronger the relationship. using BIC: with criterion =“bic” reports the change in the BIC score of the net caused by an arc removal.The more negative the change, means the BIC score will go worse if we delete that arc (i.e. the arc is important for the model). library(dplyr) options(cipen = 0) arc.strength(bn.gs1, retention, criterion = &quot;x2&quot;) %&gt;% as.data.frame() %&gt;% arrange(strength) from to strength Plea Atti 0 Plea Comm 0 Fuse Comm 0 Fuse Atti 0 Atti Comm 0 We see that the strength between Pleasure and Attitude is not strong, hence it makes sense that not all models find the relationship. That also means the the significance of this relationship is low. arc.strength(bn.gs1, retention, criterion = &quot;bic&quot;) %&gt;% as.data.frame() %&gt;% arrange(strength) from to strength Plea Comm -668.21187 Plea Atti -638.17237 Fuse Atti -302.40435 Fuse Comm -147.83732 Atti Comm 40.48931 The output reveals that, if we remove Plea -&gt; Comm, BIC will decrease with -668.211, which in bnlearn means the model will get worse. The output reveals that, if we remove Atti -&gt; Comm, BIC will increase with 40.48, which in bnlearn package means the model may improve based on this index. We see that the relationship between Pleasure and commitment for instance is very high. 4.2.2.3.2 For the hill-climbing structure Notice that we are just merely repeating the same here, just with the other method. arc.strength(bn.hc, retention, criterion = &quot;bic&quot;) %&gt;% as.data.frame() %&gt;% arrange(strength) from to strength Plea Comm -1221.6057 Plea Atti -638.1724 Fuse Atti -302.4043 Fuse Comm -277.3471 As expected, all strenghts are negative as the model is fitted against BIC, hence if it could improve the model by removing the given link, then it would do so. 4.2.2.3.3 Different information criteria evaluations Metrics of evaluation and selection among several dags: BIC, BDe, AIC scores are used to compare alternative structures and choose the best. In bnlearn, AIC, BIC, BDE closer to zero means better model; often the three indexesdo not agree. See the following examples: bnlearn::score(bn.gs1, retention, type = &quot;aic&quot;) bnlearn::score(bn.hc, retention, type = &quot;aic&quot;) bnlearn::score(bn.gs1, retention, type = &quot;bic&quot;) bnlearn::score(bn.hc, retention, type = &quot;bic&quot;) bnlearn::score(bn.gs1, retention, type = &quot;bde&quot;) bnlearn::score(bn.hc, retention, type = &quot;bde&quot;) ## [1] -11842.95 ## [1] -11919.77 ## [1] -12090.61 ## [1] -12050.12 ## [1] -11904.39 ## [1] -11983.02 We see that the models that we have learned through HC ans GS, they appear to all be the same. Probably because they were trained on the same IC, during the lecture we see that there is a difference. 4.2.2.4 IV. Metrics of predictive accuracy (error rate, confusion matrix, AUC) Here we are going to predict one variable and see what the error rate will be. library(gRain) library(gRbase) library (caTools) 4.2.2.4.1 Cross validation This function requires as one of its parameters only structure (not the full bayesian model). We use classification error (“pred”) for the node Comm (our target) as a loss function. netcv &lt;- bn.cv(retention, bn.gs1 ,loss =&quot;pred&quot; #Gives error rate ,k = 5 ,loss.args = list(target = &quot;Comm&quot;) ,debug = TRUE) netcv ## * splitting 5000 data in 5 subsets. ## ---------------------------------------------------------------- ## * fitting the parameters of the network from the training sample. ## * applying the loss function to the data from the test sample. ## &gt; classification error for node Comm is 0.19 . ## @ total loss is 0.19 . ## ---------------------------------------------------------------- ## * fitting the parameters of the network from the training sample. ## * applying the loss function to the data from the test sample. ## &gt; classification error for node Comm is 0.179 . ## @ total loss is 0.179 . ## ---------------------------------------------------------------- ## * fitting the parameters of the network from the training sample. ## * applying the loss function to the data from the test sample. ## &gt; classification error for node Comm is 0.1803607 . ## @ total loss is 0.1803607 . ## ---------------------------------------------------------------- ## * fitting the parameters of the network from the training sample. ## * applying the loss function to the data from the test sample. ## &gt; classification error for node Comm is 0.185 . ## @ total loss is 0.185 . ## ---------------------------------------------------------------- ## * fitting the parameters of the network from the training sample. ## * applying the loss function to the data from the test sample. ## &gt; classification error for node Comm is 0.204 . ## @ total loss is 0.204 . ## ---------------------------------------------------------------- ## * summary of the observed values for the loss function: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.1790 0.1804 0.1850 0.1877 0.1900 0.2040 ## ## k-fold cross-validation for Bayesian networks ## ## target network structure: ## [Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti] ## number of folds: 5 ## loss function: Classification Error ## training node: Comm ## expected loss: 0.1876721 The prediction accuracy of Comm based on 5-fold cross validation is quite high (1-0.18 = 0.82) one can also look at each individual variables. Using a testing sample to evaluate the model performance we need to transform the full bayesian model into a gRain object net1 &lt;- as.grain(bn.mle) net1 ## Independence network: Compiled: TRUE Propagated: FALSE ## Nodes: chr [1:4] &quot;Fuse&quot; &quot;Plea&quot; &quot;Atti&quot; &quot;Comm&quot; assuming Comm is the target node, we predict the probability of Comm being High, Medium and Large using net1 in the test sample. predComm &lt;- predict(net1 ,response = c(&quot;Comm&quot;) ,newdata = retention_test ,predictors = names(retention_test)[-4] #Removing target variable ,type = &quot;distribution&quot; #This returns likelihoods ) predComm = predComm$pred$Comm #View(predComm) #Shows the likelihood # write.csv(predComm, &quot;PredComm.csv&quot;, row.names = FALSE) Or add the predictions to the data file creating a new variable. Instead of probabilities of 0 or 1, one can save the actual CLASS (0/1). predComm_class &lt;- predict(object = net1 ,response = c(&quot;Comm&quot;) ,newdata = retention_test ,predictors = names (retention_test)[-4] #Removing target variable ,type = &quot;class&quot; #This just returns the class ) predCommclass &lt;- predComm_class$pred$Comm #View(predCommclass) # write.csv(predictionsTclass, &quot;PredictedTclass&quot;) 4.2.2.4.2 Alternative approach to fit and predict (Without gRain) A quick method WITHOUT using package gRain. #Speficy network model &lt;- model2network(&quot;[Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti]&quot;) #Fit the data bn.mle1 &lt;- bn.fit(model ,retention) #Make predictions predComm1 &lt;- predict(bn.mle1 ,node = &quot;Comm&quot; ,data = retention_test) #See the predictions compared to previous predictions. table(predCommclass, predComm1) ## predComm1 ## predCommclass High Low Med ## High 1098 0 0 ## Low 0 28 0 ## Med 0 0 1374 We see that this methods almost makes the same results. We cam also make a confusion matrix. table(predComm_class$pred$Comm, retention_test$Comm) ## ## High Low Med ## High 903 1 194 ## Low 2 25 1 ## Med 250 1 1123 Plotting ROC and AUC Notice that ROC and AUC is made for a two class scenario, hence it returns nine lines, one for each line. library(caTools) colAUC(predComm, retention_test[ ,4], plotROC = TRUE) #requires the predicted probabilities, not the predicted class ## High Low Med ## High vs. Low 0.9910855 0.9998397 0.9195447 ## High vs. Med 0.8753043 0.5001077 0.8764289 ## Low vs. Med 0.9556286 0.9998876 0.9876075 we get an AUC for every column of the prediction matrix our DV has 3 categories: Low, Med and High we observe that the model has problems when d istiguisging between high and medium but performs pretty well when indentifying the Low category (customers who are not commited) 4.2.3 3. Making inference and queries based on the model How do we use the model in practice to make inference with incomplete data? Below we consider several hypothetical situations. A customer is considered retained if he has a positive attitude and high commitment to the Virtual Community (hereafter, VC). How do functional usefulness and pleasure influence on attitude and commitment? We know the strength of the relationship based on the arc strength. The following analysis helps to understand the relationship, which we will see that it is not be necessarily linear Using BN, one can evaluate the expected changes in attitude, and respectively, commitment due to changes in functional usefulness and pleasure. We will set evidence in the network for Fuse and Plea and we´ll look at the cpt (conditional probability table) for Atti and Comm before and after setting the evidence. Setting (hard) evidence means setting one of the states of the variable at probability 1 (that is, as 100%). Transform the bn into a junction tree We are going to make two types of diagnositcs: Forward inference: following the directions, hence setting evidence for the parents and then see how that affects the children Backward inference: going against the directions, hence setting evidence for children and then seeing how that affects the parents. The following is also forward integration library (gRain) junction &lt;- compile(as.grain(bn.mle)) # &quot;querygrain&quot; function extracts the marginal distribution of the nodes First we are going to look at the prior probabilities, that is done in the following. # ctp for atti and comm are: querygrain(junction, nodes = &quot;Atti&quot;) querygrain(junction, nodes = &quot;Comm&quot;) ## $Atti ## Atti ## High Low Med ## 0.595014752 0.008280883 0.396704365 ## ## $Comm ## Comm ## High Low Med ## 0.44373332 0.01099592 0.54527076 Imagine a new person joins the VC reporting a Low, Medium, or High Functional Usefulness perception. This information can be fed to the network as evidence in order to predict the conditional probability of his/her attitude and commitment to VC. # new ctp(if Fuse = Low) jLow &lt;- setEvidence (junction, nodes = &quot;Fuse&quot;, states = &quot;Low&quot;) A1 &lt;- querygrain(jLow, nodes = &quot;Atti&quot;) A1 ## $Atti ## Atti ## High Low Med ## 0.15753333 0.01905778 0.82340889 We see that we enter a piece of information and then see how the attitude changes. So we see that it went from 59% to 15%, which is a strong decrease. We can do the same with commitment C1 &lt;- querygrain(jLow, nodes = &quot;Comm&quot;) C1 ## $Comm ## Comm ## High Low Med ## 0.009657778 0.157533333 0.832808889 We see that the High segment in the commitment as weel. # new ctp (if Fuse = Med) jMed &lt;- setEvidence (junction, nodes = &quot;Fuse&quot;, states = &quot;Med&quot;) A2 &lt;- querygrain(jMed, nodes = &quot;Atti&quot;) A2 C2 &lt;- querygrain(jMed, nodes = &quot;Comm&quot;) C2 # new ctp (if Fuse = High) jHigh &lt;- setEvidence (junction, nodes = &quot;Fuse&quot;, states = &quot;High&quot;) A3 &lt;- querygrain(jHigh, nodes = &quot;Atti&quot;) A3 C3 &lt;- querygrain(jHigh, nodes = &quot;Comm&quot;) C3 ## $Atti ## Atti ## High Low Med ## 0.367928447 0.004272727 0.627798826 ## ## $Comm ## Comm ## High Low Med ## 0.287643947 0.003418182 0.708937871 ## ## $Atti ## Atti ## High Low Med ## 0.6892533 0.0094000 0.3013467 ## ## $Comm ## Comm ## High Low Med ## 0.5124326 0.0094000 0.4781674 4.2.3.1 Diagnostics (Forward inference) Here we are going to set evidence for a given customer and see how probabilities for other variables are affected. We see that we get likelihood of the different states where attitude is set to low, medium and high. We see that when attitude = low, then changes in fuse will have no effect. # Summary (only for Atti) AttiHigh &lt;-c(A1$Atti[[1]], A2$Atti[[1]], A3$Atti[[1]]) AttiLow &lt;- c(A1$Atti[[2]], A2$Atti[[2]], A3$Atti[[2]]) AttiMed &lt;- c(A1$Atti[[3]], A2$Atti[[3]], A3$Atti[[3]]) df1 &lt;- data.frame(Fuse = c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;), AttiLow, AttiMed, AttiHigh) df1 matplot(rownames(df1), df1 ,type = &#39;l&#39; ,xlab = &#39;Fuse&#39; ,ylab = &#39;Probability&#39; ,ylim = c(0,1) ,col = c(3,4,2)) legend(&#39;topright&#39; ,inset = .01 ,legend = colnames(df1[,2:4]) #&quot;AttiLow&quot; &quot;AttiMed&quot; &quot;AttiHigh&quot; ,pch = 1,horiz = T ,col = c(&quot;blue&quot;,&quot;red&quot;,&quot;green&quot;)) Fuse AttiLow AttiMed AttiHigh Low 0.0190578 0.8234089 0.1575333 Med 0.0042727 0.6277988 0.3679284 High 0.0094000 0.3013467 0.6892533 Discussion as Fuse changes from low to medium to high, the high state of attitude shows an increasing trend, the medium state of attitude shows a decreasing trend, the low state of attitude shows a constant trend. it depicts that when functional usefulness is low, the probability of attitude medium is quite high (0.80) and thus functional usefulness does not radically affect the customer´s attitude. And when the attitude is just low, then the functional usefullness does not change anything. 4.2.3.1.1 let us repeat the same analysis by setting evidence in Pleasure We can do the same graphs, just where we see the relationship between pleasure and attitude. # new cpt jLow &lt;- setEvidence (junction, nodes = &quot;Plea&quot;, states = &quot;Low&quot;) A1 = querygrain(jLow, nodes = &quot;Atti&quot;) C1 = querygrain(jLow, nodes = &quot;Comm&quot;) # New cpt jMed &lt;- setEvidence (junction, nodes = &quot;Plea&quot;, states = &quot;Med&quot;) A2 = querygrain(jMed, nodes = &quot;Atti&quot;) C2 = querygrain(jMed, nodes = &quot;Comm&quot;) # New cpt jHigh &lt;- setEvidence (junction, nodes = &quot;Plea&quot;, states = &quot;High&quot;) A3 = querygrain(jHigh, nodes = &quot;Atti&quot;) C3 = querygrain(jHigh, nodes = &quot;Comm&quot;) # summary AttiHigh &lt;- c(A1$Atti[[1]], A2$Atti[[1]], A3$Atti[[1]]) AttiLow &lt;- c(A1$Atti[[2]], A2$Atti[[2]], A3$Atti[[2]]) AttiMed &lt;- c(A1$Atti[[3]], A2$Atti[[3]], A3$Atti[[3]]) df2 &lt;- data.frame(Plea = c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;), AttiLow, AttiMed, AttiHigh) options(scipen = 999) df2 matplot(rownames(df2), df2, type = &#39;l&#39;, xlab = &#39;Plea&#39; ,ylab = &#39;Probability&#39;, ylim = c(0,1) ,col = c(3,4,2)) legend(&#39;topright&#39;, inset=.01, legend=colnames(df2[,2:4]), pch=1, horiz=T, col = c(&quot;blue&quot;,&quot;red&quot;,&quot;green&quot;)) Plea AttiLow AttiMed AttiHigh Low 0.8591636 0.0469455 0.0938909 Med 0.0000000 0.5642765 0.4357235 High 0.0004711 0.1898881 0.8096408 We see that the probability of a high attitude increases as pleasure increases. Although we see that the low attitude probability decrease as pleasure increases, and at some point, the low attitude will Discussion as Plea changes from low to medium to high, the high state of attitude shows an increasing trend the medium state of attitude shows a mixed trend and the low state of attitude shows a decreasing trend When pleasure is low, the probability of attitude being low is high (0.8592). This means pleasure has a stronger relationship with attitude; hence, to increase the attitude of customers, it is more important to enhance their perceived pleasure than to enhance functional usefulness. Similar influences of changes of FUSE, PLEA and ATTI on commitment can be similarly explored. To conclude, the above inference is called forward inference because we set evidence in the parent and look at cpt changes in the child. Next, we look at backward inference - a great type of inference offered by BN models. 4.2.3.2 Diagnostic (Backward inference) Here we are going to go against the arrows. Let’s assume that the evidence given is that the customer’s attitude towards VC is high. This information is fed to the network by setting the probability of attitude being high (that is, as 1.00) and observing the changes in the parent variables (Fuse and Plea) # Prior cpt querygrain(junction, nodes = &quot;Fuse&quot;) querygrain(junction, nodes = &quot;Plea&quot;) # New cpt jHigh &lt;- setEvidence (junction, nodes = &quot;Atti&quot;, states = &quot;High&quot;) querygrain(jHigh, nodes = &quot;Fuse&quot;) querygrain(jHigh, nodes = &quot;Plea&quot;) ## $Fuse ## Fuse ## High Low Med ## 0.7206 0.0212 0.2582 ## ## $Plea ## Plea ## High Low Med ## 0.4346 0.0094 0.5560 ## ## $Fuse ## Fuse ## High Low Med ## 0.834728750 0.005612813 0.159658437 ## ## $Plea ## Plea ## High Low Med ## 0.591363295 0.001483282 0.407153423 Discussion The probability of the high state of Plea and Fuse is increasing, while the probability of the low and medium states of Plea and Fuse is decreasing. This implies that increased attitude towards interaction in a VC is because of person’s increased perception of pleasure and functional usefulness in the VC. An example Assume that the online vendor observes decreasing commitment towards participation among its customers. He can set evidence to the network that the probability of commitment is low and see the effect of the parent variables (attitude, functional usefulness and pleasure). #Prior cpt querygrain(junction, nodes = &quot;Atti&quot;) querygrain(junction, nodes = &quot;Fuse&quot;) querygrain(junction, nodes = &quot;Plea&quot;) ## $Atti ## Atti ## High Low Med ## 0.595014752 0.008280883 0.396704365 ## ## $Fuse ## Fuse ## High Low Med ## 0.7206 0.0212 0.2582 ## ## $Plea ## Plea ## High Low Med ## 0.4346 0.0094 0.5560 # New cpt CLow &lt;- setEvidence (junction, nodes = &quot;Comm&quot;, states = &quot;Low&quot;) #We set commitment to low querygrain(CLow, nodes = &quot;Atti&quot;) querygrain(CLow, nodes = &quot;Fuse&quot;) querygrain(CLow, nodes = &quot;Plea&quot;) ## $Atti ## Atti ## High Low Med ## 0.32378827 0.65614578 0.02006595 ## ## $Fuse ## Fuse ## High Low Med ## 0.61601387 0.30372232 0.08026381 ## ## $Plea ## Plea ## High Low Med ## 0.0000000 0.6962777 0.3037223 A vendor, therefore, needs to take corrective action to enhance customers’ pleasure aspect in the VC to improve customers’ commitment toward the VC. we found that it is mainly due to lack of enhancement of the pleasure aspect of the website; the vendors needed to take corrective action to provide the customers with more fun and enjoyment. We see that when the communication is set to low, then the probability of pleasure being high is 0. Modeling contradictory behavior Assume some customers interact in the VC to seek information from the VC, but do not participate in VC activities. Such customers can be considered persons with positive attitudes but low commitment. Can BN predict the reasons behind such contradictory behavior? # Prior ctp querygrain(junction, nodes = &quot;Fuse&quot;) querygrain(junction, nodes = &quot;Plea&quot;) # New ctp AHigh &lt;- setEvidence (junction, nodes = &quot;Att&quot;, states = &quot;High&quot;) AHighCLow &lt;- setEvidence (AHigh, nodes = &quot;Comm&quot;, states = &quot;Low&quot;) querygrain(AHighCLow, nodes = &quot;Fuse&quot;) querygrain(AHighCLow, nodes = &quot;Plea&quot;) ## $Fuse ## Fuse ## High Low Med ## 0.7206 0.0212 0.2582 ## ## $Plea ## Plea ## High Low Med ## 0.4346 0.0094 0.5560 ## ## $Fuse ## Fuse ## High Low Med ## 0.61601387 0.30372232 0.08026381 ## ## $Plea ## Plea ## High Low Med ## 0.0000000 0.6962777 0.3037223 Discussion The probability of attitude high and commitment low was set to 1.00. The results imply that FUSE is 0.3 likely to be low but a significant proportion is still likely to be high (0.62); instead, PLEA is most likely to be low (0.7) and unlikelly to be high (0%). This reveals that customers interact primarily because of fun, and they do not perceive the VC to be sufficiently useful to them to commit to. 4.3 Application 2: Identifying populations with Positive Expected Lift in Profit (ELP) and targeting The idea is to use BN to identify segments of customers that will most likely purchase when sending the ad (persuadable segments) and avoid sending the ad to the rest: 1) to the ones who will not buy the advertised product ever, 2) to the ones who will be offended by receiving an unwanted advertisement or call, 3) or to the ones who will always buy) If historical data is available, we start learning the relationships between the variables train and select the best model as a prelimimnary step. Below we use the model structure from the text (given),and we only have to learn the parameters (probabilities). Next,we focus exclusively on how model is used as a decision support for marketing managers. targeted.adv &lt;- read.csv(&quot;Data/Probabilistic Models for Customer Analytics/targeted.adv.csv&quot; ,header = T, colClasses = &quot;factor&quot;) head(targeted.adv) str(targeted.adv) IDnum Buy Income Sex Mailed 1 yes high male no 2 no low male no 3 no low female yes 4 no low female yes 5 no low female no 6 no low female no ## &#39;data.frame&#39;: 1000 obs. of 5 variables: ## $ IDnum : Factor w/ 1000 levels &quot;1&quot;,&quot;10&quot;,&quot;100&quot;,..: 1 113 224 335 446 557 668 779 890 2 ... ## $ Buy : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 1 1 1 1 1 2 2 2 1 ... ## $ Income: Factor w/ 3 levels &quot;high&quot;,&quot;low&quot;,&quot;medium&quot;: 1 2 2 2 2 2 1 3 1 3 ... ## $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 1 1 1 1 1 1 2 1 ... ## $ Mailed: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 2 2 1 1 2 2 2 1 ... We see that the table contains 5 columns, ID, Buy, Income, Sex and Mailed. All the variables are factor variables # Build the structure dagTA &lt;- model2network(&quot;[Income][Sex][Mailed][Buy|Income:Sex:Mailed]&quot;) plot(dagTA) We see that the all variables leads towards buy. # Learn the cpt (conditional probability table) bnTA.mle &lt;- bn.fit(dagTA, data = targeted.adv[, c(2:5)], method = &quot;mle&quot;) bnTA.mle ## ## Bayesian network parameters ## ## Parameters of node Buy (multinomial distribution) ## ## Conditional probability table: ## ## , , Mailed = no, Sex = female ## ## Income ## Buy high low medium ## no 0.4090909 0.5326087 0.5609756 ## yes 0.5909091 0.4673913 0.4390244 ## ## , , Mailed = yes, Sex = female ## ## Income ## Buy high low medium ## no 0.4197531 0.4204545 0.2812500 ## yes 0.5802469 0.5795455 0.7187500 ## ## , , Mailed = no, Sex = male ## ## Income ## Buy high low medium ## no 0.4285714 0.5306122 0.8043478 ## yes 0.5714286 0.4693878 0.1956522 ## ## , , Mailed = yes, Sex = male ## ## Income ## Buy high low medium ## no 0.4555556 0.3827160 0.5853659 ## yes 0.5444444 0.6172840 0.4146341 ## ## ## Parameters of node Income (multinomial distribution) ## ## Conditional probability table: ## high low medium ## 0.321 0.359 0.320 ## ## Parameters of node Mailed (multinomial distribution) ## ## Conditional probability table: ## no yes ## 0.514 0.486 ## ## Parameters of node Sex (multinomial distribution) ## ## Conditional probability table: ## female male ## 0.473 0.527 We know that the whole network is based upon the conditional probabilities. Now we want to calculate the ELP (Expected Lift in Profit) The function is basically \\[ELP = P(Buy = Yes| Mailed = yes) * r_s - P(Buy = Yes| Mailed = no) r_u - c\\] That applies for any given population Y. We see that we have the following notations: \\(c\\) = cost of mailing the ad to a give person \\(r_u\\) = the income obtained from a sale to en unsolicited customer \\(r_c\\) = the income obtained from a sale to en solicited customer \\(r_u\\) and \\(r_s\\) = are different because we may offer some discount in the ad We can take the following assumptions. Now we are goint to do three different scenarios: Compute the ELP for the population consisting of individuals with medium income who are male. Should we mail an ad to this population? Compute the ELP for the population consisting of individuals with medium income who are female. Should we mail the ad to this population? Compute the ELP for the population consisting of individuals with low income. Should we mail an ad this population? Scenario 1 We are going to evaluate males with high income. First we see the probability of buying when mailed and when not. c = 0.5 #Cost r_s = 8 #Income with discount r_u = 10 #Income obtained from a sale to en unsolicited customer # set evidence and get the cpt library (gRain) junctionTA &lt;- compile(as.grain(bnTA.mle)) #We have mailed Med_male_yes &lt;- setEvidence(junctionTA, nodes = c(&quot;Income&quot;, &quot;Sex&quot;, &quot;Mailed&quot;) , states = c(&quot;medium&quot;, &quot;male&quot;, &quot;yes&quot;)) querygrain(Med_male_yes, nodes = &quot;Buy&quot;) ## $Buy ## Buy ## no yes ## 0.5853659 0.4146341 Probability of buying when mailed = 41.46%. #Not mailed Med_male_no &lt;- setEvidence(junctionTA, nodes = c(&quot;Income&quot;, &quot;Sex&quot;, &quot;Mailed&quot;) , states = c(&quot;medium&quot;, &quot;male&quot;, &quot;no&quot;)) querygrain(Med_male_no, nodes = &quot;Buy&quot;) ## $Buy ## Buy ## no yes ## 0.8043478 0.1956522 Probability of buying when not mailed = 19.56% Calculate expected lift. options(digits = 2) ELP = querygrain(Med_male_yes, nodes = &quot;Buy&quot;)$Buy[[2]] * r_s - querygrain(Med_male_no, nodes = &quot;Buy&quot;)$Buy[[2]] * r_u - c ELP ## [1] 0.86 We see that the expected lift (ELP) is positive hence, we should mail the high income males. Scenario 2 In this examplle we basically do the same, just with females instead. #Assumptions c = 0.6 #Cost r_s = 7 #Income with discount r_u = 9 #Income obtained from a sale to en unsolicited customer Med_fem_yes &lt;- setEvidence(junctionTA, nodes = c(&quot;Income&quot;, &quot;Sex&quot;, &quot;Mailed&quot;) ,states = c(&quot;medium&quot;, &quot;female&quot;, &quot;yes&quot;)) querygrain(Med_fem_yes, nodes = &quot;Buy&quot;) ## $Buy ## Buy ## no yes ## 0.28 0.72 We see the probability of buying = 72% when they are mailed. Med_fem_no &lt;- setEvidence(junctionTA, nodes = c(&quot;Income&quot;, &quot;Sex&quot;, &quot;Mailed&quot;) ,states = c(&quot;medium&quot;, &quot;female&quot;, &quot;no&quot;)) querygrain(Med_fem_no, nodes = &quot;Buy&quot;) ## $Buy ## Buy ## no yes ## 0.56 0.44 We see that the probability of buying = 44% when mailed. ELP = querygrain(Med_fem_yes, nodes = &quot;Buy&quot;)$Buy[[2]] * r_s - querygrain(Med_fem_no, nodes = &quot;Buy&quot;)$Buy[[2]] * r_u - c ELP ## [1] 0.48 We see that the ELP = 0.48, hence positive. Meaning that we should mail the medium income females as well, because we see that we earn more on these. Scenario 3 We go with the same assumptions this time. We are going to look at low income customers. Low_yes &lt;- setEvidence (junctionTA, nodes = c(&quot;Income&quot;, &quot;Mailed&quot;) ,states = c(&quot;low&quot;, &quot;yes&quot;)) querygrain( Low_yes, nodes = &quot;Buy&quot;) ## $Buy ## Buy ## no yes ## 0.4 0.6 We see that the probability of buying = 60% Low_no &lt;- setEvidence (junctionTA, nodes = c(&quot;Income&quot;, &quot;Mailed&quot;) ,states = c(&quot;low&quot;, &quot;no&quot;)) querygrain(Low_no, nodes = &quot;Buy&quot;) ## $Buy ## Buy ## no yes ## 0.53 0.47 Now we see that there is 47% probability of buying when they are mailed. ELP = querygrain(Low_yes, nodes = &quot;Buy&quot;)$Buy[[2]] * r_s - querygrain(Low_no, nodes = &quot;Buy&quot;)$Buy[[2]] * r_u - c ELP ## [1] -0.62 We see that it is negative, hence we should not email the low income people. Discussion: Using BN in this application allows to identify persuadable segments of individuals who would buy only if they are sent an ad. It avoids sending ads to those who will never buy those who always buy (thus avoid wasting the ad), and those who are turned off by the advertisement when they receive it. The network can be extended with more nodes according to the characteritics of the pop. in the dataset 4.4 Application 3: Product Recommendation using Probabilistic Models (BN) 4.4.1 Method 1: Learn the probabilistic relationships using a Bayesian network learning Now we are going to use the Hill+Climbing algorithm to estimate the model. We are going to use the Bayesian Network for Collaborative Filtering. library(bnlearn) library(Rgraphviz) data = read.csv(&quot;Data/Probabilistic Models for Customer Analytics/CollFilforR.csv&quot; ,header = T,colClasses = &quot;factor&quot;,sep = &quot;;&quot;) head(data) V1 V2 V3 V4 1 4 5 4 5 1 1 2 4 1 2 1 2 5 4 5 1 5 5 5 1 5 5 NA We see that we have information on four different items. Structural expectation-maximization function learns with mising data a Hill-Climbing structure and further predict the missing values, which will be the recommendations mystructural &lt;- structural.em(data, maximize = &quot;hc&quot;, return.all = TRUE) graphviz.plot(mystructural$dag) We see the structure. mystructural$dag # the structure ## ## Bayesian network learned from Missing Data ## ## model: ## [V1][V2][V4|V1][V3|V4] ## nodes: 4 ## arcs: 2 ## undirected arcs: 0 ## directed arcs: 2 ## average markov blanket size: 1.00 ## average neighbourhood size: 1.00 ## average branching factor: 0.50 ## ## learning algorithm: Structural EM ## score-based method: Hill-Climbing ## parameter learning method: Maximum Likelihood ## imputation method: ## Posterior Expectation (Likelihood Weighting) ## penalization coefficient: 2.3 ## tests used in the learning procedure: 34 ## optimized: TRUE In the following see the conditional probabilities. mystructural$fitted # the parameters ## ## Bayesian network parameters ## ## Parameters of node V1 (multinomial distribution) ## ## Conditional probability table: ## 1 2 3 4 5 ## 0.67 0.06 0.03 0.07 0.17 ## ## Parameters of node V2 (multinomial distribution) ## ## Conditional probability table: ## 1 2 3 4 5 ## 0.58 0.17 0.03 0.03 0.19 ## ## Parameters of node V3 (multinomial distribution) ## ## Conditional probability table: ## ## V4 ## V3 1 2 4 5 ## 1 0.600 0.556 0.000 0.045 ## 2 0.400 0.444 0.000 0.061 ## 3 0.000 0.000 0.000 0.015 ## 4 0.000 0.000 0.000 0.061 ## 5 0.000 0.000 1.000 0.818 ## ## Parameters of node V4 (multinomial distribution) ## ## Conditional probability table: ## ## V1 ## V4 1 2 3 4 5 ## 1 0.000 0.000 1.000 0.571 0.176 ## 2 0.075 0.500 0.000 0.000 0.588 ## 4 0.090 0.000 0.000 0.000 0.000 ## 5 0.836 0.500 0.000 0.429 0.235 Now we see that the method imputed the NA that we had in row 6. head(data) head(mystructural$imputed) # the predictions based on the probabilistic structure V1 V2 V3 V4 1 4 5 4 5 1 1 2 4 1 2 1 2 5 4 5 1 5 5 5 1 5 5 NA V1 V2 V3 V4 1 4 5 4 5 1 1 2 4 1 2 1 2 5 4 5 1 5 5 5 1 5 5 5 Alternatively, BN model is trained with any other algorithm using non-missing registers, and used afterwards for making inference; impute() function allow us to make inference in a new data set of customers for which we want to make recommendations. data_wNA = na.omit(data) dag = tree.bayes(data_wNA, &quot;V4&quot;) # a tree bayes graphviz.plot(dag) myBN = bn.fit(dag, data_wNA) Assuming the model was trained and tested through a cross-validation procedure (as in App 1), one can further use it for making recommendations with impute() function in a new dataset dataimputed = bnlearn::impute(myBN, data, method = &quot;parents&quot;, debug = TRUE) table(is.na(dataimputed)) ## * checking node V4 . ## &gt; found 16 missing value(s). ## * checking node V1 . ## &gt; found 7 missing value(s). ## * checking node V3 . ## &gt; found 13 missing value(s). ## * checking node V2 . ## &gt; found 10 missing value(s). ## ## FALSE ## 400 compare the recommendations made by the two methods. We make confusion matrices. table(mystructural$imputed$V1, dataimputed$V1) table(mystructural$imputed$V2, dataimputed$V2) table(mystructural$imputed$V3, dataimputed$V3) table(mystructural$imputed$V4, dataimputed$V4) ## ## 1 2 3 4 5 ## 1 67 0 0 0 0 ## 2 0 6 0 0 0 ## 3 0 0 3 0 0 ## 4 0 0 0 7 0 ## 5 0 0 0 0 17 ## ## 1 2 3 4 5 ## 1 51 2 3 1 1 ## 2 0 17 0 0 0 ## 3 0 0 3 0 0 ## 4 0 0 0 3 0 ## 5 0 0 0 0 19 ## ## 1 2 3 4 5 ## 1 19 0 0 0 0 ## 2 0 16 0 0 0 ## 3 0 0 1 0 0 ## 4 0 0 0 4 0 ## 5 0 0 0 1 59 ## ## 1 2 4 5 ## 1 10 0 0 0 ## 2 0 15 0 3 ## 4 0 0 6 0 ## 5 0 0 0 66 as seen, the recommendations made by the two methods are almost the same 4.4.2 Method 2: Finding latent segments A latent class (segment) that can accurately predict the ratings for the user on all items. The advantages of this method is that is considers both the similarities between users, items and their interaction. Hence we want to use Bayesian Networks to identify segments / or clusters. These are latent, hence we cannot directly measure them. Therefore we are going to test different number of latent segments and see which has the lowest best performance according to the log likelihood. Notice that the data consists of four different variables, these could for instance be movies. library(poLCA) # Define the variables used for (the model ~1 means without covariates) f &lt;- cbind(V1, V2, V3, V4)~1 We are going to run several models with the for() function. We are going to evaluate BIC. One also have used AIC, \\(G^2\\), \\(X^2\\). min_bic &lt;- 100000 for(i in 2:10){ #We test from 2 to 10 segments. lc &lt;- poLCA(f, data, nclass=i #We iterate through no. of classes. , maxiter=3000, tol=1e-5, na.rm=FALSE, nrep=1, verbose=TRUE, calc.se=TRUE) if(lc$bic &lt; min_bic){ min_bic &lt;- lc$bic LCA_best_model&lt;-lc } } LCA_best_model ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.95 0.052 0.000 0.00 0.00 ## class 2: 0.24 0.081 0.075 0.18 0.43 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.62 0.00 0.000 0.051 0.33 ## class 2: 0.37 0.54 0.095 0.000 0.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.00 0.00 0.000 0.079 0.921 ## class 2: 0.44 0.44 0.028 0.000 0.085 ## ## $V4 ## 1 2 4 5 ## class 1: 0.00 0.00 0.12 0.88 ## class 2: 0.31 0.46 0.00 0.23 ## ## Estimated class population shares ## 0.6 0.4 ## ## Predicted class memberships (by modal posterior prob.) ## 0.61 0.39 ## ## ========================================================= ## Fit for 2 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 31 ## residual degrees of freedom: 69 ## maximum log-likelihood: -312 ## ## AIC(2): 686 ## BIC(2): 767 ## G^2(2): 160 (Likelihood ratio/deviance statistic) ## X^2(2): 381 (Chi-square goodness of fit) ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.95 0.054 0.00 0.00 0.00 ## class 2: 0.35 0.115 0.11 0.00 0.42 ## class 3: 0.00 0.000 0.00 0.56 0.44 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.62 0.00 0.00 0.051 0.33 ## class 2: 0.37 0.63 0.00 0.000 0.00 ## class 3: 0.36 0.36 0.27 0.000 0.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.00 0.00 0.00 0.078 0.92 ## class 2: 0.68 0.32 0.00 0.000 0.00 ## class 3: 0.00 0.68 0.08 0.000 0.24 ## ## $V4 ## 1 2 4 5 ## class 1: 0.00 0.00 0.12 0.88 ## class 2: 0.25 0.62 0.00 0.13 ## class 3: 0.50 0.00 0.00 0.50 ## ## Estimated class population shares ## 0.6 0.27 0.13 ## ## Predicted class memberships (by modal posterior prob.) ## 0.61 0.25 0.14 ## ## ========================================================= ## Fit for 3 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 47 ## residual degrees of freedom: 53 ## maximum log-likelihood: -298 ## ## AIC(3): 691 ## BIC(3): 813 ## G^2(3): 136 (Likelihood ratio/deviance statistic) ## X^2(3): 250 (Chi-square goodness of fit) ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.21 0.106 0.098 0.23 0.36 ## class 2: 0.37 0.000 0.000 0.00 0.63 ## class 3: 1.00 0.000 0.000 0.00 0.00 ## class 4: 0.95 0.054 0.000 0.00 0.00 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.18 0.7 0.12 0.00 0.00 ## class 2: 1.00 0.0 0.00 0.00 0.00 ## class 3: 0.00 0.0 0.00 0.54 0.46 ## class 4: 0.69 0.0 0.00 0.00 0.31 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.31 0.55 0.034 0.00 0.10 ## class 2: 1.00 0.00 0.000 0.00 0.00 ## class 3: 0.00 0.00 0.000 0.00 1.00 ## class 4: 0.00 0.00 0.000 0.09 0.91 ## ## $V4 ## 1 2 4 5 ## class 1: 0.43 0.26 0 0.31 ## class 2: 0.00 1.00 0 0.00 ## class 3: 0.00 0.00 1 0.00 ## class 4: 0.00 0.00 0 1.00 ## ## Estimated class population shares ## 0.31 0.093 0.065 0.53 ## ## Predicted class memberships (by modal posterior prob.) ## 0.29 0.1 0.06 0.55 ## ## ========================================================= ## Fit for 4 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 63 ## residual degrees of freedom: 37 ## maximum log-likelihood: -291 ## ## AIC(4): 707 ## BIC(4): 871 ## G^2(4): 126 (Likelihood ratio/deviance statistic) ## X^2(4): 308 (Chi-square goodness of fit) ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.59 0.00 0.00 0.00 0.41 ## class 2: 0.00 0.00 0.28 0.38 0.34 ## class 3: 1.00 0.00 0.00 0.00 0.00 ## class 4: 0.00 0.47 0.00 0.00 0.53 ## class 5: 0.43 0.00 0.00 0.57 0.00 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.57 0.43 0.0 0.00 0.00 ## class 2: 0.44 0.56 0.0 0.00 0.00 ## class 3: 0.74 0.00 0.0 0.00 0.26 ## class 4: 0.00 0.30 0.3 0.00 0.40 ## class 5: 0.00 0.36 0.0 0.36 0.27 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.73 0.27 0.000 0.00 0 ## class 2: 0.57 0.43 0.000 0.00 0 ## class 3: 0.00 0.00 0.000 0.00 1 ## class 4: 0.00 0.61 0.079 0.31 0 ## class 5: 0.00 0.00 0.000 0.00 1 ## ## $V4 ## 1 2 4 5 ## class 1: 0 0.79 0 0.21 ## class 2: 1 0.00 0 0.00 ## class 3: 0 0.00 0 1.00 ## class 4: 0 0.30 0 0.70 ## class 5: 0 0.00 1 0.00 ## ## Estimated class population shares ## 0.17 0.11 0.49 0.15 0.092 ## ## Predicted class memberships (by modal posterior prob.) ## 0.16 0.1 0.5 0.15 0.09 ## ## ========================================================= ## Fit for 5 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 79 ## residual degrees of freedom: 21 ## maximum log-likelihood: -271 ## ## AIC(5): 700 ## BIC(5): 905 ## G^2(5): 96 (Likelihood ratio/deviance statistic) ## X^2(5): 119 (Chi-square goodness of fit) ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.50 0.000 0.00 0.50 0.0000 ## class 2: 0.34 0.000 0.00 0.00 0.6632 ## class 3: 1.00 0.000 0.00 0.00 0.0000 ## class 4: 0.94 0.059 0.00 0.00 0.0000 ## class 5: 0.50 0.500 0.00 0.00 0.0004 ## class 6: 0.00 0.000 0.18 0.24 0.5882 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.00 1.00 0.00 0.00 0.00 ## class 2: 1.00 0.00 0.00 0.00 0.00 ## class 3: 0.00 0.00 0.00 0.54 0.46 ## class 4: 0.69 0.00 0.00 0.00 0.31 ## class 5: 0.00 1.00 0.00 0.00 0.00 ## class 6: 0.31 0.46 0.23 0.00 0.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.50 0.00 0.000 0.00 0.50 ## class 2: 1.00 0.00 0.000 0.00 0.00 ## class 3: 0.00 0.00 0.000 0.00 1.00 ## class 4: 0.00 0.00 0.000 0.09 0.91 ## class 5: 0.00 1.00 0.000 0.00 0.00 ## class 6: 0.35 0.59 0.059 0.00 0.00 ## ## $V4 ## 1 2 4 5 ## class 1: 0.00 0 0 1.00 ## class 2: 0.00 1 0 0.00 ## class 3: 0.00 0 1 0.00 ## class 4: 0.00 0 0 1.00 ## class 5: 0.00 1 0 0.00 ## class 6: 0.71 0 0 0.29 ## ## Estimated class population shares ## 0.06 0.11 0.065 0.54 0.06 0.17 ## ## Predicted class memberships (by modal posterior prob.) ## 0.06 0.1 0.06 0.55 0.06 0.17 ## ## ========================================================= ## Fit for 6 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 95 ## residual degrees of freedom: 5 ## maximum log-likelihood: -269 ## ## AIC(6): 728 ## BIC(6): 976 ## G^2(6): 92 (Likelihood ratio/deviance statistic) ## X^2(6): 195 (Chi-square goodness of fit) ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.00 0.00 0.28 0.37 0.35 ## class 2: 0.00 0.00 0.00 0.00 1.00 ## class 3: 0.50 0.00 0.00 0.50 0.00 ## class 4: 1.00 0.00 0.00 0.00 0.00 ## class 5: 0.34 0.00 0.00 0.00 0.66 ## class 6: 0.35 0.65 0.00 0.00 0.00 ## class 7: 1.00 0.00 0.00 0.00 0.00 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.44 0.56 0.00 0.00 0.00 ## class 2: 0.00 0.25 0.75 0.00 0.00 ## class 3: 0.00 1.00 0.00 0.00 0.00 ## class 4: 0.00 0.00 0.00 0.54 0.46 ## class 5: 1.00 0.00 0.00 0.00 0.00 ## class 6: 0.00 0.54 0.00 0.00 0.46 ## class 7: 0.75 0.00 0.00 0.00 0.25 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.56 0.44 0.00 0.0 0.0 ## class 2: 0.00 0.84 0.16 0.0 0.0 ## class 3: 0.50 0.00 0.00 0.0 0.5 ## class 4: 0.00 0.00 0.00 0.0 1.0 ## class 5: 1.00 0.00 0.00 0.0 0.0 ## class 6: 0.00 0.60 0.00 0.4 0.0 ## class 7: 0.00 0.00 0.00 0.0 1.0 ## ## $V4 ## 1 2 4 5 ## class 1: 1 0.00 0 0.00 ## class 2: 0 0.00 0 1.00 ## class 3: 0 0.00 0 1.00 ## class 4: 0 0.00 1 0.00 ## class 5: 0 1.00 0 0.00 ## class 6: 0 0.49 0 0.51 ## class 7: 0 0.00 0 1.00 ## ## Estimated class population shares ## 0.11 0.063 0.06 0.066 0.11 0.11 0.49 ## ## Predicted class memberships (by modal posterior prob.) ## 0.1 0.07 0.06 0.06 0.1 0.11 0.5 ## ## ========================================================= ## Fit for 7 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 111 ## residual degrees of freedom: -11 ## maximum log-likelihood: -256 ## ## AIC(7): 735 ## BIC(7): 1024 ## G^2(7): 64 (Likelihood ratio/deviance statistic) ## X^2(7): 73 (Chi-square goodness of fit) ## ## ALERT: number of parameters estimated ( 111 ) exceeds number of observations ( 100 ) ## ## ALERT: negative degrees of freedom; respecify model ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.00 0.00 0.00 1 0.000 ## class 2: 1.00 0.00 0.00 0 0.000 ## class 3: 0.00 0.00 0.24 0 0.763 ## class 4: 0.00 0.00 0.00 0 1.000 ## class 5: 0.00 1.00 0.00 0 0.000 ## class 6: 1.00 0.00 0.00 0 0.000 ## class 7: 1.00 0.00 0.00 0 0.000 ## class 8: 0.45 0.45 0.00 0 0.098 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.57 0.43 0.00 0.00 0.00 ## class 2: 0.00 0.00 0.00 0.54 0.46 ## class 3: 0.44 0.56 0.00 0.00 0.00 ## class 4: 0.00 0.25 0.75 0.00 0.00 ## class 5: 0.00 0.00 0.00 0.00 1.00 ## class 6: 0.54 0.46 0.00 0.00 0.00 ## class 7: 0.75 0.00 0.00 0.00 0.25 ## class 8: 0.00 1.00 0.00 0.00 0.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0 0.57 0.00 0 0.43 ## class 2: 0 0.00 0.00 0 1.00 ## class 3: 1 0.00 0.00 0 0.00 ## class 4: 0 0.85 0.15 0 0.00 ## class 5: 0 0.00 0.00 1 0.00 ## class 6: 1 0.00 0.00 0 0.00 ## class 7: 0 0.00 0.00 0 1.00 ## class 8: 0 1.00 0.00 0 0.00 ## ## $V4 ## 1 2 4 5 ## class 1: 1.00 0.00 0 0.0 ## class 2: 0.00 0.00 1 0.0 ## class 3: 0.47 0.53 0 0.0 ## class 4: 0.00 0.00 0 1.0 ## class 5: 0.00 0.00 0 1.0 ## class 6: 0.00 0.50 0 0.5 ## class 7: 0.00 0.00 0 1.0 ## class 8: 0.00 1.00 0 0.0 ## ## Estimated class population shares ## 0.07 0.066 0.13 0.067 0.05 0.065 0.49 0.067 ## ## Predicted class memberships (by modal posterior prob.) ## 0.07 0.06 0.13 0.07 0.05 0.06 0.5 0.06 ## ## ========================================================= ## Fit for 8 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 127 ## residual degrees of freedom: -27 ## maximum log-likelihood: -244 ## ## AIC(8): 743 ## BIC(8): 1073 ## G^2(8): 46 (Likelihood ratio/deviance statistic) ## X^2(8): 40 (Chi-square goodness of fit) ## ## ALERT: number of parameters estimated ( 127 ) exceeds number of observations ( 100 ) ## ## ALERT: negative degrees of freedom; respecify model ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.00 0.0 0.00 0.00 1.0000 ## class 2: 0.34 0.0 0.00 0.00 0.6648 ## class 3: 0.00 1.0 0.00 0.00 0.0000 ## class 4: 1.00 0.0 0.00 0.00 0.0000 ## class 5: 0.50 0.5 0.00 0.00 0.0003 ## class 6: 1.00 0.0 0.00 0.00 0.0000 ## class 7: 0.00 0.0 0.33 0.33 0.3333 ## class 8: 0.00 0.0 0.00 1.00 0.0000 ## class 9: 1.00 0.0 0.00 0.00 0.0000 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.00 0.25 0.75 0.00 0.00 ## class 2: 1.00 0.00 0.00 0.00 0.00 ## class 3: 0.00 0.00 0.00 0.00 1.00 ## class 4: 0.00 1.00 0.00 0.00 0.00 ## class 5: 0.00 1.00 0.00 0.00 0.00 ## class 6: 0.75 0.00 0.00 0.00 0.25 ## class 7: 0.00 1.00 0.00 0.00 0.00 ## class 8: 1.00 0.00 0.00 0.00 0.00 ## class 9: 0.00 0.00 0.00 0.54 0.46 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.00 0.86 0.14 0 0.00 ## class 2: 1.00 0.00 0.00 0 0.00 ## class 3: 0.00 0.00 0.00 1 0.00 ## class 4: 1.00 0.00 0.00 0 0.00 ## class 5: 0.00 1.00 0.00 0 0.00 ## class 6: 0.00 0.00 0.00 0 1.00 ## class 7: 0.67 0.00 0.00 0 0.33 ## class 8: 0.00 1.00 0.00 0 0.00 ## class 9: 0.00 0.00 0.00 0 1.00 ## ## $V4 ## 1 2 4 5 ## class 1: 0 0 0 1 ## class 2: 0 1 0 0 ## class 3: 0 0 0 1 ## class 4: 0 0 0 1 ## class 5: 0 1 0 0 ## class 6: 0 0 0 1 ## class 7: 1 0 0 0 ## class 8: 1 0 0 0 ## class 9: 0 0 1 0 ## ## Estimated class population shares ## 0.07 0.11 0.05 0.03 0.06 0.49 0.09 0.04 0.066 ## ## Predicted class memberships (by modal posterior prob.) ## 0.07 0.1 0.05 0.03 0.06 0.5 0.09 0.04 0.06 ## ## ========================================================= ## Fit for 9 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 143 ## residual degrees of freedom: -43 ## maximum log-likelihood: -232 ## ## AIC(9): 750 ## BIC(9): 1123 ## G^2(9): 22 (Likelihood ratio/deviance statistic) ## X^2(9): 18 (Chi-square goodness of fit) ## ## ALERT: number of parameters estimated ( 143 ) exceeds number of observations ( 100 ) ## ## ALERT: negative degrees of freedom; respecify model ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.20 0.0 0.18 0 0.6132 ## class 2: 0.00 0.0 0.00 0 1.0000 ## class 3: 0.00 0.0 0.00 1 0.0000 ## class 4: 0.00 0.0 0.00 1 0.0000 ## class 5: 1.00 0.0 0.00 0 0.0000 ## class 6: 0.50 0.5 0.00 0 0.0009 ## class 7: 1.00 0.0 0.00 0 0.0000 ## class 8: 0.00 1.0 0.00 0 0.0000 ## class 9: 1.00 0.0 0.00 0 0.0000 ## class 10: 0.75 0.0 0.00 0 0.2500 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.59 0.41 0 0.00 0.000 ## class 2: 0.00 0.00 1 0.00 0.000 ## class 3: 1.00 0.00 0 0.00 0.000 ## class 4: 0.00 1.00 0 0.00 0.000 ## class 5: 0.00 0.00 0 0.00 1.000 ## class 6: 0.00 1.00 0 0.00 0.000 ## class 7: 0.00 0.00 0 0.92 0.077 ## class 8: 0.00 0.00 0 0.00 1.000 ## class 9: 0.97 0.00 0 0.00 0.026 ## class 10: 0.00 1.00 0 0.00 0.000 ## ## $V3 ## 1 2 3 4 5 ## class 1: 1.00 0 0.00 0 0 ## class 2: 0.00 1 0.00 0 0 ## class 3: 0.00 1 0.00 0 0 ## class 4: 0.00 0 0.00 0 1 ## class 5: 0.00 0 0.00 0 1 ## class 6: 0.00 1 0.00 0 0 ## class 7: 0.00 0 0.00 0 1 ## class 8: 0.00 0 0.00 1 0 ## class 9: 0.00 0 0.00 0 1 ## class 10: 0.75 0 0.25 0 0 ## ## $V4 ## 1 2 4 5 ## class 1: 0.38 0.6249 0.00 0.00 ## class 2: 0.00 0.0008 0.00 1.00 ## class 3: 1.00 0.0000 0.00 0.00 ## class 4: 0.00 0.0000 1.00 0.00 ## class 5: 0.00 0.0000 0.18 0.82 ## class 6: 0.00 1.0000 0.00 0.00 ## class 7: 0.00 0.0000 1.00 0.00 ## class 8: 0.00 0.0000 0.00 1.00 ## class 9: 0.00 0.0000 0.00 1.00 ## class 10: 0.00 0.0000 0.00 1.00 ## ## Estimated class population shares ## 0.16 0.06 0.04 0.034 0.14 0.06 0.036 0.05 0.38 0.04 ## ## Predicted class memberships (by modal posterior prob.) ## 0.16 0.06 0.04 0.03 0.15 0.06 0.04 0.05 0.37 0.04 ## ## ========================================================= ## Fit for 10 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 159 ## residual degrees of freedom: -59 ## maximum log-likelihood: -239 ## ## AIC(10): 796 ## BIC(10): 1210 ## G^2(10): 41 (Likelihood ratio/deviance statistic) ## X^2(10): 40 (Chi-square goodness of fit) ## ## ALERT: number of parameters estimated ( 159 ) exceeds number of observations ( 100 ) ## ## ALERT: negative degrees of freedom; respecify model ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.95 0.052 0.000 0.00 0.00 ## class 2: 0.24 0.081 0.075 0.18 0.43 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.62 0.00 0.000 0.051 0.33 ## class 2: 0.37 0.54 0.095 0.000 0.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.00 0.00 0.000 0.079 0.921 ## class 2: 0.44 0.44 0.028 0.000 0.085 ## ## $V4 ## 1 2 4 5 ## class 1: 0.00 0.00 0.12 0.88 ## class 2: 0.31 0.46 0.00 0.23 ## ## Estimated class population shares ## 0.6 0.4 ## ## Predicted class memberships (by modal posterior prob.) ## 0.61 0.39 ## ## ========================================================= ## Fit for 2 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 31 ## residual degrees of freedom: 69 ## maximum log-likelihood: -312 ## ## AIC(2): 686 ## BIC(2): 767 ## G^2(2): 160 (Likelihood ratio/deviance statistic) ## X^2(2): 381 (Chi-square goodness of fit) ## We see that the best model is with 2 segments. For the best model we can look at the posterior probabilities. head(LCA_best_model$posterior) # matrix of posterior class membership probabilities ## [,1] ## [1,] 1.000000000000000000000000000000000000000000000000000000 ## [2,] 0.000000000000000000000000000000000031627520483257955497 ## [3,] 0.000000000000000000000000000000000000000000000000000061 ## [4,] 1.000000000000000000000000000000000000000000000000000000 ## [5,] 0.999999998375615928303261625842424109578132629394531250 ## [6,] 0.999999993632272321519849356263875961303710937500000000 ## [,2] ## [1,] 0.0000000000000000000025 ## [2,] 1.0000000000000000000000 ## [3,] 1.0000000000000000000000 ## [4,] 0.0000000000000000000158 ## [5,] 0.0000000016243840415222 ## [6,] 0.0000000063677276792557 Instead of looking at the probabilities we can also look at the actual class predictions. LCA_best_model$predclass %&gt;% head() # class membership table(LCA_best_model$predclass) LCA_best_model$P # size of each class ## [1] 1 2 2 1 1 1 ## ## 1 2 ## 61 39 ## [1] 0.6 0.4 We see the different predictions, and the table shows the distribution. Lastly we have the proportions for each class. options(scipen = 0) # The estimated class-conditional response probabilities LCA_best_model$probs # Ploting (graph=TRUE) them for better interpretation ch2 &lt;- poLCA(f, data, nclass = 2, graph = TRUE) ## $V1 ## 1 2 3 4 5 ## class 1: 0.95 0.053 1.0e-88 1.8e-21 1.4e-26 ## class 2: 0.24 0.080 7.5e-02 1.8e-01 4.3e-01 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.62 6.9e-21 2.7e-53 5.1e-02 3.3e-01 ## class 2: 0.37 5.4e-01 9.5e-02 4.7e-22 2.8e-09 ## ## $V3 ## 1 2 3 4 5 ## class 1: 6.5e-21 5.3e-25 6.6e-49 7.9e-02 0.921 ## class 2: 4.4e-01 4.4e-01 2.8e-02 1.4e-21 0.085 ## ## $V4 ## 1 2 4 5 ## class 1: 4.7e-54 4.9e-22 1.2e-01 0.88 ## class 2: 3.1e-01 4.6e-01 1.3e-09 0.23 ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.30 0.074 0.074 0.15 0.41 ## class 2: 0.95 0.049 0.000 0.00 0.00 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.41 0.48 0.11 0.000 0.0 ## class 2: 0.76 0.00 0.00 0.049 0.2 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.56 0.41 0.037 0.000 0.00 ## class 2: 0.00 0.00 0.000 0.049 0.95 ## ## $V4 ## 1 2 4 5 ## class 1: 0.33 0.41 0.000 0.26 ## class 2: 0.00 0.00 0.049 0.95 ## ## Estimated class population shares ## 0.4 0.6 ## ## Predicted class memberships (by modal posterior prob.) ## 0.4 0.6 ## ## ========================================================= ## Fit for 2 latent classes: ## ========================================================= ## number of observations: 68 ## number of estimated parameters: 31 ## residual degrees of freedom: 37 ## maximum log-likelihood: -212 ## ## AIC(2): 487 ## BIC(2): 555 ## G^2(2): 148 (Likelihood ratio/deviance statistic) ## X^2(2): 508 (Chi-square goodness of fit) ## We see the values for each variable, e.g., in class 1 V2, we primarily have 1’s and 2’s. Where the class 2 vector 2 has only values 1 and 5. # save the class data_copy &lt;- data data_copy$class &lt;- factor(LCA_best_model$predclass) head(data_copy) V1 V2 V3 V4 class 1 4 5 4 1 5 1 1 2 2 4 1 2 1 2 2 5 4 5 1 1 5 5 5 1 1 5 5 NA 1 Now we see that the class that we estimate the observation to be within is added. # learn a BN classifier considering the class as the root node mystructural &lt;- structural.em(data_copy, maximize = &quot;hc&quot;, return.all = TRUE) graphviz.plot(mystructural$dag) We see that the hill climbing method actually sets V3 as an ancestor and not class. This we want to correct. mystructural$dag &lt;- set.arc(mystructural$dag, from = &quot;class&quot;, to = &quot;V3&quot;) graphviz.plot (mystructural$dag) bn.mle &lt;- bn.fit (mystructural$dag, data = data_copy, method = &quot;mle&quot;) Now we have corrected the DAG that is not correct. Hence, we are able to start making inference. Lets say we have a customer who has reported the following: V1 = 1, V2=5, V3=5. Then what would the rating for V4 be? library(gRain) junction &lt;- compile(as.grain(bn.mle)) #We set evidance V1V2V3 &lt;- setEvidence(junction, nodes = c(&quot;V1&quot;, &quot;V2&quot;, &quot;V3&quot;) ,states = c(&quot;1&quot;, &quot;5&quot;, &quot;5&quot;)) querygrain(V1V2V3, nodes = &quot;class&quot;) ## $class ## class ## 1 2 ## 1 0 It is most likely that this person is in class 1. querygrain(V1V2V3, nodes = &quot;V4&quot;) #estimated preference: 0.12*4 + 0.88*5 = 4.88 ## $V4 ## V4 ## 1 2 4 5 ## 0.00 0.00 0.12 0.88 We see that there is 12% probability of rating 4, and 88% probability of rating 5, hence we can aggregate on those. \\(0.12 * 4 + 0.88 * 5 = 4.88\\) 4.5 Latent models for cross selling and acquisition sequence In this section I will introduce two methods. Latent trait model - which is for corss selling, so leans towards the recommender systems as we have previously seen. Latent Markov approach (the hidden Markov chain) - for time-series (dynamic approach). Compared to association rule models, Bayesian Networks collaborative filtering etc. we see that the methods introduced in this chapter is more about the sequence of events and analyzing these. The new method will look at what the customer has bought up until now, or a given date and make recommendations based on these. What data type?, we will see that the two approaches need data in products as column dummies and users as rows, e.g., Notice that the LMM model needs one row pr. user pr. timeperiod. Lets recap, what is a latent trait? This is an udnerlying factor that is unknown hence latent. Therefore, we are going to esimate these. Latent train = underlying characteristics. This is something that cannot be directly measured, that meaning be answered by one question (e.g., as how old are you?), but must be answered through a number of questions. Example of a latent trait. Let’s say that we want to measure intelligence. The procedure would be to ask several questions. So this is basically like an exam, the more correct answers you have, the higher the intelligence. Then one may say, can we assume if a student can answer the most difficult question, will he then be able to answer all the other less difficult questions? If yes, then we in principle only need to know which question was the hardest that he correctly answered, and then we basically just need to look at this item to make recommendations, that could be grading in this example. If not, then we need to know all the other answers as well. Different models needs different solutions. He shows this: Different models for different scenarios Observed variables = manifest variables We see that the latent trait models, which we are going to work with, takes categorical input but will give an continous output. (or more correct, we assume that the underlying scale is continuous). This is also known as Item Response Theory (IRT) He then introduce the item response function (IRF), it looks like the following \\[P(Y = 1 | O_j,a_i,b_i) = \\frac{e^{a_i(O_j-b_i}} {1+e^{a_i(O_j-b_i}}\\] Where \\(a_i\\) = an items location is defined as the amount of latetn trait needed to have a 50% probability of endorsing the item. \\(b\\) = the higher on the trait level a respondent needs to be in order to endorse the item. The Item Response Function Endorsing an item = the probability of buying the product. In the following plot we see two different persons, where the their latent scores are estimated to be different, although they share the same probability of buying the product, hence such curves could look like this: IRF examples We see that they have different latent scores. In a later example we are going to look at bank customers. Where we base the latent score on what financial services that a given customer has. Hence the latent score = financial maturity, this is visualized with the following: IRF example financial services Here we see the relationships between getting the financial services and the expected financial maturity, for instance to have probability of 50% of having a check account, the maturity is low, although to have a loan, you expect the persons to be much much more mature. We see that -1 maturity = 50% probability of having a loan, but that also implies that almost certainty of a person having a check account. Naturally that means that one could also make a list of these models, and we want to know: \\(a_i\\) = the slope parameter, the slope of the sigmoid at 50% \\(b_i\\) = Position parameter, this is the latent score (financial maturity in the example above) at 50% buying probability. See an example below. a_i and b_i scores for each financial service Notice that financial maturity is merely an expression for how far up the ladder (what products a customer have) a given person is and then we can set the B value to plot into the function to assess what products you expect the customer to have. Hence we should be able to calculate the latent score for a given customer based on a set of predictor variables, an example in the following: Example of predictor varaibles + significance and influence on latent score We see that the higher th T-ratio, the higher significance. Then we can also look at the B, e.g., if you are renting, it is a negative effect on financial maturity. Why is this important? We need to know this, to make appropriate recommendations, hence a person with a low latent score (financial maturity) may not get a mortgage loan although he may get a check account. 4.5.1 Latent Trait Model - Applying the LTM Package in R The LSAT data is having 5 questions of increasing difficulty (although not in the actual order) (5 items) and we are going to make an analysis on this. library(ltm) head(LSAT) #We see that itmes are one hot encoded Item 1 Item 2 Item 3 Item 4 Item 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 #Some summary descriptions des &lt;- descript(LSAT) des ## ## Descriptive statistics for the &#39;LSAT&#39; data-set ## ## Sample: ## 5 items and 1000 sample units; 0 missing values ## ## Proportions for each level of response: ## 0 1 logit ## Item 1 0.076 0.92 2.50 ## Item 2 0.291 0.71 0.89 ## Item 3 0.447 0.55 0.21 ## Item 4 0.237 0.76 1.17 ## Item 5 0.130 0.87 1.90 ## ## ## Frequencies of total scores: ## 0 1 2 3 4 5 ## Freq 3 20 85 237 357 298 ## ## ## Point Biserial correlation with Total Score: ## Included Excluded ## Item 1 0.36 0.11 ## Item 2 0.57 0.15 ## Item 3 0.62 0.17 ## Item 4 0.53 0.14 ## Item 5 0.44 0.12 ## ## ## Cronbach&#39;s alpha: ## value ## All Items 0.29 ## Excluding Item 1 0.28 ## Excluding Item 2 0.24 ## Excluding Item 3 0.22 ## Excluding Item 4 0.25 ## Excluding Item 5 0.27 ## ## ## Pairwise Associations: ## Item i Item j p.value ## 1 1 5 0.565 ## 2 1 4 0.208 ## 3 3 5 0.113 ## 4 2 4 0.059 ## 5 1 2 0.028 ## 6 2 5 0.009 ## 7 1 3 0.003 ## 8 4 5 0.002 ## 9 3 4 7e-04 ## 10 2 3 4e-04 We can see the frequency of the different scores etc. The important thing with the descriptive statistics In the proportions for each level of response: We see that item 1 is the simplest and item 3 is the most difficult. That we will be clear from the res output. 0 = proportion of wrong answers, 1 = proportion of correct answers. The other important thing is the pairwise associations: where we see that if there is correlation between the different questions. High p = correlation, low correlation = low p-value. We want to find some high p-values, of not, the model does not make any sense. Now lets fit the model. fit &lt;- ltm(LSAT ~ z1) #z1 for the birnbaum model fit ## ## Call: ## ltm(formula = LSAT ~ z1) ## ## Coefficients: ## Dffclt Dscrmn ## Item 1 -3.36 0.82 ## Item 2 -1.37 0.72 ## Item 3 -0.28 0.89 ## Item 4 -1.87 0.69 ## Item 5 -3.12 0.66 ## ## Log.Lik: -2467 We see: Coefficients: that is where they are placed on the scale, e.g., item 3 (Q3) leads to the highest maturity. Dffclt = location parameters Dscrmn = the slope We can look at the different scores depending on the correct answers, that is for each of the persons. res &lt;- factor.scores(fit) res #Fore respondents ## ## Call: ## ltm(formula = LSAT ~ z1) ## ## Scoring Method: Empirical Bayes ## ## Factor-Scores for observed response patterns: ## Item 1 Item 2 Item 3 Item 4 Item 5 Obs Exp z1 se.z1 ## 1 0 0 0 0 0 3 2.3 -1.895 0.80 ## 2 0 0 0 0 1 6 5.9 -1.479 0.80 ## 3 0 0 0 1 0 2 2.6 -1.460 0.80 ## 4 0 0 0 1 1 11 8.9 -1.041 0.80 ## 5 0 0 1 0 0 1 0.7 -1.331 0.80 ## 6 0 0 1 0 1 1 2.6 -0.911 0.80 ## 7 0 0 1 1 0 3 1.2 -0.891 0.80 ## 8 0 0 1 1 1 4 6.0 -0.463 0.81 ## 9 0 1 0 0 0 1 1.8 -1.438 0.80 ## 10 0 1 0 0 1 8 6.4 -1.019 0.80 ## 11 0 1 0 1 1 16 13.6 -0.573 0.81 ## 12 0 1 1 0 1 3 4.4 -0.441 0.81 ## 13 0 1 1 1 0 2 2.0 -0.420 0.81 ## 14 0 1 1 1 1 15 13.9 0.023 0.83 ## 15 1 0 0 0 0 10 9.5 -1.373 0.80 ## 16 1 0 0 0 1 29 34.6 -0.953 0.80 ## 17 1 0 0 1 0 14 15.6 -0.933 0.80 ## 18 1 0 0 1 1 81 76.6 -0.506 0.81 ## 19 1 0 1 0 0 3 4.7 -0.803 0.80 ## 20 1 0 1 0 1 28 25.0 -0.373 0.81 ## 21 1 0 1 1 0 15 11.5 -0.352 0.81 ## 22 1 0 1 1 1 80 83.5 0.093 0.83 ## 23 1 1 0 0 0 16 11.3 -0.911 0.80 ## 24 1 1 0 0 1 56 56.1 -0.483 0.81 ## 25 1 1 0 1 0 21 25.6 -0.463 0.81 ## 26 1 1 0 1 1 173 173.3 -0.022 0.83 ## 27 1 1 1 0 0 11 8.4 -0.329 0.82 ## 28 1 1 1 0 1 61 62.5 0.117 0.83 ## 29 1 1 1 1 0 28 29.1 0.139 0.83 ## 30 1 1 1 1 1 298 296.7 0.606 0.85 We see that Z1 is where a person is set on a the latent scale. where person no. 30 (or this group of persons as there are 298) has a latent score of 0.606. He is also plotted with the red line in the following plot. Now we can also plot the item response functions. I have added ablines in the plot, to visualize the latent scores of 50% probability of buying. plot(fit) #Lets add plot the coefficient and check if it adds up to 50% prob. abline(h = 0.5,lty = 2,col = &quot;darkgrey&quot;) abline(v = -0.280,lty = 2,col = &quot;darkgrey&quot;) text(x = -0.0280,y = 0.024,&quot;-0.0280 (coef)&quot;,col = &quot;green&quot;,cex = 0.7) text(x = 1.9,y = 0.51,&quot;50% prob.&quot;,col = &quot;green&quot;,cex = 0.7) abline(v = 0.606,lty = 2,col = &quot;darkred&quot;) #Group 30 from the res text(x = 1.0,y = 0.26,&quot;Person 30&quot;,col = &quot;darkred&quot;,cex = 0.7) We see that at the visualize maturity, there is almost certinaty that the person is correctly answering item 1 (Q1). 4.5.2 The latent (hidden) Markov model Let me first summarize the Markov Chain The Markov chain model looks as the following: The Markov Chain model This model has some transition probabilities assigned. Transition Probabilities We see that state 1 is the most recent purchasers and then state 5 is the lost customers. Hence we see one can move from e.g., state 4 to 1. The Markov Chain is that there is a probability of moving upwards in the states (recency states) or they can move down in states. In the end there is a catch all sate, where we assume that you will stay the rest of the time, namely where you dont buy again. But what is the Hidden Markov Chain? (HMC) It is basically what it says, it is hidden. But what does the states then reflect? In the hidden markov chain the states reflect different buying patterns, hence you will try to estimate if a given person has changed his buying patterns and what that means for him. The difference to the Latent Trait Model (LTM): It is that previously we just had one row pr. person (or group with identical person) we are now having one row pr. customer pr. transaction. To explain this, he applied a case with COOP, the data looked the following: COOP data Characteristics of the study (hence also the HMC) Assume that households may be in different states with different purchasing patterns, which is in general also the assumption for the HMC. Across time a household is allowed to switch form one state to another, which is also the assumption for HMC. Then the Markovian assumption, meaning that the latent state membership for a household in a given time period is assumed to only depend on the state membership in the previous time period. Why take a look at the chain above, and see that this is the only thing we need to know, to address which outcomes there may be. The procedure: They calculated the probability of a households buying products within given categories, see the following: Proportions of buying certain items: The example is one household, where each row is the timeperiod and each column is a product category. We see that for instance there is a tendency to buy more vegetables in the periods. We can also show the probability of buying an item in the different states, see the following: State probabilities of buying items Hence we see that in the overall probability of buying something decrease as the state lowers. The same goes for each group. Now we can also look at the transition probabilities. We see: Big values omn the diagonal, indicating that there is a great probability of a person staying in the same state. We see that the probability of moving from state 7 to state 6 is 18.6%, on the other hand if you are in state 6, there is a probability of going to state 7. In general, we see a very low probability of reaching state 7 (those buying the least). Transition probabilities Then the question is, what are we actually adopting, when moving from one state to another. Products that are adopted when moving from one state to another, e.g., when moving from state 7 (all item groups being low) we see that people start adding dairy products and so on. Hence we see that adding dairy indicating that a customer on an upwards journey. Then we can see that if a person starts adding vegetables he will end in state 5 and if he adds eggs and vegetables, he will end up ion state 4. What do we get out of this? In the example he showed one of the COOP private label milk, where they originally marketed a beer on one side of the beers, where they then added ecological eggs, as we see that this will manipulate the customer upwards in the states. This was a great results. Hence again, it about knowing enough about the customer to know what to recommend them, and not just market some random product. "],["customer-behavior.html", "5 Customer behavior 5.1 Characteristics of cliskstream 5.2 Applications with clickstream data 5.3 AB Testing", " 5 Customer behavior 5.1 Characteristics of cliskstream We see that this can be organized in a pyramid. Characteristics of clickstream data We see that the more information we have about the actual user the more valuable is the clickstream data. We saw an example with clickstreams from the google merchendise, here we information on the cookie, timestamps, eventID, sessionID, destination (the actual page they are visiting etc.). Naturally this must be organized so it can be used for analysis. It the slidedeck there is a bunch of steps for preparing data, although I would argue that they are very case sensitive. Challenges with clickstreams: One must choose the right period for evaluation, as we want to have as many full cycles as possible, hence if we start the survey at the 1 of january, then customers starting to visit the page in december will appear as new customers, while they are not. The same applies at the end of the period. Customer use different devices for the same purposes, these are difficult to track. Cookies expires after a certain time period. If a user is having different windows open, it is difficult to know which page the user is actually on. Many patterns in the data is due to how the website is constructed. Where do customers come from?: one could investigate the following: What type of device is used, PC, tableg, mobile etc. What product category is visited. What is the entry channel? advertising, direct access, social media etc. Time of entry: At what time the person visit the page? How does the person behave on the webiste?: it is important to know which panels the customer choose, where they go from a product page, from the front page, from the basket etc. Conversion rates: We need to know the conversion rates of the customers. 5.2 Applications with clickstream data For modeling sequences of clicks: neural networks (NN) and Markov chains are two of the most common type of models. NN are black-box models best for prediction (not covered here). Markov models are transparent models that are used both for predicting and understanding customer behavior. In this application, I focus on the “clickstream” library to model the data with Markov chains models. From time to time I refer to other R packages that offer similar functionality. What is a (discrete state) Markov Chain? (note: we can have Markov chains with discrete and continous states and in discrete and in continuous time). A Markov chain is a stochastic process X that takes state m from a ﬁnite set of states M at each time n. If the state in n only depends on the recent k states, we call X a Markov chain of order k. For example: 0-order Markov chain = the probability to be in any of the m states in the next step is independent of the present state. Hence states are completely independent of each other. 1st-order Markov chain = the probability to be in any of the m states in the next step is independent of the previous states given the present state (one-period memory) 2nd-order Markov chain = two-periods memory 3rd-order Markov chain = three-periods memory …so on Markov chains can be described by transition probability matrices =&gt; Each value in these matrices is a parameter Higher-order Markov chains have \\((m−1)m^k\\) model parameters =&gt; the number of lag parameters increases exponentially Markov Chain It is about sequences of events and we want to use markov chain to analyse the sequence of events, i.e. the clickstream. Hence what we want to get out of it, is the likelyhood of converting. Recall in a Bayesian Network, that variables would only have influence in one direction and hence not create loops etc. So we see that for instance a sequence of events, you would only assess the adjacent click events. With markov chain, we are able to make more relationships e.g., non adjacent events having influence of each other. Recall that HJ showed a hidden markov chain where this is not a hidden markov chain. Why Markov Chain and not a Bayesian Network? We see that a person can be jumping among a set of pages, hence we can create cycles (where one person can start and end at the same page), this is not allowed in a bayesian network, hence we cannot apply this. How we select the Markov Chain order: Usually, a user considering a Product Page might either Add the product to the shopping cart, view Product Reviews, follow a Product Recommendation, or Search for another product. Moe (2003) proposes that the probability for a transition to either of the possible next states depends on the MODE (browsing, searching, or buying) the user is currently in. This MODE (latent state) can be identiﬁed when considering the recent k states. Model order selection is usually based on a criteria like AIC or BIC. The function fitMarkovChain() estimates the parameters of a Markov chain model of order k. How does R interpret the clickstreams? Clickstreams = collection of data sequences - with different sizes! Notice that session = an individual, i.e., their user session. P = page An example: Session 1: P1 P2 P1 P3 P4 Defer Session 2: P3 P4 P1 P3 Defer Session 3: P5 P1 P6 P7 P6 P7 P8 P7 Buy Session 4: P9 P2 P11 P12 P11 P13 P11 Buy Session 5: P4 P6 P11 P6 P1 P3 Defer Session 6: P3 P13 P12 P4 P12 P1 P4 P1 P3 Defer - this is examplified in the DAG Session 7: P10 P5 P10 P8 P8 P5 P1 P7 Buy Session 8: P9 P2 P1 P9 P3 P1 Defer Session 9: P5 P8 P5 P7 P4 P1 P6 P4 Defer There are 13 possible product pages and two absorbing stages, that are stages you can end up in. Hence we see that a clickstream is a sequence of events. This can naturally be shown as a vector in R and we can apply the function readClickstreams(), the input must be a commaseperated file. The following is an example where we start in P3, then go to p13, P12, P4, P12, P1, P4, P1 back to P3 and then leave. The graph may be a biiit difficult to follow, but following the path (directions) one will end up in defer. Notice that we did not do this during the class, although I did it for fun. library(ggdag) #Generating the relationships dag &lt;- dagitty::dagitty(&quot;dag { P3 -&gt; P13 -&gt; P12 -&gt; P4 -&gt; P12 -&gt; P1 -&gt; P4 -&gt; P1 -&gt; P3 -&gt; Defer }&quot; ) tidy_dag &lt;- tidy_dagitty(dag) #Adding information on the color starting_page &lt;- &quot;P3&quot; tidy_dag$data &lt;- dplyr::mutate(tidy_dag$data, colour = ifelse(tidy_dag$data$name == starting_page ,&quot;Starting page&quot;, &quot;Visited page&quot;)) #Making the plot tidy_dag %&gt;% ggplot(aes( x = x, y = y, xend = xend, yend = yend )) + geom_dag_point(aes(colour = colour)) + geom_dag_edges() + geom_dag_text() + theme_dag() 5.2.1 A motivating example library(clickstream) cls &lt;- readClickstreams(file = &quot;Data/Customer behavior/sample.csv&quot; , sep = &quot;,&quot; , header = TRUE) cls #Shows sessions summary(cls) #Summary stats # writeClickstreams(cls, &quot;sample.csv&quot;, header = TRUE, sep = &quot;,&quot;) ## Clickstreams ## ## Session1: P1 P2 P1 P3 P4 Defer ## Session2: P3 P4 P1 P3 Defer ## Session3: P5 P1 P6 P7 P6 P7 P8 P7 Buy ## Session4: P9 P2 P11 P12 P11 P13 P11 Buy ## Session5: P4 P6 P11 P6 P1 P3 Defer ## Session6: P3 P13 P12 P4 P12 P1 P4 P1 P3 Defer ## Session7: P10 P5 P10 P8 P8 P5 P1 P7 Buy ## Session8: P9 P2 P1 P9 P3 P1 Defer ## Session9: P5 P8 P5 P7 P4 P1 P6 P4 Defer ## Observations: 9 ## ## Click Frequencies: ## Buy Defer P1 P10 P11 P12 P13 P2 P3 P4 P5 P6 P7 ## 3 6 11 2 4 3 2 3 7 7 5 5 5 ## P8 P9 ## 4 3 We see the frequencies of each page, where it is clear that P1 is most visited, hence might also be the front page. Now we can fit the model and s options(scipen = 999,digits = 2) order &lt;- 2 mc &lt;- fitMarkovChain(clickstreamList = cls,order = order #= two-periods memory, i.e. 2 lags ,control = list(optimizer = &quot;quadratic&quot;)) mc ## Higher-Order Markov Chain (order=2) ## ## Transition Probabilities: ## ## Lag: 1 ## lambda: 0.22 ## Buy Defer P1 P10 P11 P12 P13 P2 P3 P4 P5 P6 P7 P8 P9 ## Buy 0 0 0.000 0.0 0.25 0.00 0.0 0.00 0.00 0.00 0.0 0.0 0.4 0.00 0.00 ## Defer 0 0 0.091 0.0 0.00 0.00 0.0 0.00 0.43 0.29 0.0 0.0 0.0 0.00 0.00 ## P1 0 0 0.000 0.0 0.00 0.33 0.0 0.67 0.14 0.43 0.4 0.2 0.0 0.00 0.00 ## P10 0 0 0.000 0.0 0.00 0.00 0.0 0.00 0.00 0.00 0.2 0.0 0.0 0.00 0.00 ## P11 0 0 0.000 0.0 0.00 0.33 0.5 0.33 0.00 0.00 0.0 0.2 0.0 0.00 0.00 ## P12 0 0 0.000 0.0 0.25 0.00 0.5 0.00 0.00 0.14 0.0 0.0 0.0 0.00 0.00 ## P13 0 0 0.000 0.0 0.25 0.00 0.0 0.00 0.14 0.00 0.0 0.0 0.0 0.00 0.00 ## P2 0 0 0.091 0.0 0.00 0.00 0.0 0.00 0.00 0.00 0.0 0.0 0.0 0.00 0.67 ## P3 0 0 0.364 0.0 0.00 0.00 0.0 0.00 0.00 0.00 0.0 0.0 0.0 0.00 0.33 ## P4 0 0 0.091 0.0 0.00 0.33 0.0 0.00 0.29 0.00 0.0 0.2 0.2 0.00 0.00 ## P5 0 0 0.000 0.5 0.00 0.00 0.0 0.00 0.00 0.00 0.0 0.0 0.0 0.50 0.00 ## P6 0 0 0.182 0.0 0.25 0.00 0.0 0.00 0.00 0.14 0.0 0.0 0.2 0.00 0.00 ## P7 0 0 0.091 0.0 0.00 0.00 0.0 0.00 0.00 0.00 0.2 0.4 0.0 0.25 0.00 ## P8 0 0 0.000 0.5 0.00 0.00 0.0 0.00 0.00 0.00 0.2 0.0 0.2 0.25 0.00 ## P9 0 0 0.091 0.0 0.00 0.00 0.0 0.00 0.00 0.00 0.0 0.0 0.0 0.00 0.00 ## ## Lag: 2 ## lambda: 0.78 ## Buy Defer P1 P10 P11 P12 P13 P2 P3 P4 P5 P6 P7 P8 P9 ## Buy 0 0 0.1 0.0 0.00 0.00 0.5 0.00 0.00 0.0 0.0 0.0 0.00 0.25 0.00 ## Defer 0 0 0.3 0.0 0.00 0.00 0.0 0.00 0.50 0.0 0.0 0.2 0.00 0.00 0.00 ## P1 0 0 0.2 0.0 0.33 0.00 0.0 0.00 0.25 0.2 0.0 0.0 0.33 0.25 0.67 ## P10 0 0 0.0 0.5 0.00 0.00 0.0 0.00 0.00 0.0 0.0 0.0 0.00 0.00 0.00 ## P11 0 0 0.0 0.0 0.67 0.00 0.0 0.00 0.00 0.2 0.0 0.0 0.00 0.00 0.33 ## P12 0 0 0.0 0.0 0.00 0.33 0.0 0.33 0.25 0.0 0.0 0.0 0.00 0.00 0.00 ## P13 0 0 0.0 0.0 0.00 0.33 0.0 0.00 0.00 0.0 0.0 0.0 0.00 0.00 0.00 ## P2 0 0 0.0 0.0 0.00 0.00 0.0 0.00 0.00 0.0 0.0 0.0 0.00 0.00 0.00 ## P3 0 0 0.1 0.0 0.00 0.00 0.0 0.33 0.00 0.4 0.0 0.2 0.00 0.00 0.00 ## P4 0 0 0.2 0.0 0.00 0.33 0.5 0.00 0.00 0.0 0.2 0.0 0.00 0.00 0.00 ## P5 0 0 0.0 0.0 0.00 0.00 0.0 0.00 0.00 0.0 0.2 0.0 0.00 0.25 0.00 ## P6 0 0 0.0 0.0 0.00 0.00 0.0 0.00 0.00 0.2 0.2 0.4 0.00 0.00 0.00 ## P7 0 0 0.1 0.0 0.00 0.00 0.0 0.00 0.00 0.0 0.2 0.0 0.67 0.25 0.00 ## P8 0 0 0.0 0.5 0.00 0.00 0.0 0.00 0.00 0.0 0.2 0.2 0.00 0.00 0.00 ## P9 0 0 0.0 0.0 0.00 0.00 0.0 0.33 0.00 0.0 0.0 0.0 0.00 0.00 0.00 ## ## Start Probabilities: ## ## P1 P10 P3 P4 P5 P9 ## 0.11 0.11 0.22 0.11 0.22 0.22 ## ## End Probabilities: ## ## Buy Defer ## 0.33 0.67 The outcome: the two transition probabilities matrices for the two lags start probabilities for the states the corresponding clickstreams started end probabilities for the states the corresponding clickstreams ended We see that with lag = 1, ther is only conversion (buying probability) from P11 and P7, although looking one further step back, we see more pages may also lead to a purchase. I guess P11 and P7 have something to do with a basket or a checkout. Then we can also see the different probabilities of moving from one page to another. Lastly we can see the different start probabilities, that is the probability of a person starting on a given page. Also we can see the end probabilities, that being posterior probabilities of buying or not. Lets now compute the summary of the fitted model and plot the transition probabilities. summary(mc) plot(mc, order = order) ## Higher-Order Markov Chain (order=2) with 15 states. ## The Markov Chain has absorbing states. ## ## Observations: 70 ## LogLikelihood: -66 ## AIC: 196 ## BIC: 268 We see that fitMarkovChain() computes the log-likelihood, hence we can calculate AIC and BIC. Here we see the flow of typical flow 5.2.1.1 i) Predict either the next click or the ﬁnal click (state) of a customer. Now we can use it to make a prediction. If a customer starts with the clickstream P9 P2, what will do next? #Pattern to evaluate pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;P9&quot;, &quot;P2&quot;)) #using predict resultPattern &lt;- predict(mc , startPattern = pattern , dist = 1 #How far we want to predict, when he has clicked p9 and p2 ) resultPattern #the user will most likely click on P1 next ## Sequence: P1 ## Probability: 0.67 ## Absorbing Probabilities: ## None ## 1 NaN There is a 66% chance of clicking P1. Now we make another prediction where a person has viewed page 9 and 2, with the following: Predict 2 steps into the future. Here we want the likelyhood of purchase pattern &lt;- new(&quot;Pattern&quot; ,sequence = c(&quot;P9&quot;, &quot;P2&quot;) #A sequence for the session ,absorbingProbabilities = data.frame(Buy = 0.333, Defer = 0.667) #provide probs for buying ) resultPattern &lt;- predict(mc, startPattern = pattern, dist = 2) resultPattern ## Sequence: P1 P3 ## Probability: 0.26 ## Absorbing Probabilities: ## Buy Defer ## 1 0.058 0.94 The person visit products P1 and then P3, although this pattern only has a 26% probability. Purchasing probability is 5.83% after 2 further clicks, and the person is most likely to defer the purchase. Online stores often have evidence on how many of the visitors convert to a buyer (this info is typically used to formulate initial absorbing probabilities for all users). But, particularly for customers who log on to their account, the online stores can also know how many times a particular user has been only visiting the online store and how often she has bought a product, This information can be used to formulate initial absorbing probabilities for a user. If for example a user has been logged in and ﬁnally bought a product in 50% of her log-ins, we can compute absorbing probabilities (posterior) for a stream of clicks: Now imagine that the probability of buying and defering is 50/50. absorbingProbabilities &lt;- c(0.5, 0.5) sequence &lt;- c(&quot;P9&quot;, &quot;P2&quot;) for (s in sequence) { absorbingProbabilities &lt;- absorbingProbabilities * data.matrix(subset(mc@absorbingProbabilities, state == s, select = c(&quot;Buy&quot;, &quot;Defer&quot;))) } absorbingProbabilities &lt;- absorbingProbabilities /sum(absorbingProbabilities) absorbingProbabilities ## Buy Defer ## 15 0.23 0.77 22.62% to ﬁnally buy a product after she has visited products P9 and P2. Although we see that it is cumbersome to just hardcode the patterns and the absorbing rates, because the customer journey is not homogenious for all customers. Therefore, we can define cluster and generalize on the clusters instead of on all customers. That is done in the following section 5.2.1.2 ii) Clustering and generalizing upon clickstreams An alternative before running the model is to identify segments of customers by clustering clickstreams and afterwards building a model for each cluster (Huang, Ng, Ching, Ng, and Cheung, 2001, k-means alg. and Euclidean distance) set.seed(12345) clusters &lt;- clusterClickstreams(clickstreamList = cls, order = 1,centers = 3) clusters ## [[1]] ## Clickstreams ## ## Session3: P5 P1 P6 P7 P6 P7 P8 P7 Buy ## Session7: P10 P5 P10 P8 P8 P5 P1 P7 Buy ## Session9: P5 P8 P5 P7 P4 P1 P6 P4 Defer ## ## [[2]] ## Clickstreams ## ## Session1: P1 P2 P1 P3 P4 Defer ## Session4: P9 P2 P11 P12 P11 P13 P11 Buy ## Session8: P9 P2 P1 P9 P3 P1 Defer ## ## [[3]] ## Clickstreams ## ## Session2: P3 P4 P1 P3 Defer ## Session5: P4 P6 P11 P6 P1 P3 Defer ## Session6: P3 P13 P12 P4 P12 P1 P4 P1 P3 Defer We see that this creates three clusters. Notice that this is a list element, hence we could also look them up individually. Now we can fit a Markov Chain each of the clusters mc_clu1 &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[1]], order = 2,control = list(optimizer = &quot;quadratic&quot;)) mc_clu1 mc_clu2 &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[2]], order = 2,control = list(optimizer = &quot;quadratic&quot;)) #mc_clu2 mc_clu3 &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[3]], order = 2,control = list(optimizer = &quot;quadratic&quot;)) #mc_clu3 # or write these objects to ﬁle with writeClickstreams(). ## Higher-Order Markov Chain (order=2) ## ## Transition Probabilities: ## ## Lag: 1 ## lambda: 1 ## Buy Defer P1 P10 P4 P5 P6 P7 P8 ## Buy 0 0 0.00 0.0 0.0 0.0 0.00 0.4 0.00 ## Defer 0 0 0.00 0.0 0.5 0.0 0.00 0.0 0.00 ## P1 0 0 0.00 0.0 0.5 0.4 0.00 0.0 0.00 ## P10 0 0 0.00 0.0 0.0 0.2 0.00 0.0 0.00 ## P4 0 0 0.00 0.0 0.0 0.0 0.33 0.2 0.00 ## P5 0 0 0.00 0.5 0.0 0.0 0.00 0.0 0.50 ## P6 0 0 0.67 0.0 0.0 0.0 0.00 0.2 0.00 ## P7 0 0 0.33 0.0 0.0 0.2 0.67 0.0 0.25 ## P8 0 0 0.00 0.5 0.0 0.2 0.00 0.2 0.25 ## ## Lag: 2 ## lambda: 0.000000033 ## Buy Defer P1 P10 P4 P5 P6 P7 P8 ## Buy 0 0 0.33 0.0 0 0.0 0.00 0.00 0.25 ## Defer 0 0 0.00 0.0 0 0.0 0.33 0.00 0.00 ## P1 0 0 0.00 0.0 0 0.0 0.00 0.33 0.25 ## P10 0 0 0.00 0.5 0 0.0 0.00 0.00 0.00 ## P4 0 0 0.33 0.0 0 0.2 0.00 0.00 0.00 ## P5 0 0 0.00 0.0 0 0.2 0.00 0.00 0.25 ## P6 0 0 0.00 0.0 1 0.2 0.33 0.00 0.00 ## P7 0 0 0.33 0.0 0 0.2 0.00 0.67 0.25 ## P8 0 0 0.00 0.5 0 0.2 0.33 0.00 0.00 ## ## Start Probabilities: ## ## P10 P5 ## 0.33 0.67 ## ## End Probabilities: ## ## Buy Defer ## 0.67 0.33 We see that this construct the same output as we have previously seen, when just analyzing one session. 5.2.2 Second exercise (clickstream) more extensive Full example with simulated data: clickstreams for 100,000 user sessions clicks are either 1 of 7 products or on one of the two ﬁnal states “Buy” and “Defer”. set.seed(123) cls &lt;- randomClickstreams(states = c(&quot;P1&quot;, &quot;P2&quot;, &quot;P3&quot;, &quot;P4&quot;, &quot;P5&quot;, &quot;P6&quot;, &quot;P7&quot;, &quot;Defer&quot;, &quot;Buy&quot;), startProbabilities = c(0.2, 0.25, 0.1, 0.15, 0.1, 0.1, 0.1, 0, 0), transitionMatrix = matrix(c(0.01, 0.09, 0.05, 0.21, 0.12, 0.17, 0.11, 0.2, 0.04, 0.1, 0, 0.29, 0.06, 0.11, 0.13, 0.21, 0.1, 0, 0.07, 0.16, 0.03, 0.25, 0.23, 0.08, 0.03, 0.12, 0.03, 0.16, 0.14, 0.07, 0, 0.05, 0.22, 0.19, 0.1, 0.07, 0.24, 0.27, 0.17, 0.13, 0, 0.03, 0.09, 0.06, 0.01, 0.11, 0.18, 0.04, 0.15, 0.26, 0, 0.1, 0.11, 0.05, 0.21, 0.07, 0.08, 0.2, 0.14, 0.18, 0.02, 0.08, 0.02, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), nrow = 9), meanLength = 50, n = 100000) summary(cls) ## Observations: 100000 ## ## Click Frequencies: ## Buy Defer P1 P2 P3 P4 P5 P6 P7 ## 22087 77813 108767 113760 86334 111054 97129 93397 89521 Here we want to select the right orders. Here we test all the models in order 1 to 5 and then we evaluate BIC for each. maxOrder &lt;- 3 #Max no. of previous steps to evaluate result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = cls, order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result Order AIC BIC 1 2685427 2685543 2 2684008 2684240 3 2684028 2684376 We see that order 2 has the BIC closest to 0, hence we go for order 2. Now we can fit the model with order of 2. mc &lt;- fitMarkovChain(clickstreamList = cls, order = 2,control = list(optimizer = &quot;quadratic&quot;)) mc ## Higher-Order Markov Chain (order=2) ## ## Transition Probabilities: ## ## Lag: 1 ## lambda: 0.00000071 ## Buy Defer P1 P2 P3 P4 P5 P6 P7 ## Buy 0 0 0.0395 0.000 0.031 0.071 0.0094 0.048 0.021 ## Defer 0 0 0.1995 0.101 0.121 0.100 0.0607 0.109 0.080 ## P1 0 0 0.0099 0.100 0.070 0.160 0.2394 0.110 0.212 ## P2 0 0 0.0896 0.000 0.160 0.140 0.2702 0.180 0.070 ## P3 0 0 0.0504 0.289 0.030 0.071 0.1707 0.041 0.080 ## P4 0 0 0.2112 0.061 0.251 0.000 0.1298 0.153 0.199 ## P5 0 0 0.1198 0.109 0.229 0.050 0.0000 0.258 0.139 ## P6 0 0 0.1704 0.130 0.080 0.218 0.0301 0.000 0.179 ## P7 0 0 0.1098 0.211 0.029 0.191 0.0898 0.101 0.020 ## ## Lag: 2 ## lambda: 1 ## Buy Defer P1 P2 P3 P4 P5 P6 P7 ## Buy 0 0 0.037 0.033 0.033 0.028 0.029 0.027 0.039 ## Defer 0 0 0.096 0.110 0.099 0.117 0.128 0.099 0.120 ## P1 0 0 0.154 0.128 0.152 0.116 0.090 0.153 0.116 ## P2 0 0 0.143 0.147 0.147 0.110 0.085 0.136 0.147 ## P3 0 0 0.103 0.065 0.134 0.101 0.121 0.144 0.090 ## P4 0 0 0.110 0.188 0.094 0.166 0.144 0.116 0.131 ## P5 0 0 0.121 0.160 0.081 0.161 0.133 0.076 0.123 ## P6 0 0 0.115 0.106 0.120 0.104 0.143 0.125 0.113 ## P7 0 0 0.120 0.065 0.141 0.097 0.127 0.126 0.121 ## ## Start Probabilities: ## ## P1 P2 P3 P4 P5 P6 P7 ## 0.201 0.253 0.100 0.148 0.098 0.100 0.099 ## ## End Probabilities: ## ## Buy Defer P1 P2 P3 P4 P5 P6 P7 ## 0.22087 0.77813 0.00012 0.00015 0.00011 0.00011 0.00016 0.00017 0.00018 We see: Transition probabilities between different pages and outcomes both at lag 1 and lag 2 (that is two steps back) Start probabilities: there is the highest chance of starting on page 2. End probabilities: We see the probabilities of ending on different pages or in the absorbing stages, buying or deferring. Now we can start making predictions. We see that for a customer visits product 3 and 4 pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;P3&quot;, &quot;P4&quot;) #If a person visit P3 and then P4 , absorbingProbabilities = data.frame(Buy = 0.22 #mc@end[&quot;Buy&quot;] , Defer = 0.78 #mc@end[&quot;Defer&quot;] ) ) resultPattern &lt;- predict(mc, startPattern = pattern, dist = 1) #Pred one step ahead resultPattern ## Sequence: P1 ## Probability: 0.15 ## Absorbing Probabilities: ## Buy Defer ## 1 0.07 0.93 We see that there is a 15% chance of visiting page 1, and 7% chance of buying. Now we do some clustering. Hence, we predict for clusters instead of depicted clickstreams. library(doParallel) CoreCount &lt;- makePSOCKcluster(detectCores()-1) registerDoParallel(CoreCount) clusters_sim &lt;- clusterClickstreams(clickstreamList = cls, order = 1, centers = 3) # takes 5-10 min. to converge stopCluster(CoreCount) registerDoSEQ() Now we could look at each cluster, I choose not to. It is basically what we have seen before with the transition probabilities, start and end probabilities. summary(clusters_sim$clusters[[1]]) summary(clusters_sim$clusters[[2]]) summary(clusters_sim$clusters[[3]]) ## Observations: 17080 ## ## Click Frequencies: ## Buy Defer P1 P2 P3 P4 P5 P6 P7 ## 3918 13156 18019 30752 32478 22749 19522 15591 13843 ## Observations: 34059 ## ## Click Frequencies: ## Buy Defer P1 P2 P3 P4 P5 P6 P7 ## 7821 26144 59025 63554 39576 61161 62488 59616 57113 ## Observations: 48861 ## ## Click Frequencies: ## Buy Defer P1 P2 P3 P4 P5 P6 P7 ## 10348 38513 31723 19454 14280 27144 15119 18190 18565 Now we can loop through the the no. of steps to use for prediction. The following is merely for cluster 1. # mc for clu 1 maxOrder &lt;- 3 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters_sim$clusters[[1]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result Order AIC BIC 1 536233 536333 2 536253 536453 3 536273 536574 We see that the best model is an order 1, as BIC is closest to 0. mc_clu1 &lt;- fitMarkovChain(clickstreamList = clusters_sim$clusters[[1]], order = 1) summary(mc_clu1) ## First-Order Markov Chain with 9 states. ## The Markov Chain has absorbing states. ## ## Observations: 170028 ## LogLikelihood: -268106 ## AIC: 536233 ## BIC: 536333 We see that there are states and also includes absorbing states. Based on this model we can also make predictions pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;P1&quot;, &quot;P4&quot;, &quot;P6&quot;) ,absorbingProbabilities = data.frame(Buy = 0.22, Defer = 0.78)) resultPattern &lt;- predict(mc_clu1, startPattern = pattern, dist = 1) resultPattern ## Sequence: P2 ## Probability: 0.27 ## Absorbing Probabilities: ## Buy Defer ## 1 0.077 0.92 Hence a person that cluster of clickstreams will have a 7.7% chance of buying something. 5.2.3 A concrete eaxmple with real data The following sections include four different options. Applying the Markov Chain model. Modeling click streams with higher-order. This is basically what we saw in the latent trait model, where we just needed to know the maturity to estimate how likely a person is to have other financial services. Use frequencies of events instead of sequences. Use neural networks, either with sequence data or not. 5.2.3.1 (1) Markov Chain Example with real data (A Danish company provided us with a file of clickstream data on their e-commerce customers). library(clickstream) mydata &lt;- readClickstreams(file = &quot;Data/Customer behavior/StepsDesktop.csv&quot;, sep=&quot;,&quot;,header = T) summary(mydata) ## Observations: 37644 ## ## Click Frequencies: ## AddCartPage CartPage CategoryPage ## 6026 7704 76699 ## ChannelAd ChannelDirectAccess ChannelOther ## 38483 13052 828 ## ChannelPriceComparison ChannelSearchEngine ChannelSocialNetworks ## 5294 19093 521 ## ChannelWebmail Conversion DeliveryPage ## 3686 2095 2901 ## DiscountPage FrontPage LandingPage ## 13887 13932 5976 ## OtherPage ProductDetails ProductPage ## 7918 60901 82347 ## Recommendation SearchFunction ## 2002 7015 We see that there are 37k observations and then we can see the frequencies of the different events and 20 different pages. Now we are going to analyze which order to apply. maxOrder &lt;- 3 #One could also use 5 for instance result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = mydata, order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result Order AIC BIC 1 970365 970592 2 970407 970861 3 970449 971130 We see that order of 1, i.e., looking one step back in time is the most appropriate. mc &lt;- fitMarkovChain(clickstreamList = mydata, order = 1 ,control = list(optimizer = &quot;quadratic&quot;)) mc ## First-Order Markov Chain ## ## Transition Probabilities: ## ## Lag: 1 ## lambda: 1 ## AddCartPage CartPage CategoryPage ChannelAd ## AddCartPage 0.04935 0.00196 0.00028 0.000078 ## CartPage 0.67694 0.16814 0.00510 0.000104 ## CategoryPage 0.13054 0.06072 0.35146 0.321815 ## ChannelAd 0.01095 0.08414 0.07781 0.013339 ## ChannelDirectAccess 0.00707 0.05567 0.02705 0.000130 ## ChannelOther 0.00017 0.01627 0.00154 0.000026 ## ChannelPriceComparison 0.00371 0.01921 0.00436 0.000026 ## ChannelSearchEngine 0.00472 0.01529 0.03382 0.000078 ## ChannelSocialNetworks 0.00017 0.00042 0.00012 0.000000 ## ChannelWebmail 0.00168 0.00154 0.00209 0.000026 ## Conversion 0.00017 0.23713 0.00015 0.000052 ## DeliveryPage 0.00101 0.04403 0.00123 0.001976 ## DiscountPage 0.01011 0.00252 0.00897 0.045814 ## FrontPage 0.00202 0.04866 0.00362 0.069605 ## LandingPage 0.01886 0.01865 0.01256 0.004394 ## OtherPage 0.00269 0.00659 0.00377 0.007020 ## ProductDetails 0.03756 0.00168 0.00221 0.000520 ## ProductPage 0.00539 0.19184 0.44469 0.534841 ## Recommendation 0.00286 0.00028 0.00003 0.000078 ## SearchFunction 0.03402 0.02524 0.01913 0.000078 ## ChannelDirectAccess ChannelOther ChannelPriceComparison ## AddCartPage 0.000000 0.0000 0.00038 ## CartPage 0.007444 0.0012 0.00000 ## CategoryPage 0.133615 0.3043 0.04649 ## ChannelAd 0.003837 0.0060 0.01890 ## ChannelDirectAccess 0.009977 0.0012 0.00038 ## ChannelOther 0.000077 0.0036 0.00000 ## ChannelPriceComparison 0.000153 0.0000 0.00057 ## ChannelSearchEngine 0.000230 0.0000 0.00019 ## ChannelSocialNetworks 0.000000 0.0000 0.00000 ## ChannelWebmail 0.000000 0.0000 0.00000 ## Conversion 0.000614 0.1111 0.00000 ## DeliveryPage 0.013738 0.0024 0.00000 ## DiscountPage 0.023638 0.0326 0.00019 ## FrontPage 0.469839 0.0688 0.00265 ## LandingPage 0.002993 0.0048 0.00038 ## OtherPage 0.110668 0.2512 0.00057 ## ProductDetails 0.000077 0.0000 0.00057 ## ProductPage 0.221489 0.2126 0.92876 ## Recommendation 0.000000 0.0000 0.00000 ## SearchFunction 0.001612 0.0000 0.00000 ## ChannelSearchEngine ChannelSocialNetworks ChannelWebmail ## AddCartPage 0.000000 0.0000 0.00000 ## CartPage 0.000000 0.0000 0.00000 ## CategoryPage 0.367494 0.0154 0.06079 ## ChannelAd 0.008959 0.0115 0.00054 ## ChannelDirectAccess 0.000210 0.0000 0.00000 ## ChannelOther 0.000000 0.0000 0.00000 ## ChannelPriceComparison 0.000000 0.0000 0.00000 ## ChannelSearchEngine 0.002463 0.0000 0.00000 ## ChannelSocialNetworks 0.000000 0.0077 0.00000 ## ChannelWebmail 0.000000 0.0000 0.01113 ## Conversion 0.000000 0.0000 0.00000 ## DeliveryPage 0.029812 0.0787 0.00081 ## DiscountPage 0.012260 0.0134 0.47707 ## FrontPage 0.182228 0.0326 0.00760 ## LandingPage 0.000943 0.0000 0.38697 ## OtherPage 0.054647 0.1631 0.03012 ## ProductDetails 0.000210 0.0000 0.00000 ## ProductPage 0.340616 0.6775 0.02497 ## Recommendation 0.000052 0.0000 0.00000 ## SearchFunction 0.000105 0.0000 0.00000 ## Conversion DeliveryPage DiscountPage FrontPage ## AddCartPage 0 0.00613 0.002394 0.000987 ## CartPage 0 0.06958 0.017353 0.018916 ## CategoryPage 0 0.14048 0.087707 0.423308 ## ChannelAd 0 0.07834 0.088819 0.067440 ## ChannelDirectAccess 0 0.19825 0.025902 0.046056 ## ChannelOther 0 0.01094 0.002223 0.002796 ## ChannelPriceComparison 0 0.01575 0.005642 0.002467 ## ChannelSearchEngine 0 0.04945 0.019918 0.040793 ## ChannelSocialNetworks 0 0.00350 0.000427 0.000822 ## ChannelWebmail 0 0.00481 0.037955 0.001069 ## Conversion 0 0.06783 0.003334 0.001151 ## DeliveryPage 0 0.08796 0.001453 0.040464 ## DiscountPage 0 0.00569 0.099932 0.055679 ## FrontPage 0 0.03545 0.009061 0.019821 ## LandingPage 0 0.02188 0.083091 0.061436 ## OtherPage 0 0.12823 0.003932 0.039395 ## ProductDetails 0 0.01357 0.005471 0.002385 ## ProductPage 0 0.02976 0.478629 0.016120 ## Recommendation 0 0.00088 0.000085 0.000082 ## SearchFunction 0 0.03151 0.026671 0.158812 ## LandingPage OtherPage ProductDetails ProductPage ## AddCartPage 0.00070 0.00102 0.02673 0.05902 ## CartPage 0.00894 0.02368 0.00655 0.01202 ## CategoryPage 0.10252 0.08110 0.16182 0.16968 ## ChannelAd 0.03785 0.06185 0.07025 0.11732 ## ChannelDirectAccess 0.00929 0.12694 0.01985 0.03536 ## ChannelOther 0.00053 0.00869 0.00172 0.00303 ## ChannelPriceComparison 0.00456 0.00733 0.00942 0.02113 ## ChannelSearchEngine 0.00999 0.03817 0.02846 0.04409 ## ChannelSocialNetworks 0.00035 0.00204 0.00072 0.00100 ## ChannelWebmail 0.00508 0.01039 0.00577 0.00541 ## Conversion 0.00123 0.00324 0.00016 0.00063 ## DeliveryPage 0.00158 0.11910 0.00101 0.00186 ## DiscountPage 0.73134 0.00647 0.02325 0.02449 ## FrontPage 0.00491 0.01533 0.00199 0.00324 ## LandingPage 0.02419 0.01891 0.00770 0.01045 ## OtherPage 0.01823 0.23616 0.00439 0.00736 ## ProductDetails 0.00123 0.00239 0.58466 0.39921 ## ProductPage 0.01542 0.17533 0.02837 0.05276 ## Recommendation 0.00000 0.00017 0.00904 0.01284 ## SearchFunction 0.02208 0.06168 0.00812 0.01910 ## Recommendation SearchFunction ## AddCartPage 0.00519 0.00307 ## CartPage 0.00311 0.00767 ## CategoryPage 0.02595 0.23397 ## ChannelAd 0.02543 0.03283 ## ChannelDirectAccess 0.00467 0.01703 ## ChannelOther 0.00052 0.00061 ## ChannelPriceComparison 0.00467 0.00537 ## ChannelSearchEngine 0.00727 0.01734 ## ChannelSocialNetworks 0.00000 0.00046 ## ChannelWebmail 0.00104 0.00107 ## Conversion 0.00000 0.00061 ## DeliveryPage 0.00104 0.00338 ## DiscountPage 0.00623 0.00430 ## FrontPage 0.00156 0.00782 ## LandingPage 0.00311 0.00706 ## OtherPage 0.00156 0.21034 ## ProductDetails 0.02958 0.00997 ## ProductPage 0.57187 0.32755 ## Recommendation 0.29683 0.00046 ## SearchFunction 0.01038 0.10908 ## ## Start Probabilities: ## ## ChannelAd ChannelDirectAccess ChannelOther ## 0.4453 0.1287 0.0040 ## ChannelPriceComparison ChannelSearchEngine ChannelSocialNetworks ## 0.0689 0.2847 0.0094 ## ChannelWebmail ## 0.0589 ## ## End Probabilities: ## ## AddCartPage CartPage CategoryPage ## 0.002364 0.015222 0.246440 ## ChannelAd ChannelDirectAccess ChannelPriceComparison ## 0.000611 0.000584 0.000053 ## ChannelSearchEngine ChannelWebmail Conversion ## 0.000186 0.000027 0.055653 ## DeliveryPage DiscountPage FrontPage ## 0.016364 0.058150 0.047099 ## LandingPage OtherPage ProductDetails ## 0.007172 0.054431 0.148736 ## ProductPage Recommendation SearchFunction ## 0.331713 0.001992 0.013203 Since we have names on the rows and columns, it is a bit more confusion to read. Although the principle is the same as we have seen previously. We have to read it from column to row when interpreting probabilities of going from one page to another. Notice that we are not having any absorbing states. Lets predict what happens one step ahead given the different pages are visited. pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;DeliveryPage&quot;)) resultPattern &lt;- predict(mc, startPattern = pattern, dist = 1) resultPattern # pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;AddCartPage&quot;)) # resultPattern &lt;- predict(mc, startPattern = pattern, dist = 1) # resultPattern # # pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;AddCartPage&quot;, &quot;CartPage&quot;)) # resultPattern &lt;- predict(mc, startPattern = pattern, dist = 1) # resultPattern # # pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;DeliveryPage&quot;)) # resultPattern &lt;- predict(mc, startPattern = pattern, dist = 1) # resultPattern ## Sequence: ChannelDirectAccess ## Probability: 0.2 ## Absorbing Probabilities: ## None ## 1 NaN We see that when you have visited the delivery page, then there is a 20% probability that you will go to the channel direct access, whatever that is. Now we can cluster the different click streams and make predictions based on these. # Clustering first library(doParallel) CoreCount &lt;- makePSOCKcluster(detectCores()-1) registerDoParallel(CoreCount) clusters_ex &lt;- clusterClickstreams(clickstreamList = mydata, order = 1, centers = 3) # takes 5-10 min. to converge stopCluster(CoreCount) registerDoSEQ() summary(clusters_ex$clusters[[1]]) # clu 1 ## Observations: 3088 ## ## Click Frequencies: ## AddCartPage CartPage CategoryPage ## 4537 6399 11620 ## ChannelAd ChannelDirectAccess ChannelOther ## 5249 2453 244 ## ChannelPriceComparison ChannelSearchEngine ChannelSocialNetworks ## 1323 1615 28 ## ChannelWebmail Conversion DeliveryPage ## 224 1870 781 ## DiscountPage FrontPage LandingPage ## 2966 2861 1217 ## OtherPage ProductDetails ProductPage ## 982 12012 16144 ## Recommendation SearchFunction ## 343 1678 We see that the first cluster contain approx. 3.100 observations out of the 37.644 observations. Lets find optimal order based on AIC and BIC maxOrder &lt;- 2 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters_ex$clusters[[1]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result #Lag = 1 is the best. Order AIC BIC 1 236608 236801 2 236650 237037 Now we can do the same for the other clusters. Notice that I have commented out the prints, in general we just see that a lag of 1 is the most approapriate. #summary(clusters_ex$clusters[[2]]) # clu 2 # mc for clu 2 maxOrder &lt;- 3 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters_ex$clusters[[2]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result #summary(clusters_ex$clusters[[3]]) # clu 3 # mc for clu 3 maxOrder &lt;- 3 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters_ex$clusters[[3]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result Result = lag of 1 is the most appropriate. mc_clu1 &lt;- fitMarkovChain(clickstreamList = clusters_ex$clusters[[1]], order = 1) summary(mc_clu1) mc_clu2 &lt;- fitMarkovChain(clickstreamList = clusters_ex$clusters[[2]], order = 1) mc_clu3 &lt;- fitMarkovChain(clickstreamList = clusters_ex$clusters[[3]], order = 1) ## First-Order Markov Chain with 20 states. ## The Markov Chain has absorbing states. ## ## Observations: 74546 ## LogLikelihood: -118283 ## AIC: 236608 ## BIC: 236801 Plot some graphical representations. Although we see that there are so many outcomes making it difficult to interpret. par(mfrow = c(1,1)) #SEE IF THIS WORKS TO COLLECT THE PLOTS. plot(mc_clu1, order = 1) #plot(mc_clu2, order = 1) #plot(mc_clu3, order = 1) The following is another way of representing the same. # model graphical representation (ii) par(mfrow=c(1,3)) par(mar=c(1, 1, 4, 0)) set.seed(11) plot(mc_clu1, order = 1, digits = 1, minProbability = 0.40, vertex.color=0, vertex.frame.color=0, vertex.shape=&quot;none&quot;, vertex.size=8, vertex.size2=3, vertex.label.dist=0.4, vertex.color=&quot;transparent&quot;, vertex.label.font=2, vertex.label.cex=0.95, vertex.label.degree=1, vertex.label.color=&quot;black&quot;, edge.arrow.size=0.2, edge.label.cex = 0.9, edge.curved=0, edge.label.font=4, margin=c(0,0,0,0.15), main =&quot;Cluster 1&quot;) set.seed(11) plot(mc_clu2, order = 1, digits = 1, minProbability = 0.40, vertex.color=0, vertex.frame.color=0, vertex.shape=&quot;none&quot;, vertex.size=8, vertex.size2=3, vertex.label.dist=0.4, vertex.color=&quot;transparent&quot;, vertex.label.font=2, vertex.label.cex=0.95, vertex.label.degree=1, vertex.label.color=&quot;black&quot;, edge.arrow.size=0.2, edge.label.cex = 0.9, edge.curved=0, edge.label.font=4, margin=c(0,0,0,0.15), main =&quot;Cluster 2&quot;) set.seed(11) plot(mc_clu3, order = 1, digits = 1, minProbability = 0.40, vertex.color=0, vertex.frame.color=0, vertex.shape=&quot;none&quot;, vertex.size=8, vertex.size2=3, vertex.label.dist=0.4, vertex.color=&quot;transparent&quot;, vertex.label.font=2, vertex.label.cex=0.95, vertex.label.degree=1, vertex.label.color=&quot;black&quot;, edge.arrow.size=0.2, edge.label.cex = 0.9, edge.curved=0, edge.label.font=4, margin=c(0,0,0,0.15), main =&quot;Cluster 3&quot;) Characterizing the clusters. Who are the customers underlying the three patterns of movement within the website? Notice that this does not apply to her notes cl.2 and cl.3 search a lot through Category page cl.1 search more through Product page cl.1 and cl.3 search more varied (use more channels than cl.2) cl.3 are more likely to enter through DirectAccess -&gt; Front page pattern than the other two clusterss. In this application the product page was generalized. However, if a more fine-grained clicktream data were available by product brand it will allow the manager to take informed decisions about the preferences of each cluster. Now we can predict the next step for each cluster. Notice that the sequence for each prediction is the same. # predicting the next steps pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;AddCartPage&quot;, &quot;CartPage&quot;)) resultPattern &lt;- predict(mc_clu1, startPattern = pattern, dist = 1) resultPattern pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;AddCartPage&quot;, &quot;CartPage&quot;)) resultPattern &lt;- predict(mc_clu2, startPattern = pattern, dist = 1) resultPattern pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;AddCartPage&quot;, &quot;CartPage&quot;)) resultPattern &lt;- predict(mc_clu3, startPattern = pattern, dist = 1) resultPattern ## Sequence: Conversion ## Probability: 0.25 ## Absorbing Probabilities: ## None ## 1 NaN ## ## ## Sequence: ProductPage ## Probability: 0.32 ## Absorbing Probabilities: ## None ## 1 NaN ## ## ## Sequence: ProductPage ## Probability: 0.22 ## Absorbing Probabilities: ## None ## 1 NaN We see the following: Cluster 1 has 25% probability of going to conversion Cluster 2 has 22% probability of going to the product page Cluster 3 has 21% probability of going to the product page 5.2.3.2 (2) Using association rule mining Here we loose the sequence of events, but we use association rules. So like the maturity example we had with Hans, where if you have been at product 2 then you assume he has seen product 1. That might not always be true. Hence we don’t see how many times a customer has visited a product, as it is just looking at whether we visited a product or not. Summary if the method: we are going to apply arules to visualize association rules. Also we are going to calculate minsupport using the apropri algorithm. Notice that we are going to apply three clusters again. library(&quot;arules&quot;) library(&quot;arulesSequences&quot;) # looking by clustered data trans_clu1 &lt;- as.transactions(clusters$clusters[[1]]) sequences_clu1 &lt;- as(apriori(trans_clu1, parameter = list(support = 0.50)), &quot;data.frame&quot;) sequences_clu1 %&gt;% head() # subrules &lt;- subset(sequences_clu1, support&gt;0.05) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.8 0.1 1 none FALSE TRUE 5 0.5 1 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 1 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[9 item(s), 3 transaction(s)] done [0.00s]. ## sorting and recoding items ... [6 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 done [0.00s]. ## writing ... [96 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. rules support confidence coverage lift count {} =&gt; {P1} 1.00 1 1.00 1 3 {} =&gt; {P5} 1.00 1 1.00 1 3 {} =&gt; {P7} 1.00 1 1.00 1 3 {} =&gt; {P8} 1.00 1 1.00 1 3 {Buy} =&gt; {P1} 0.67 1 0.67 1 2 {Buy} =&gt; {P5} 0.67 1 0.67 1 2 We see the different rules in the rows above. The following runs the same for the other clusters. trans_clu2 &lt;- as.transactions(clusters$clusters[[2]]) sequences_clu2 &lt;- as(apriori(trans_clu2, parameter = list(support = 0.50)), &quot;data.frame&quot;) #sequences_clu2 trans_clu3&lt;- as.transactions(clusters$clusters[[3]]) sequences_clu3 &lt;- as(apriori(trans_clu3, parameter = list(support = 0.50)), &quot;data.frame&quot;) #sequences_clu3 The corresponding output shows that pattern sequences are supported by at least 50% of the clickstreams in each cluster. cluster 1 of clickstream is the most heterogeneeous. The most common pattern {ChannelAd} =&gt; {ProductPage} has support in 41% of the clicks in clu1. This patterns reflects that this customers are most likely acquired through re-targeting. Visualizing library(&quot;arulesViz&quot;) sequences_clu1 &lt;- apriori(trans_clu1, parameter = list(support = 0.40)) ruleExplorer(sequences_clu1) # sequences_clu2 &lt;- apriori(trans_clu2, parameter = list(support = 0.50)) # ruleExplorer(sequences_clu2) # sequences_clu3 &lt;- apriori(trans_clu3, parameter = list(support = 0.50)) # ruleExplorer(sequences_clu3) # this approach is also useful for discovering product/brands that are seen together 5.2.3.3 (3) Use frequencies instead of the sequence If we dont want to work with sequences, we just look at frequencies. This has the advantage as many machine learning methods now can be applied, for instance a random forest. Although we do completely miss out on the sequence. frequencyDF &lt;- frequencies(mydata) head(frequencyDF) ChannelAd ProductPage ProductDetails ChannelSearchEngine CategoryPage FrontPage DeliveryPage ChannelDirectAccess SearchFunction LandingPage DiscountPage AddCartPage CartPage OtherPage ChannelWebmail ChannelPriceComparison Recommendation Conversion ChannelSocialNetworks ChannelOther 0001d300-936c-402d-a7e0-daccc63a4965 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0001fe38-9692-44bb-8c16-4382735d0db6 0 0 0 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0002f4e7-57cd-469c-9131-52eefedab5b9 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 00063d31-2726-4ee4-a2b4-66758ef0b5c4 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 000ChannelAd9df-8f20-4a09-bfb1-59c14808b627 0 1 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 000ef85d-cff3-47e4-a109-a078615de11a 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Here we just get a table showing each IDs for visitors and then you just get to see how many times the given person has visited the page. 5.2.3.4 (4) Neural networks This is just using Neural Networks # (6) A fourth option is to use Neural Networks (recurrent or not) with sequences # of clickstream data purely for prediction purposes. 5.2.4 Optional exercise # (6) Optional # Consider another real-life data set from Cadez, I., Heckerman, D., Meek, C., # Smyth, P., White, S. (2003) Model-based clustering and visualization # of navigation patterns on a web site, Data Mining and Knowledge Discovery, 399-424. # The dataset msnbc323 (Melnykov 2016a) is available # in data(&quot;msnbc323&quot;, package = &quot;ClickClust&quot;). # There are 323 clickstream sequences that involve 17 different states: # (1) frontpage, (2) news, # (3) tech, (4) local, # (5) opinion, (6) on-air, # (7) miscellaneous, (8) weather, # (9) msn-news, (10) health-on-air, # (11) living, (12) business, # (13) msn-sports, (14) sports, # (15) summary-news, (16) bbs, # and (17) travel. # The length of sequences varies from 35 to 362. # There are 289 possible transitions among the 17 states. library(ClickClust) data(&quot;msnbc323&quot;, package = &quot;ClickClust&quot;) summary(msnbc323) clusters &lt;- clusterClickstreams(clickstreamList = msnbc323, order = 1, centers = 3) # takes 5-10 min. to converge # mc for clu 1 maxOrder &lt;- 5 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[1]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result #1 # mc for clu 2 maxOrder &lt;- 5 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[2]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result #1 # mc for clu 3 maxOrder &lt;- 5 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[3]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result #1 mc_clu1 &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[1]], order = 1) summary(mc_clu1) mc_clu2 &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[2]], order = 1) summary(mc_clu2) mc_clu3 &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[3]], order = 1) summary(mc_clu3) # graphically par(mfrow=c(1,3)) par(mar=c(1, 1, 4, 0)) set.seed(11) plot(mc_clu1, order = 1, digits = 1, minProbability = 0.40, vertex.color=0, vertex.frame.color=0, vertex.shape=&quot;none&quot;, vertex.size=8, vertex.size2=3, vertex.label.dist=0.4, vertex.color=&quot;transparent&quot;, vertex.label.font=2, vertex.label.cex=0.95, vertex.label.degree=1, vertex.label.color=&quot;black&quot;, edge.arrow.size=0.2, edge.label.cex = 0.9, edge.curved=0, edge.label.font=4, margin=c(0,0,0,0.15), main =&quot;Cluster 1&quot;) set.seed(11) plot(mc_clu2, order = 1, digits = 1, minProbability = 0.40, vertex.color=0, vertex.frame.color=0, vertex.shape=&quot;none&quot;, vertex.size=8, vertex.size2=3, vertex.label.dist=0.4, vertex.color=&quot;transparent&quot;, vertex.label.font=2, vertex.label.cex=0.95, vertex.label.degree=1, vertex.label.color=&quot;black&quot;, edge.arrow.size=0.2, edge.label.cex = 0.9, edge.curved=0, edge.label.font=4, margin=c(0,0,0,0.15), main =&quot;Cluster 2&quot;) set.seed(11) plot(mc_clu3, order = 1, digits = 1, minProbability = 0.40, vertex.color=0, vertex.frame.color=0, vertex.shape=&quot;none&quot;, vertex.size=8, vertex.size2=3, vertex.label.dist=0.4, vertex.color=&quot;transparent&quot;, vertex.label.font=2, vertex.label.cex=0.95, vertex.label.degree=1, vertex.label.color=&quot;black&quot;, edge.arrow.size=0.2, edge.label.cex = 0.9, edge.curved=0, edge.label.font=4, margin=c(0,0,0,0.15), main =&quot;Cluster 3&quot;) # interpret: # Cluster 2 is entirely driven by transitions within the same categories # Thus, this group represents people who make the majority of transitions within their # preferred category and do not change categories frequently. # Cluster 1 The second cluster is characterized by higher probabilities of transitions # front page–news, news–news, msn-news, and summary–news. # The transition front page–news reﬂects the common pattern for the users starting with # the category front page to proceed directly to the category news. # Once the reader gets to the category news, he or she typically stays within it or proceeds # to summary.Thus, the second cluster consists of people mostly concerned with news. # Cluster 3 is characterized by transitions travel-health-on-air, travel-frontpage, # consisting of the people concerned about travelling issues. # The analysis of this dataset illustrates how click-plots can # be used and interpreted to discover interesting navigation patterns common for # observations within detected clusters. 5.3 AB Testing Basically this is experimenting with for instance a website. For instance changing the location of the basket. The ultimate goal is to be able to find out what works on a website, and what does not. The lecture had the following outline: What is A/B testing? When is the organization ready to use A/B test and why should you use A/B test? How to perform an A/B test? Pitfalls and challenges. A/B tests and bandits. 5.3.1 (1 + 2): What is it and when to apply it Basically you just funnel some visitors into one website layout and the other to another. Then one of the groups is a control group and the other, the treatment group. Then you want to assess if the treatment group is actually buying more stuff. To evaluate this, one can apply statistical tests. You want to apply it if you have operate in different countries, as the one culture may prefer one layout over another. Hence you want to collect that information. Hence instead of just making something that looks good, you want to make something that also works. 5.3.2 (3): How to perform an A/B test The is two steps: Agree on what you want to optimize, e.g., basket size, conversion rates, amount of visitors, the time a person is on the website etc. Then you must select an appropriate metric for this. This must be distributed throughout the organization. This is also known as the Overall Evaluation Criterion (OEC). Report many other metrics and diagnostics to support the OEC. Now you must find out: What sample size do you need? The more variance that that a metric contains, the more samples do you also need. How long will you be experimenting. Often it goes over one or two weeks Making statistical test of different means We want to find n avg_rev_pr_user &lt;- 3.75 #I.e. the control change_rev &lt;- 0.05 delta &lt;- change_rev * avg_rev_pr_user a &lt;- 0.05 beta &lt;- 0.2 power &lt;- 1-beta #Statistical power we want to achieve sigma &lt;- 30 z1 &lt;- abs(qnorm(a/2)) #Because it is double sided z2 &lt;- abs(qnorm(power)) 2*((z1 + z2)^2*sigma^2)/(delta^2) ## [1] 401863 We see that we need 401.863 observations. An R example library(pwr) mde &lt;- 0.1 #minimum detectable effect cr_a &lt;- 0.25 #The expected conversion rate for group A (the control group) alpha &lt;- 0.05 #The false positive rate - significance level power &lt;- 0.8 #Statistical power ptpt &lt;- pwr.2p.test(h = ES.h(p1 = cr_a,p2 = (1+mde)*cr_a) ,sig.level = alpha ,power = power) n_obs &lt;- ceiling(ptpt$n) n_obs #Required pairs of observations (1 pair = 1 control + 1 treatment) ## [1] 4860 We see that we need 4.860 observations, that is for both the control and the treatment. 5.3.3 (4): Pitfalls and challenges. Rampup He talks about slowly scaling up the treatment group to find bugs. Then you should start small and monitor the visitors that enter the website. Common problems Looking at too many metrics - the more measures you look at, the greater is the cahnce of observing random fluctuations. One must be aware of robots, we do not want to count these as a visitor. Many managers don’t let the tests run their course Not keeping it simple. Too many changes on the website will make the conclusions vague, because you don’t know what worked and what no. You must be aware of the surroundings, like what dates are your comparing, different timezone? time periods? What browser? What device is used? To deal with the latter, one could make clusters, for instance we only want to make the test on phone or tablet visitors. 5.3.4 (5): A/B tests and bandits. We have to terms that must be clarified first: Exploration: That is where you are actually making the A/B testing, hence collecting all of your data. Running the test may not cost money, although if one landing page is better than the other, then you might be loosing revenue, hence it is costly in that way, or at least an opportunity cost. Exploitation: We see that what we find from the exploration we can start using and that is basically the exploitation. This is also called the bandit approach, as when we collect information we start using it to minimize the opportunity cost. This bandit approach has the following advnatages: Earn while you learn Automation of selection proces, he presented an example, where the algorithm could automatically select which add to show, based on the given information. Introducing randomness: we give the bandit the ability to explore new information, so it does not only end up confirming itself, as one ad would be used 99% of the time, as it previously has been the best. To run the exploitations, we can apply two different algorithms: Epsilon Greedy Policy Thompson Sampling Policy Simulations - example in R In the following we are going simulate the same example with two different types of simulators: Epsilon Greedy Policy Thompson Sampling Policy Epsilon Greedy An epsilon-greedy policy is a randomized policy that mixes exploitation with a predetermined amount of exploration. For any epsilon \\(\\epsilon\\) \\(\\in\\) (in) [0,1] (meaning epsilons between 0 and 1), the policy randomly allocates \\(\\epsilon\\) of the observations allocated uniformly across the K ads, and allocates 1 - \\(\\epsilon\\) of observations to the ad with the largest observed mean. library(contextual) horizon &lt;- 1000 #1000 timesteps simulations &lt;- 1000 #It means that simulator runs 1.000 simulations for each of the 1000 time steps # Epsilon greedy horizon &lt;- 1000 simulations &lt;- 1000 conversionProbabilities &lt;- c(0.05,0.10,0.15,0.20,0.25) bandit &lt;- BasicBernoulliBandit$new(weights = conversionProbabilities) policy &lt;- EpsilonGreedyPolicy$new(epsilon = 0.10) agent &lt;- Agent$new(policy = policy,bandit = bandit) historyEG &lt;- Simulator$new(agents = agent,horizon = horizon,simulations = simulations)$run() plot(historyEG,type = &quot;arms&quot;,legend_labels = c(&quot;Ad 1&quot;,&quot;Ad 2&quot;,&quot;Ad 3&quot;,&quot;Ad 4&quot;,&quot;Ad 5&quot;) ,legend_title = &quot;Epsilon Greedy&quot;,legend_position = &quot;topright&quot;,smooth = TRUE) We see that we feed the simulator information about the conversion probabilities for the given adds. Then we run 1000 simulations, and we see that there is a tendency towards selecting add 5, naturally because this has the highest conversion probabilities. If we for instance was to set add 5 conversion probability to 0.025 instead of 0.25, then we would see that add 4 would start dominate the add choices. summary(historyEG) ## ## Agents: ## ## EpsilonGreedy ## ## Cumulative regret: ## ## agent t sims cum_regret cum_regret_var cum_regret_sd ## EpsilonGreedy 1000 1000 32 592 24 ## ## ## Cumulative reward: ## ## agent t sims cum_reward cum_reward_var cum_reward_sd ## EpsilonGreedy 1000 1000 218 712 27 ## ## ## Cumulative reward rate: ## ## agent t sims cur_reward cur_reward_var cur_reward_sd ## EpsilonGreedy 1000 1000 0.22 0.71 0.027 we see that we had a reward of 21.81%, which is naturally lower than if we just used ad 5, which had a conversion of 25%. The variance is 71% and the standard deviation 2.66% Thompson Sampling Alogorithm Here we see the add to be presented is based upon probabilities. These are found by using beta distributions (recall the CLV lecture, where with alpha and beta we can basically make all kinds of beta distributions). The following makes a Thompson sampling #Thompson scaling conversionProbabilities &lt;- c(0.05,0.10,0.15,0.20,0.25) bandit &lt;- BasicBernoulliBandit$new(weights = conversionProbabilities) policy &lt;- ThompsonSamplingPolicy$new(alpha = 1,beta = 1) agent &lt;- Agent$new(policy,bandit) historyThompson &lt;- Simulator$new(agent,horizon,simulations)$run() plot(historyThompson,type = &quot;arms&quot;,legend_labels = c(&quot;Ad 1&quot;,&quot;Ad 2&quot;,&quot;Ad 3&quot;,&quot;Ad 4&quot;,&quot;Ad 5&quot;) ,legend_title = &quot;Thompson Sampling&quot;,legend_position = &quot;topright&quot;,smooth = TRUE) Here we practically see the same result. Although as he mentions, the Thompson sampling is better at exploiting the high probability adds, thus we see that it uses add 5 more often than the epsilon greedy function. summary(historyThompson) ## ## Agents: ## ## ThompsonSampling ## ## Cumulative regret: ## ## agent t sims cum_regret cum_regret_var cum_regret_sd ## ThompsonSampling 1000 1000 30 159 13 ## ## ## Cumulative reward: ## ## agent t sims cum_reward cum_reward_var cum_reward_sd ## ThompsonSampling 1000 1000 220 328 18 ## ## ## Cumulative reward rate: ## ## agent t sims cur_reward cur_reward_var cur_reward_sd ## ThompsonSampling 1000 1000 0.22 0.33 0.018 We see that the reward (conversion) is 21.99% hence slightly higher, while the variance is 32.83% variance and 1.81% standard deviation. Comparison between Greedy Search and Thompson Sampling We see that the Thompson sampling is having slightly better conversion rates, that is because it quicker picks up on ad 5 which is the most successful. Also we see that the variance and standard deviation is half the size of the Epsilon Greedy. "],["privacy.html", "6 Privacy", " 6 Privacy We have different eras of privacy research, Privacy eras We see that we have 8 different data concerns: Data security: Customers fearing what happens with their data and how might get access to your data, e.g., from hacking. Secretive data collection: That governments and companies are secretaely storing information about you. Junk mail and spam: Some customers fear that entering personal information at webpages leads to all kinds of junk and spam mail. Third-party access: "]]
