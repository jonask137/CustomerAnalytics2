[["index.html", "Customer Analytics setup", " Customer Analytics setup "],["customer-lifetime-value.html", "1 Customer Lifetime Value 1.1 Non-contractual The matrix formulation 1.2 Contractual - Valuing customer base + residual lifetime value 1.3 Non-Contractual Setting - Probabilitic Models", " 1 Customer Lifetime Value Contractual vs. non contractual: Contractual setting: We see that a customer pays a constant amount each month. Notice that one may also buy extra stuff. Non contractual setting: We see that a customer can buy at any given time, e.g., a supermarket. Discrete purchase: That is when a person makes purchase for a given time period, that is for instance subscriptions Continuous purchase: That is a person is able to make a purchase at any given time. In the following we see an example of placing orders either in the beginning or at the end of the period. The difference between case 1 and 2, is that in case 2 it is not a new customer, as we do not have a purchase in time period 0. Discrete purchases See section CLV continuous time accounting for when you get the money in the CLV templates. 1.1 Non-contractual The matrix formulation This is for a non contractual setting. This comes in relation to having different states, where the last state is an absorbing state, which indicates a lost customer. It can be shown with a Markov Chain. Different states We see that there are different probabilities of starting in the different states. Then for that given state you can either move one down or move up to state one. Where state 1 = most recent purchase. Notice that this illustration has four states, state one equal the most recent purchasers while state 5 is the absorbing state. In this state there is 0 probability of moving up to state 1, hence the customer is lost. In the following example we have a scenario of the cost being 4 and a purchase giving a margin og 40 For an infinite period #Facts margin &lt;- 40 costs &lt;- 4 discount_factor &lt;- 1.2 states &lt;- 5 #Transition probabilities p1 &lt;- 0.3 p2 &lt;- 0.2 p3 &lt;- 0.15 p4 &lt;- 0.05 p5 &lt;- 0 #Transition probability matrix P &lt;- matrix(nrow = states,byrow = TRUE ,c(p1,1-p1,0,0,0, p2,0,1-p2,0,0, p3,0,0,1-p3,0, p4,0,0,0,1-p4, p5,0,0,0,1-p5 )# NOTE, ADD STATES BELOW, AND EXTEND THE MATRIX ) # Profit vector (G for profit :) ) G &lt;- matrix(nrow = states ,c(margin - costs, #Purchase only occurs in state 1 -costs, -costs, -costs, 0)) #We spend no money on the last recency state, because we dont mail them # NOTE THE AMOUNT OF LINES MUST = STATES #Identify matrix I &lt;- diag(states) #Customer lifetime value CLV &lt;- solve(I-1/(discount_factor)*P) %*% G CLV ## [,1] ## [1,] 52.319609 ## [2,] 5.553784 ## [3,] 1.250773 ## [4,] -1.820016 ## [5,] 0.000000 We see that for customers starting in state 1, we have a profit of 52, if the customer is starting in state 2, then the profit is 5.6, if starting in state three then 1.25 and fourth they are not positive. Notice that the approach above show the infinite lifetime of the customers. We may be interested in finding out what the lifetime of a customer is, given a certain period. That we look into in the following Infinite modified CLV It is modified as the infinite solution, does not tell what they earn after a certain period, the following does, i guess that is why it is called modified We can also calculate the infinite modified solution, where we apply the transition probabilities and assess how the lifetime value of a customer starting in a specific state is. Notice, we are already having the customer. #Note, it is prerequisite to run the code above first # Infinite modified solution * # Transition probabilities p1=0.3 p2=0.2 p3=0.15 p4=0 #Notice, that we discovered above that recency state 4 where negative, so we will not invest in these. Then we also assume, that they will not buy anything. One may also have kept p = 0.05 p5=0 # Transition probability matrix Pmod=matrix(nrow=states,byrow=T, c(p1,1-p1,0,0,0, p2,0,1-p2,0,0, p3,0,0,1-p3,0, p4,0,0,0,1-p4, p5,0,0,0,1-p5 ) ) Gmod=matrix(c(margin-costs, #G for profit :) -costs, -costs, 0, #Notice, that we remove the cost, that is also why we know that p4 = 0 0) ,nrow=states) Gmod ## [,1] ## [1,] 36 ## [2,] -4 ## [3,] -4 ## [4,] 0 ## [5,] 0 # Customer lifetime value CLV = solve(I-1/(discount_factor)*Pmod) %*% Gmod CLV ## [,1] ## [1,] 53.149425 ## [2,] 6.620690 ## [3,] 2.643678 ## [4,] 0.000000 ## [5,] 0.000000 # Now we see that we do not loose money on cohort 4 Finite period Notice that the following is just for a finite period, where the calculations above are infinite. We see that the result is not really different, just as we saw in the example in excel. # 4 periods ** p2 &lt;- P %*% P #P = the first period p3 &lt;- p2 %*% P #By matrix multiplication can we find the proportions in consecutive periods p4 &lt;- p3 %*% P P ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.30 0.7 0.0 0.00 0.00 ## [2,] 0.20 0.0 0.8 0.00 0.00 ## [3,] 0.15 0.0 0.0 0.85 0.00 ## [4,] 0.05 0.0 0.0 0.00 0.95 ## [5,] 0.00 0.0 0.0 0.00 1.00 p4 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.139700 0.136500 0.1288 0.1428 0.4522 ## [2,] 0.076800 0.081200 0.1008 0.0952 0.6460 ## [3,] 0.039025 0.033075 0.0490 0.0714 0.8075 ## [4,] 0.009750 0.008050 0.0084 0.0238 0.9500 ## [5,] 0.000000 0.000000 0.0000 0.0000 1.0000 What do we see from this. We see that in period 1 recency 1, we have 30% stayers and 70% that moves to recency 2, notice that the stayers, will be those that actually buy something. Then by matrix multiplication we can see that 13.97% of those originally in state 1 is still in state 1. On the other hand can we see that 45% of the customers are lost. That is because in this scenario, there is always a greater chance of loosing the customer rather than them staying. Thus if we had more than 50% of staying, then naturally we would have more people staying in state one (repetitive buys) compared to completely lost customers (those in the absorbing state.) In the following we can replicate the lifetime value through matrix multiplication as well. res1 &lt;- G + 1/discount_factor * P %*% G + 1/discount_factor^2 * p2 %*% G + 1/discount_factor^3 * p3 %*% G + 1/discount_factor^4 * p4 %*% G res1 ## [,1] ## [1,] 50.1149691 ## [2,] 4.2199074 ## [3,] 0.5921103 ## [4,] -1.9801312 ## [5,] 0.0000000 1.2 Contractual - Valuing customer base + residual lifetime value We see that the customer base can be estimated with the sBG approach. In this scenario we have different cohorts, for instance all those that that signed up in a given period. Hence we are working in a contratual setting. We see that we are able to measure how many customers that are actually left in the different cohorts. Customer base Then we can also show the retention rates for the given cohorts. Customer base retention rates We see that the retention rate is actually increasing. Two explanations could be: Getting more loyal customers Filtering out in the heterogeneous customer base - this also implies that we assume heterogeneity Then you may ask, how do we get the retention rates and hence the survival rates? This is estimated with the sBG (Shifted-Beta-Geometric model), with a beta distribution you are able to reflect many different shapes, hence we can also replicate the retention rates. Examples of different outcomes can be seen in the following picture. Why is it called ‘shifted’? That is because a customer cannot leave in time period 0, as they have paid for that period. Shifted-Beta-Geometric model (sBG) Application of this is made in the excel sheet. The following also simulate two different scenarios. alpha1 &lt;- 0.676 beta1 &lt;- 3.86 alpha2 &lt;- 0.7009 beta2 &lt;- 1.1712 theta &lt;- (1:99)/100 p.beta_high = dbeta(theta,alpha1,beta1) p.beta.reg = dbeta(theta,alpha2,beta2) matplot(theta,cbind(p.beta_high,p.beta.reg),xlab = &quot;Churn probability&quot; ,ylab = &quot;Density&quot;,type = &quot;l&quot;,col = c(&quot;blue&quot;,&quot;red&quot;) ,main = &quot;Composition of customers with respect to churn&quot;) legend(0.7,7,legend = c(&quot;High_end&quot;,&quot;Regular&quot;),pch = c(16,1) ,col = c(&quot;blue&quot;,&quot;red&quot;),cex = 0.7) We can also calculate the amount of people that are expected to churn in period one, for this we just need the alpha and the beta values (or at least the estimated values). That is done with \\(P = \\frac{\\alpha}{\\alpha + \\beta}\\) options(digits = 3) alpha1/(alpha1 + beta1) #0.149 alpha2/(alpha2 + beta2) #0.374 ## [1] 0.149 ## [1] 0.374 Hence we see that 37% of the regular customers will churn, while only 15% of the high end customers will churn (leave). That is as expected as we see the greatest density of the of regular churners is higher than the high end, where we see that most lie in the lower region. 1.3 Non-Contractual Setting - Probabilitic Models library(BTYD) #Buy till you die. Now we are going to move into non-contractual models estimate the lifetime value of such a customer base, we are going to apply probabilistic models to estimate the churn rate but also the rate of purchase (transaction rate). We are working with a non-contratual setting. Also these models are specifically applicable when the company is not able to point out the exact time, that the customer will drop out, so the opposite of when we have a contratual setting, where we know exactly if the customer does not renew his contract. I have put notes to to BG/NBD, hence one may go directly to that section to see some details. It is put as a note to describe the other approaches. Summary of the models: We see the following summary: Summary of probability models Individual level = drop out of a given customer Heterogeneity = difference in customers, hence difference in when customers tend to drop out, e.g., some drops out quickly while others will stay long. Counting = about estimating no. of transactions Timing = Estimating when a customer will leave in the future What defines the model? We see that the individual level and heterogeneity are described by different distributions. These combinations make up the model, e.g., Poisson and Gamma distrbution combination to estimate respectively Individual level and Heterogeneity, is called the NBD (negative binomial distribution). And the combination of Exponential and Gamma distribution is called the Pareto distribution. Hence to estimate the Pareto distribution, you naturally need the parameters estimating both the Exponential and Gamma distribution. Notice that we are not going to look at the BB distribution!! We are going to apply integrated models. What is integrated models?: These models apply the the different combinations seen in the first table. Hence we want a distribution both for estimating no. of transactions but also a model to describe when a customer will leave and that is why we both use NBD with BG and Pareto. We see that the integrated models yields two distributions, one for the dropout (Pareto or BG) and one for the transactions (NBD). We see that from the table above: If we are in a continuous setting and want to count no. of transactions, then we go with NBD. This is reflected by the poisson distribution and the heterogenirty of the customers can be estimated by a gamma distribution. If we have a discrete time period (hence not continous purchase = contractual), we can estimate the transaction rate with a shifted geometirc model and the heterogenity (dropout rate) can be estimated with a Beta distribution. This is applied with the sBG model. If we have a timing model, this is basically where we do not know when a customer makes a purchase and they can dropout at any given time. We assume that any customer is having lifetime, and this can be estimated with an exponential distribution. We also assume heterogeneity in the customers, i.e., different tendencies to drop out, this is estimated with a beta distribution. We use the Pareto model for this approach. 1.3.1 Pareto/NBD vs. BG/NBD The difference between these is the timing, meaning when a customer will leave. In the Pareto estimate of timing, a customer can leave at any point in time, where the BG model only allows to leave after a purchase. That makes the Pareto model more flexible than the BG model. Lets take an example of the BG/NBD: BG/NBD Example We see in section (a), that the higher the death rate the fewer purchases do we expect a customer to make, and the higher the purchase rate, the more transactions do we estimate him to have. Although, the probability of customer being alive is is actually decreasing when we increase purchase rate while holding death rate constant. That is due to how the model is build, as the BG model only allows for a customer dying after a purchase. Therefore when the purchase rate is higher, then he will also make more purchases, but for each purchase there is also a probability of dying. Therefore we also see in the last table, we see that we observe a customer in 10 periods, 1 being the first and 10 the last. We see that the more recent the customer has placed a purchase, the greater is the expected number of purchases in a 24 period. Hence implying that the longer time since the last purchase, the greater is the probability it is just because the customer is lost. In the CDNow case, he compared the two models, and we see that they are very similar. Pareto/NBD vs. BG/NBD We see that they are very similar in the way they replicate the actual data. 1.3.2 Pareto/NBD NBD = Negative Binomial Distribution Here we have a model, that aim to predict (or estimate) the transaction rate and the dropout rate. Also the model allows for heterogeneity in the customers, meaning that all are not the same and so will the dropout not be. We are working with 4 parameters, two for estimating the transaction rate and two for estimating the dropout rate. Notice that the model is similar to the e.g., the BG/NBD, that is because this is the foundation hereof. We see that the Pareto/NBD is from 1959 and the BG/NBD is from 2005, hence it builds on the Pareto/NBD. The reason for making the BG/NDB is to make it less cumbersome to estimate, although with the computing power now, we see that the Pareto/NBD model can easily be estimated. Therefore the Pareto/NBD is often preferred over the BG/NBD. 1.3.2.1 Data preparation We only need three pieces of information for every person: How many transactions the customer in the training period - Frequency Denoted as \\(x\\) The time of their last transaction - Recency Denoted as \\(t.x\\) The total time for which they were observed Denoted as \\(T.cal\\). For time of the calibration period. Thus we must construct a data frame with a row for each customer and the information above. We use dc.ReadLines, this is basically the same as read.csv. Notice that elog is a file with a lot of transactions. Thus we have customer identifier, date and amount. cdnowElog &lt;- system.file(&quot;data/cdnowElog.csv&quot;, package = &quot;BTYD&quot;) elog &lt;- dc.ReadLines(cdnowElog, cust.idx = 2 ,date.idx = 3, sales.idx = 5) elog[1:3,] cust date sales 1 19970101 29.3 1 19970118 29.7 1 19970802 15.0 Notice the name ‘elog’, that is for event log. Hence it logs all of the events. We want to transform the date to a date type instead. elog$date &lt;- as.Date(elog$date, &quot;%Y%m%d&quot;); elog[1:3,] cust date sales 1 1997-01-01 29.3 1 1997-01-18 29.7 1 1997-08-02 15.0 The following is able to merge transactions in the same customer. Before running the code we have 6919 observations (events). elog &lt;- dc.MergeTransactionsOnSameDate(elog); After merging we have 6696 events. Thus it appears that some customers made several purchases on the same day. Now we can split the data to get a train (calibration) and test period. end.of.cal.period &lt;- as.Date(&quot;1997-09-30&quot;) #Insert the last date in the train period. elog.cal &lt;- elog[which(elog$date &lt;= end.of.cal.period), ] Notice, the BG/NBD model deals with repeat transactions, hence the first transaction is ignored. split.data &lt;- dc.SplitUpElogForRepeatTrans(elog.cal); clean.elog &lt;- split.data$repeat.trans.elog; Now we are going to create a customer-by-time matrix. That is simply a matrix with each customer on each row and then days as columns. The following is just a snip. #Customer-by-time: Repeatitive transactions freq.cbt &lt;- dc.CreateFreqCBT(clean.elog); freq.cbt[1:3,1:5] ## date ## cust 1997-01-08 1997-01-09 1997-01-10 1997-01-11 1997-01-12 ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## 6 0 0 0 1 0 Since we initially deleted all first transactions, the frequence customer-by-time matrix does not have any of the customer who made zero repeat transactions. These customers are still important hence they will be included in the matrix with the following. #Customer-by-time: All first transactions transactions tot.cbt &lt;- dc.CreateFreqCBT(elog) #Merging both customer-by-time matrices. cal.cbt &lt;- dc.MergeCustomers(tot.cbt, freq.cbt) birth.periods &lt;- split.data$cust.data$birth.per last.dates &lt;- split.data$cust.data$last.date cal.cbs.dates &lt;- data.frame(birth.periods, last.dates ,end.of.cal.period) cal.cbs &lt;- dc.BuildCBSFromCBTAndDates(cal.cbt ,cal.cbs.dates ,per=&quot;week&quot;) # we want to see weekly data Notes: 1. cbs = customer-by-sufficient-statistics 2. We use per = \"week\" to get data shown per week, as ultimately it show the same as pr. day and returns a more simple data frame. In general the procedure above can be executed in the following command: dc.ElogToCbsCbt(). But they have chosen to show the process, so one is able to make iterations to the data if that is needed in other applications. You’ll be glad to hear that, for the process described above, the package contains a single function to do everything for you: dc.ElogToCbsCbt. 1.3.2.2 Parameter Estimation First we must estimate the parameters. Notice that the function takes a starting point with the following values for the parameters (1,1,1,1). params &lt;- pnbd.EstimateParameters(cal.cbs); params ## [1] 0.553 10.580 0.606 11.656 LL &lt;- pnbd.cbs.LL(params, cal.cbs); LL ## [1] -9595 The log likelihood estimates the fit to the data, thus we have a log-likelihood of -9494.98. We are not able to deduct much from the number itself. But we are able to compare it with other models. Basically, we want to select the model with a log-likelihood that is as close to 0 as possible. In the following we run consecutive estimations with its own output as its starting point. p.matrix &lt;- c(params, LL); for (i in 1:2){ params &lt;- pnbd.EstimateParameters(cal.cbs, params); LL &lt;- pnbd.cbs.LL(params, cal.cbs); p.matrix.row &lt;- c(params, LL); p.matrix &lt;- rbind(p.matrix, p.matrix.row); } colnames(p.matrix) &lt;- c(&quot;r&quot;, &quot;alpha&quot;, &quot;s&quot;, &quot;beta&quot;, &quot;LL&quot;); rownames(p.matrix) &lt;- 1:3; p.matrix ## r alpha s beta LL ## 1 0.553 10.6 0.606 11.7 -9595 ## 2 0.553 10.6 0.606 11.7 -9595 ## 3 0.553 10.6 0.606 11.7 -9595 We see that the log-likelihood, hence the fit, is the same, but some of the parameters are changing slightly. We have the following parameters: Describing the the gamma mixing distribution of the NBD transaction process r Alpha See figure 1.1 Describing the gamme mixing distrbibution of the Pareto (or gamma exponentiated) dropout process s beta See figure 1.2 pnbd.PlotTransactionRateHeterogeneity(params) Figure 1.1: Transaction rate heterogeneity of estimated parameters Few have a very high transaction rate, where the majority lies in the south eastern region. pnbd.PlotDropoutRateHeterogeneity(params) Figure 1.2: Dropout rate heterogeneity of estimated parameters Few have a very high dropout rate, where the majority lies in the south eastern region. 1.3.2.3 Individual Level Estimations Now we can also make some estimations on the individual level. We can do: Estimating purchases in the calibration period and hold out period. Calculating probability that the customer is still alive at the end of the calibration period. First we can estimate the number of transactions we expect a newly acquired customer to make in a given time period. pnbd.Expectation(params ,t=52 #We have weekly data ) ## [1] 1.47 We expect a newly acquired customer to make 1.47 repeat purchases in a time period of one year. Calling information from an individual customer. cal.cbs[&quot;1516&quot;,] ## x t.x T.cal ## 26.0 30.9 31.0 Now we can store the information, to be used for estimating purchases in the holdout period. x &lt;- cal.cbs[&quot;1516&quot;, &quot;x&quot;] t.x &lt;- cal.cbs[&quot;1516&quot;, &quot;t.x&quot;] T.cal &lt;- cal.cbs[&quot;1516&quot;, &quot;T.cal&quot;] pnbd.ConditionalExpectedTransactions(params ,T.star = 52 ,x ,t.x ,T.cal) ## [1] 25.5 We expect 25 purchases in the hold out period. Second we can calculate the probability that the customer is still alive in the end of the hold out period. pnbd.PAlive(params, x, t.x, T.cal) ## [1] 0.998 We see that there is a probability of 99%, hence it is almost certain. Conditional expectation function We see the increasing frequency paradox in action: (What ever this is) for (i in seq(10, 25, 5)){ cond.expectation &lt;- pnbd.ConditionalExpectedTransactions( params, T.star = 52, x = i, t.x = 20, T.cal = 39) cat (&quot;x:&quot;,i,&quot;\\t Expectation:&quot;,cond.expectation, fill = TRUE) } ## x: 10 Expectation: 0.706 ## x: 15 Expectation: 0.144 ## x: 20 Expectation: 0.0225 ## x: 25 Expectation: 0.00309 1.3.2.4 Plotting / Goodness-of-fit In the training period We want to assess how the model fits the actual transactions pnbd.PlotFrequencyInCalibration(params, cal.cbs ,censor = 7 #The frequency groups ) ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 freq.6 freq.7+ ## n.x.actual 1411 439 214 100 62.0 38.0 29.0 64.0 ## n.x.expected 1434 397 194 112 70.6 46.4 31.5 70.8 Notice that all bins reflect the frequency and then the y show the amount of customers in the repeat transactions group, notice that group 0 only made one purchase We see that the model estimates the actual transactions very well. The hold out period elog &lt;- dc.SplitUpElogForRepeatTrans(elog)$repeat.trans.elog; x.star &lt;- rep(0, nrow(cal.cbs)); cal.cbs &lt;- cbind(cal.cbs, x.star); elog.custs &lt;- elog$cust; for (i in 1:nrow(cal.cbs)){ current.cust &lt;- rownames(cal.cbs)[i] tot.cust.trans &lt;- length(which(elog.custs == current.cust)) cal.trans &lt;- cal.cbs[i, &quot;x&quot;] cal.cbs[i, &quot;x.star&quot;] &lt;- tot.cust.trans - cal.trans } cal.cbs[1:3,] ## x t.x T.cal x.star ## 1 2 30.43 38.9 1 ## 2 1 1.71 38.9 0 ## 3 0 0.00 38.9 0 Now we can plot the expected frequency. T.star &lt;- 39 # length of the holdout period censor &lt;- 7 # This censor serves the same purpose described above x.star &lt;- cal.cbs[,&quot;x.star&quot;] comp &lt;- pnbd.PlotFreqVsConditionalExpectedFrequency(params, T.star, cal.cbs, x.star, censor) We see that the hold out period is estimated very well in this example. Instead of looking at the plot, we can also show a matrix containing the numbers. rownames(comp) &lt;- c(&quot;act&quot;, &quot;exp&quot;, &quot;bin&quot;) comp ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 freq.6 freq.7+ ## act 0.237 0.697 1.39 1.56 2.53 2.95 3.86 6.36 ## exp 0.138 0.600 1.20 1.71 2.40 2.91 3.82 6.40 ## bin 1411.000 439.000 214.00 100.00 62.00 38.00 29.00 64.00 Now we want to look at weekly transactions. tot.cbt &lt;- dc.CreateFreqCBT(elog) d.track.data &lt;- rep(0, 7 * 78) origin &lt;- as.Date(&quot;1997-01-01&quot;) for (i in colnames(tot.cbt)){ date.index &lt;- difftime(as.Date(i), origin) + 1; d.track.data[date.index] &lt;- sum(tot.cbt[,i]); } w.track.data &lt;- rep(0, 78) for (j in 1:78){ w.track.data[j] &lt;- sum(d.track.data[(j*7-6):(j*7)]) } T.cal &lt;- cal.cbs[,&quot;T.cal&quot;] T.tot &lt;- 78 n.periods.final &lt;- 78 inc.tracking &lt;- pnbd.PlotTrackingInc(params, T.cal, T.tot, w.track.data, n.periods.final) We see that the model generalizes pretty well the period. Instead of looking at the plot we can also show the underlying numbers. inc.tracking[,20:25] ## [,1] [,2] [,3] [,4] [,5] [,6] ## actual 73.0 55.0 70.0 33 56.0 99.0 ## expected 78.3 76.4 74.6 73 71.4 69.9 We can also show a cummulated sum over the weeks, notice that the hold period and the training period is separated by the dashed line. cum.tracking.data &lt;- cumsum(w.track.data) cum.tracking &lt;- pnbd.PlotTrackingCum(params ,T.cal ,T.tot ,cum.tracking.data ,n.periods.final) cum.tracking[,20:25] ## [,1] [,2] [,3] [,4] [,5] [,6] ## actual 1359 1414 1484 1517 1573 1672 ## expected 1309 1385 1460 1533 1604 1674 1.3.3 BG/NBD Used in a non-contractual situation in which customers can make a purchase at any time. The model describes the rate at which customers make purchases and the rate at which they drop out. This is done with four parameters and also allowing for heterogeneity. The following is the BG/NBD model on the CDNow data. Note: calibration = training 1.3.3.1 Data Preparation We only need three pieces of information for every person: How many transactions the customer in the training period - Frequency Denoted as \\(x\\) The time of their last transaction - Recency Denoted as \\(t.x\\) The total time for which they were observed Denoted as \\(T.cal\\). For time of the calibration period. Thus we must construct a data frame with a row for each customer and the information above. We use dc.ReadLines, this is basically the same as read.csv. Notice that elog is a file with a lot of transactions. Thus we have customer identifier, date and amount. cdnowElog &lt;- system.file(&quot;data/cdnowElog.csv&quot;, package = &quot;BTYD&quot;) elog &lt;- dc.ReadLines(cdnowElog, cust.idx = 2 ,date.idx = 3, sales.idx = 5) elog[1:3,] cust date sales 1 19970101 29.3 1 19970118 29.7 1 19970802 15.0 Notice the name ‘elog’, that is for event log. Hence it logs all of the events. We want to transform the date to a date type instead. elog$date &lt;- as.Date(elog$date, &quot;%Y%m%d&quot;); elog[1:3,] cust date sales 1 1997-01-01 29.3 1 1997-01-18 29.7 1 1997-08-02 15.0 The following is able to merge transactions in the same customer. Before running the code we have 6919 observations (events). elog &lt;- dc.MergeTransactionsOnSameDate(elog); After merging we have 6696 events. Thus it appears that some customers made several purchases on the same day. Now we can split the data to get a train (calibration) and test period. end.of.cal.period &lt;- as.Date(&quot;1997-09-30&quot;) #Insert the last date in the train period. elog.cal &lt;- elog[which(elog$date &lt;= end.of.cal.period), ] Notice, the BG/NBD model deals with repeat transactions, hence the first transaction is ignored. split.data &lt;- dc.SplitUpElogForRepeatTrans(elog.cal); clean.elog &lt;- split.data$repeat.trans.elog; Now we are going to create a customer-by-time matrix. That is simply a matrix with each customer on each row and then days as columns. The following is just a snip. #Customer-by-time: Repeatitive transactions freq.cbt &lt;- dc.CreateFreqCBT(clean.elog); freq.cbt[1:3,1:5] ## date ## cust 1997-01-08 1997-01-09 1997-01-10 1997-01-11 1997-01-12 ## 1 0 0 0 0 0 ## 2 0 0 0 0 0 ## 6 0 0 0 1 0 Since we initially deleted all first transactions, the frequence customer-by-time matrix does not have any of the customer who made zero repeat transactions. These customers are still important hence they will be included in the matrix with the following. #Customer-by-time: All first transactions transactions tot.cbt &lt;- dc.CreateFreqCBT(elog) #Merging both customer-by-time matrices. cal.cbt &lt;- dc.MergeCustomers(tot.cbt, freq.cbt) birth.periods &lt;- split.data$cust.data$birth.per last.dates &lt;- split.data$cust.data$last.date cal.cbs.dates &lt;- data.frame(birth.periods, last.dates ,end.of.cal.period) cal.cbs &lt;- dc.BuildCBSFromCBTAndDates(cal.cbt ,cal.cbs.dates ,per=&quot;week&quot;) # we want to see weekly data Notes: 1. cbs = customer-by-sufficient-statistics 2. We use per = \"week\" to get data shown per week, as ultimately it show the same as pr. day and returns a more simple data frame. In general the procedure above can be executed in the following command: dc.ElogToCbsCbt(). But they have chosen to show the process, so one is able to make iterations to the data if that is needed in other applications. 1.3.3.2 Parameter Estimation We use the following formula to estiamte the parameters. They take starting point in par.start = c(1, 3, 1, 3), and iterate from there. params &lt;- bgnbd.EstimateParameters(cal.cbs); params ## p1 p2 p3 p4 ## 0.243 4.414 0.793 2.426 #Log-Likelyhood of parameters LL &lt;- bgnbd.cbs.LL(params, cal.cbs); LL ## [1] -9582 p.matrix &lt;- c(params, LL); for (i in 1:2){ params &lt;- bgnbd.EstimateParameters(cal.cbs, params); LL &lt;- bgnbd.cbs.LL(params, cal.cbs); p.matrix.row &lt;- c(params, LL); p.matrix &lt;- rbind(p.matrix, p.matrix.row); } colnames(p.matrix) &lt;- c(&quot;r&quot;, &quot;alpha&quot;, &quot;a&quot;, &quot;b&quot;, &quot;LL&quot;); rownames(p.matrix) &lt;- 1:3; p.matrix ## r alpha a b LL ## 1 0.243 4.41 0.793 2.43 -9582 ## 2 0.243 4.41 0.793 2.43 -9582 ## 3 0.243 4.41 0.793 2.43 -9582 We have the following parameters: r = referring to the transaction - describes the gamma mixing distribution of the NBD transaction process. See figure 1.3 alpha = refers to the transaction - describes the gamma mixing distribution of the NBD transaction process. See figure 1.3 a = alpha that manipulates the beta distribution - explains dropout rate. See figure 1.4 b = beta, that manipulates the beta distribution - explains dropout rate. See figure 1.4 They mention that it is good to try different starting points, i guess that could merely be incorporated into the loop. bgnbd.PlotTransactionRateHeterogeneity(params) Figure 1.3: Transaction rate heterogeneity of estimated parameters bgnbd.PlotDropoutRateHeterogeneity(params) Figure 1.4: Dropout probability heterogeneity of estimated parameters 1.3.3.3 Individual Level Estimation This is about predicting on individual customer level. First we can estimate the number of transactions we expect a newly acquired customer to make in a give time period. bgnbd.Expectation(params ,t=52) # = 1 year, has we have 52 weeks ## p3 ## 1.44 We see that he is expected to make 1.44 purchases. The follwing show customer 1516. cal.cbs[&quot;1516&quot;,] ## x t.x T.cal ## 26.0 30.9 31.0 We see that he made 26 repeat transactions in the calibration period. Period 30.8 was the most recent period of a purchase, indicating that he bought towards the end of period 30. Now we can save the attributes in objects. x &lt;- cal.cbs[&quot;1516&quot;, &quot;x&quot;] t.x &lt;- cal.cbs[&quot;1516&quot;, &quot;t.x&quot;] T.cal &lt;- cal.cbs[&quot;1516&quot;, &quot;T.cal&quot;] Finally we can calculate the expected number of purchases in the holdout period. bgnbd.ConditionalExpectedTransactions(params ,T.star = 52 #Prediction period, hence 52 weeks. ,x = x #No. of repeat trans. in cal. period. ,t.x = t.x #Most recent period with purchase ,T.cal = T.cal) #Length of cal. period. ## p3 ## 25.8 We see that the customer is expected to make 25.76 purchases in the hold-out-period. bgnbd.PAlive(params = params ,x = x #No. of repeat transactions in the cal period ,t.x = t.x #Time of most recent repeat transaction ,T.cal = T.cal) #Length of calibration period ## p3 ## 0.969 We see that there is a probability of 96.8% that he will be alive after the end of the calibration period. That also seems intuitively high, as customer with the most recent transaction was in 38.4, hence this customer must also be close to the end of the calibration period. for (i in seq(10, 25, 5)){ cond.expectation &lt;- bgnbd.ConditionalExpectedTransactions( params, T.star = 52, x = i, t.x = 20, T.cal = 39) cat (&quot;x:&quot;,i,&quot;\\t Expectation:&quot;,cond.expectation, fill = TRUE) } ## x: 10 Expectation: 0.347 ## x: 15 Expectation: 0.0428 ## x: 20 Expectation: 0.00416 ## x: 25 Expectation: 0.000358 I am not entirely sure what is meant by the ‘increasing frequency paradox’ as of the conditional expectation 1.3.3.4 Plotting / Goodness-of-fit First of all we can plot the actual vs. predicted in the calibration period. bgnbd.PlotFrequencyInCalibration(params = params ,cal.cbs = cal.cbs ,censor = 7 #All 7+ are binned. ) ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 freq.6 freq.7+ ## n.x.actual 1411 439 214 100 62.0 38.0 29.0 64.0 ## n.x.expected 1408 460 192 101 59.8 38.1 25.5 71.2 We see that the model replicates the variance that we observe in the data very well. What we see is amount of customers on y and amount of transactinos in training period. Now we construct the hold/out period. elog &lt;- dc.SplitUpElogForRepeatTrans(elog)$repeat.trans.elog; x.star &lt;- rep(0, nrow(cal.cbs)); cal.cbs &lt;- cbind(cal.cbs, x.star); elog.custs &lt;- elog$cust; for (i in 1:nrow(cal.cbs)){ current.cust &lt;- rownames(cal.cbs)[i] tot.cust.trans &lt;- length(which(elog.custs == current.cust)) cal.trans &lt;- cal.cbs[i, &quot;x&quot;] cal.cbs[i, &quot;x.star&quot;] &lt;- tot.cust.trans - cal.trans } cal.cbs[1:3,] ## x t.x T.cal x.star ## 1 2 30.43 38.9 1 ## 2 1 1.71 38.9 0 ## 3 0 0.00 38.9 0 We fill in the characteristics to the model. And we can plot T.star &lt;- 39 # length of the holdout period censor &lt;- 7 # This censor serves the same purpose described above x.star &lt;- cal.cbs[,&quot;x.star&quot;] comp &lt;- bgnbd.PlotFreqVsConditionalExpectedFrequency(params ,T.star ,cal.cbs ,x.star ,censor) Figure 1.5: Actual vs. conditional expected transactions in the holdout period. rownames(comp) &lt;- c(&quot;act&quot;, &quot;exp&quot;, &quot;bin&quot;) comp ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 freq.6 freq.7+ ## act 0.237 0.697 1.39 1.56 2.53 2.95 3.86 6.36 ## exp 0.225 0.523 1.04 1.52 2.16 2.65 3.50 6.16 ## bin 1411.000 439.000 214.00 100.00 62.00 38.00 29.00 64.00 Now we can assess how well the our model predict how many transactions will occur in each week. First, we need to convert the total transactions pr. day into a customer-by-time matrix (cbt). And then we can transform them into weekly observations instead of daily. tot.cbt &lt;- dc.CreateFreqCBT(elog) d.track.data &lt;- rep(0,7 * 78) #Corresponding to the amount of daily transactions in the holdout origin &lt;- as.Date(&quot;1997-01-01&quot;) for (i in colnames(tot.cbt)){ date.index &lt;- difftime(as.Date(i), origin) + 1; d.track.data[date.index] &lt;- sum(tot.cbt[,i]); } w.track.data &lt;- rep(0, 78) for (j in 1:78){ w.track.data[j] &lt;- sum(d.track.data[(j*7-6):(j*7)]) } Now we can plot and compare the number of transactions with the predicted (expected). T.cal &lt;- cal.cbs[,&quot;T.cal&quot;] T.tot &lt;- 78 #78 because we have 78 weeks in total, if daily then 7*78=546 n.periods.final &lt;- 78 #Notice, often this will just be the same, but does not have to be inc.tracking &lt;- bgnbd.PlotTrackingInc(params, T.cal ,T.tot, w.track.data ,n.periods.final) Figure 1.6: Actual vs. expected incremental purchasing behaviour. inc.tracking[,20:25] ## [,1] [,2] [,3] [,4] [,5] [,6] ## actual 73.0 55.0 70 33.0 56.0 99.0 ## expected 76.9 74.9 73 71.3 69.7 68.2 We can now plot the following with the actual vs. the expected (predicted). cum.tracking.data &lt;- cumsum(w.track.data) cum.tracking &lt;- bgnbd.PlotTrackingCum(params, T.cal ,T.tot, cum.tracking.data ,n.periods.final) Figure 1.7: Actual vs. expected cumulative purchasing behaviour. cum.tracking[,20:25] ## [,1] [,2] [,3] [,4] [,5] [,6] ## actual 1359 1414 1484 1517 1573 1672 ## expected 1312 1387 1460 1532 1601 1670 Notice that the dashed line show the shift from calibration to hold out sample. We see that our model fits the data very well! And also captures the trend. 1.3.4 BG/BB I am not sure if this is a part of the curriculum 1.3.4.1 Data Preparation simElog &lt;- system.file(&quot;data/discreteSimElog.csv&quot; ,package = &quot;BTYD&quot;) elog &lt;- dc.ReadLines(simElog, cust.idx = 1, date.idx = 2) elog[1:3,] cust date 1 1970-01-01 1 1975-01-01 1 1977-01-01 elog$date &lt;- as.Date(elog$date, &quot;%Y-%m-%d&quot;) max(elog$date); ## [1] &quot;1983-01-01&quot; min(elog$date); ## [1] &quot;1970-01-01&quot; # let&#39;s make the calibration period end somewhere in-between T.cal &lt;- as.Date(&quot;1977-01-01&quot;) simData &lt;- dc.ElogToCbsCbt(elog, per=&quot;year&quot;, T.cal) cal.cbs &lt;- simData$cal$cbs freq&lt;- cal.cbs[,&quot;x&quot;] rec &lt;- cal.cbs[,&quot;t.x&quot;] trans.opp &lt;- 7 # transaction opportunities cal.rf.matrix &lt;- dc.MakeRFmatrixCal(freq, rec, trans.opp) cal.rf.matrix[1:5,] ## x t.x n.cal custs ## [1,] 0 0 7 2900 ## [2,] 1 1 7 933 ## [3,] 1 2 7 218 ## [4,] 2 2 7 489 ## [5,] 1 3 7 95 1.3.4.2 Parameter Estimation data(donationsSummary); rf.matrix &lt;- donationsSummary$rf.matrix params &lt;- bgbb.EstimateParameters(rf.matrix); LL &lt;- bgbb.rf.matrix.LL(params, rf.matrix); p.matrix &lt;- c(params, LL); for (i in 1:2){ params &lt;- bgbb.EstimateParameters(rf.matrix, params); LL &lt;- bgbb.rf.matrix.LL(params, rf.matrix); p.matrix.row &lt;- c(params, LL); p.matrix &lt;- rbind(p.matrix, p.matrix.row); } colnames(p.matrix) &lt;- c(&quot;alpha&quot;, &quot;beta&quot;, &quot;gamma&quot;, &quot;delta&quot;, &quot;LL&quot;); rownames(p.matrix) &lt;- 1:3; p.matrix; ## alpha beta gamma delta LL ## 1 1.2 0.75 0.657 2.78 -33226 ## 2 1.2 0.75 0.657 2.78 -33226 ## 3 1.2 0.75 0.657 2.78 -33226 1.3.4.3 Individual Level Estimations bgbb.Expectation(params, n=10); ## [1] 3.18 # customer A n.cal = 6 n.star = 10 x = 0 t.x = 0 bgbb.ConditionalExpectedTransactions(params, n.cal ,n.star, x, t.x) ## [1] 0.13 # customer B x = 4 t.x = 5 bgbb.ConditionalExpectedTransactions(params, n.cal ,n.star, x, t.x) ## [1] 3.63 1.3.4.4 Plotting / Goodness-of-fit bgbb.PlotFrequencyInCalibration(params, rf.matrix) ## 0 1 2 3 4 5 6 ## actual.frequency 3464 1823 1430 1085 1036 1063 1203 ## expected.frequency 3455 1889 1349 1113 1018 1027 1253 holdout.cbs &lt;- simData$holdout$cbs x.star &lt;- holdout.cbs[,&quot;x.star&quot;] n.star &lt;- 5 # length of the holdout period x.star &lt;- donationsSummary$x.star comp &lt;- bgbb.PlotFreqVsConditionalExpectedFrequency(params, n.star, rf.matrix, x.star) rownames(comp) &lt;- c(&quot;act&quot;, &quot;exp&quot;, &quot;bin&quot;) comp ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 freq.6 ## act 1.74e-01 0.433 0.813 1.39 2.06 2.64 3.53 ## exp 7.29e-02 0.325 0.709 1.33 2.03 2.78 3.75 ## bin 3.46e+03 1823.000 1430.000 1085.00 1036.00 1063.00 1203.00 comp &lt;- bgbb.PlotRecVsConditionalExpectedFrequency(params, n.star, rf.matrix, x.star) rownames(comp) &lt;- c(&quot;act&quot;, &quot;exp&quot;, &quot;bin&quot;) comp ## rec.0 rec.1 rec.2 rec.3 rec.4 rec.5 rec.6 ## act 1.74e-01 2.18e-01 0.39 0.487 0.809 1.65 2.95 ## exp 7.29e-02 8.57e-02 0.18 0.404 0.851 1.73 3.03 ## bin 3.46e+03 1.09e+03 890.00 706.000 654.000 1136.00 3163.00 inc.track.data &lt;- donationsSummary$annual.trans n.cal &lt;- 6 xtickmarks &lt;- 1996:2006 inc.tracking &lt;- bgbb.PlotTrackingInc(params, rf.matrix ,inc.track.data ,xticklab = xtickmarks) rownames(inc.tracking) &lt;- c(&quot;act&quot;, &quot;exp&quot;) inc.tracking ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] ## act 5652 4674 4019 3552 3555 3163 3110 2938 2703 2573 1936 ## exp 5536 4717 4148 3725 3396 3132 2914 2730 2574 2438 2319 cum.track.data &lt;- cumsum(inc.track.data) cum.tracking &lt;- bgbb.PlotTrackingCum(params, rf.matrix, cum.track.data ,xticklab = xtickmarks) rownames(cum.tracking) &lt;- c(&quot;act&quot;, &quot;exp&quot;) cum.tracking ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] ## act 5652 10326 14345 17897 21452 24615 27725 30663 33366 35939 37875 ## exp 5536 10253 14401 18125 21521 24653 27566 30297 32871 35309 37627 1.3.5 Examples from the lecture 1.3.5.1 Pareto/NBD PREDICTING LONG TERM CUSTOMER VALUE WITH BTYD PACKAGE Pareto/NBD (negative binomial distribution) modeling of repeat-buying behavior in a noncontractual setting Based on Schmittlein, Morrison, and Colombo (1987), “Counting Your Customers: Who Are They and What Will They Do Next?” Management Science, 33, 1-24. Required data for model is: “customer-by-sufficient-statistic” (cbs) matrix with the ‘sufficient’ stats being: This is the most important table, this is the information that is needed to run the model. frequency of transaction (no of repeat transactions) recency (time of last transaction) and total time observed Main model params are: beta unobserved shape parameter for Pareto dropout process s unobserved scale parameter for Pareto dropout process r unobserved shape parameter for NBD transaction alpha unobserved scale parameter for NBD transaction Data are divided into earlier calibration and later holdout segments, using a single date as the cut point. “cal” data are used to predict “holdout” data library(BTYD) # Read in data cdnowElog &lt;- system.file(&quot;data/cdnowElog.csv&quot;, package = &quot;BTYD&quot;) # The raw data file raw &lt;- read.csv(cdnowElog) head(raw) masterid sampleid date cds sales 4 1 2e+07 2 29.3 4 1 2e+07 2 29.7 4 1 2e+07 1 15.0 4 1 2e+07 2 26.5 21 2 2e+07 3 63.3 21 2 2e+07 1 11.8 We see the id of the customer and the date of transactions, and also amount of cd’s purchases and with the amount of sale.s # Create event log from file &quot;cdnowElog.csv&quot;, which has # customer IDs in the second column, dates in the third column, and # sales numbers in the fifth column. elog &lt;- dc.ReadLines(system.file(&quot;data/cdnowElog.csv&quot;, package=&quot;BTYD&quot;),2,3,5) elog[,&quot;date&quot;] &lt;- as.Date(elog[,&quot;date&quot;], &quot;%Y%m%d&quot;) head(elog) #take a look on the raw data cust date sales 1 1997-01-01 29.3 1 1997-01-18 29.7 1 1997-08-02 15.0 1 1997-12-12 26.5 2 1997-01-01 63.3 2 1997-01-13 11.8 This is a new dataset where we have changed the names and formated the date. Notice, that this should be the format of all tables if you want to run the model data &lt;- dc.ElogToCbsCbt(elog, per=&quot;week&quot;, T.cal=as.Date(&quot;1997-09-30&quot;)) attributes(data) ## $names ## [1] &quot;cal&quot; &quot;holdout&quot; &quot;cust.data&quot; We see that we have different attributes on the data. Notice that it is stored as a list. attributes(data$cal) ## $names ## [1] &quot;cbs&quot; &quot;cbt&quot; We see that the calibration data has both tables cbs and cbt. head(data$cal$cbs) #take a look on the calibration data ## x t.x T.cal ## 1 2 30.43 38.9 ## 2 1 1.71 38.9 ## 3 0 0.00 38.9 ## 4 0 0.00 38.9 ## 5 0 0.00 38.9 ## 6 7 29.43 38.9 We get the information: x = number of transactions. We see that there are only two transactions now, before we had 4, that is because one is in the hold out sample and we removed the first transaction. 1.3.5.1.1 Estimate parameters for Pareto/NBD model based upon calibration data # initial estimate (params2 &lt;- pnbd.EstimateParameters(data$cal$cbs)) ## [1] 0.553 10.580 0.606 11.656 We see the four params in the following order, r, alpha, s, beta # look at log likelihood (LL &lt;- pnbd.cbs.LL(params2, data$cal$cbs)) ## [1] -9595 # make a series of estimates, see if they converge to the same solution p.matrix &lt;- c(params2, LL) for (i in 1:20) { params2 &lt;- pnbd.EstimateParameters(data$cal$cbs, params2) LL &lt;- pnbd.cbs.LL(params2, data$cal$cbs) p.matrix.row &lt;- c(params2, LL) p.matrix &lt;- rbind(p.matrix, p.matrix.row) } #colnames(p.matrix) &lt;- c(&quot;beta&quot;,&quot;s&quot;,&quot;r&quot;,&quot;Alpha&quot;,&quot;LL&quot;) # examine p.matrix ## [,1] [,2] [,3] [,4] [,5] ## p.matrix 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 ## p.matrix.row 0.553 10.6 0.606 11.7 -9595 We see that the parameters are very stable, hence they dont change much in the iterations. We see the average dropout rate: s/beta #The average dropout rate 0.6060/11.65 # = 0.05201717. i.e. ## [1] 0.052 r = p.matrix[1,3] Alpha = p.matrix[1,4] r/Alpha ## p.matrix ## 0.052 We see the average transaction rate: #THe average transaction rate 0.5533/10.58 #=0.05229679 i.e. ## [1] 0.0523 beta = p.matrix[1,1] s = p.matrix[1,2] beta/s ## p.matrix ## 0.0523 Notice, it is a miracle that it so similar to the dropout rate. # use final set of values (params2 &lt;- p.matrix[dim(p.matrix)[1],1:4]) ## [1] 0.553 10.578 0.606 11.660 #The drop-out rate - Pareto part pnbd.PlotDropoutRateHeterogeneity(params2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## x.axis.ticks 0 0.00314 0.00628 0.00942 0.0126 0.0157 0.0188 0.022 ## heterogeneity Inf 28.04491 20.57566 16.90749 14.5532 12.8494 11.5290 10.460 ## [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] ## x.axis.ticks 0.0251 0.0282 0.0314 0.0345 0.0377 0.0408 0.0439 0.0471 0.0502 ## heterogeneity 9.5670 8.8049 8.1434 7.5614 7.0440 6.5800 6.1610 5.7803 5.4327 ## [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26] ## x.axis.ticks 0.0534 0.0565 0.0596 0.0628 0.0659 0.0691 0.0722 0.0753 0.0785 ## heterogeneity 5.1138 4.8203 4.5491 4.2979 4.0645 3.8473 3.6446 3.4552 3.2779 ## [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] ## x.axis.ticks 0.0816 0.0847 0.0879 0.091 0.0942 0.0973 0.1 0.104 0.107 0.11 ## heterogeneity 3.1117 2.9556 2.8088 2.671 2.5406 2.4178 2.3 2.192 2.089 1.99 ## [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] ## x.axis.ticks 0.113 0.116 0.119 0.122 0.126 0.129 0.132 0.135 0.138 0.141 0.144 ## heterogeneity 1.898 1.810 1.727 1.648 1.573 1.502 1.434 1.370 1.309 1.251 1.195 ## [,48] [,49] [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] ## x.axis.ticks 0.148 0.151 0.154 0.157 0.160 0.163 0.166 0.169 0.173 0.176 0.179 ## heterogeneity 1.143 1.092 1.045 0.999 0.956 0.914 0.875 0.837 0.801 0.767 0.734 ## [,59] [,60] [,61] [,62] [,63] [,64] [,65] [,66] [,67] [,68] [,69] ## x.axis.ticks 0.182 0.185 0.188 0.191 0.195 0.198 0.201 0.204 0.207 0.210 0.213 ## heterogeneity 0.703 0.673 0.645 0.618 0.592 0.567 0.543 0.520 0.499 0.478 0.458 ## [,70] [,71] [,72] [,73] [,74] [,75] [,76] [,77] [,78] [,79] [,80] ## x.axis.ticks 0.217 0.220 0.223 0.226 0.229 0.232 0.235 0.239 0.242 0.245 0.248 ## heterogeneity 0.439 0.421 0.404 0.387 0.371 0.356 0.341 0.327 0.314 0.301 0.289 ## [,81] [,82] [,83] [,84] [,85] [,86] [,87] [,88] [,89] [,90] [,91] ## x.axis.ticks 0.251 0.254 0.257 0.261 0.264 0.267 0.270 0.273 0.276 0.279 0.282 ## heterogeneity 0.277 0.266 0.255 0.245 0.235 0.225 0.216 0.207 0.199 0.191 0.183 ## [,92] [,93] [,94] [,95] [,96] [,97] [,98] [,99] [,100] ## x.axis.ticks 0.286 0.289 0.292 0.295 0.298 0.301 0.304 0.308 0.311 ## heterogeneity 0.176 0.169 0.162 0.156 0.149 0.144 0.138 0.132 0.127 The average lifetime: Hence we take one and divide it by the average dropout rate. 1/(0.6060/11.65) #i.e. ## [1] 19.2 r = p.matrix[1,3] Alpha = p.matrix[1,4] 1/(r/Alpha) ## p.matrix ## 19.2 We can then plot what characterizes the unobserved lifetime for a customer with an average dropoutrate # Plot average tendency to drop-out x &lt;- seq(0, 40, length.out=1000) dat &lt;- data.frame(x=x, densx=dexp(x, rate=0.052)) #Notice, 0.052 is the average dropout rate library(ggplot2) ggplot(dat, aes(x=x, y=densx)) + geom_line() # The transaction rate - NBD part pnbd.PlotTransactionRateHeterogeneity(params2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## x.axis.ticks 0 0.00332 0.00664 0.00995 0.0133 0.0166 0.0199 0.0232 ## heterogeneity Inf 28.37867 20.10409 16.19515 13.7510 12.0172 10.6953 9.6393 ## [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] ## x.axis.ticks 0.0265 0.0299 0.0332 0.0365 0.0398 0.0431 0.0464 0.0498 0.0531 ## heterogeneity 8.7680 8.0318 7.3983 6.8454 6.3574 5.9225 5.5321 5.1792 4.8585 ## [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26] ## x.axis.ticks 0.0564 0.0597 0.063 0.0664 0.0697 0.073 0.0763 0.0796 0.0829 ## heterogeneity 4.5656 4.2970 4.050 3.8216 3.6102 3.414 3.2315 3.0613 2.9023 ## [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] ## x.axis.ticks 0.0863 0.0896 0.0929 0.0962 0.0995 0.103 0.106 0.109 0.113 0.116 ## heterogeneity 2.7536 2.6142 2.4833 2.3604 2.2447 2.136 2.033 1.936 1.845 1.758 ## [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] ## x.axis.ticks 0.119 0.123 0.126 0.129 0.133 0.136 0.139 0.143 0.146 0.149 0.153 ## heterogeneity 1.676 1.599 1.525 1.456 1.390 1.327 1.268 1.211 1.157 1.106 1.058 ## [,48] [,49] [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] ## x.axis.ticks 0.156 0.159 0.163 0.166 0.169 0.173 0.176 0.179 0.182 0.186 0.189 ## heterogeneity 1.011 0.967 0.926 0.886 0.848 0.811 0.777 0.744 0.712 0.682 0.653 ## [,59] [,60] [,61] [,62] [,63] [,64] [,65] [,66] [,67] [,68] [,69] ## x.axis.ticks 0.192 0.196 0.199 0.202 0.206 0.209 0.212 0.216 0.219 0.222 0.226 ## heterogeneity 0.626 0.600 0.575 0.551 0.528 0.506 0.485 0.465 0.446 0.428 0.410 ## [,70] [,71] [,72] [,73] [,74] [,75] [,76] [,77] [,78] [,79] [,80] ## x.axis.ticks 0.229 0.232 0.236 0.239 0.242 0.246 0.249 0.252 0.255 0.259 0.262 ## heterogeneity 0.394 0.378 0.362 0.348 0.334 0.320 0.307 0.295 0.283 0.272 0.261 ## [,81] [,82] [,83] [,84] [,85] [,86] [,87] [,88] [,89] [,90] [,91] ## x.axis.ticks 0.265 0.269 0.272 0.275 0.279 0.282 0.285 0.289 0.292 0.295 0.299 ## heterogeneity 0.250 0.241 0.231 0.222 0.213 0.205 0.196 0.189 0.181 0.174 0.167 ## [,92] [,93] [,94] [,95] [,96] [,97] [,98] [,99] [,100] ## x.axis.ticks 0.302 0.305 0.309 0.312 0.315 0.319 0.322 0.325 0.328 ## heterogeneity 0.161 0.154 0.148 0.143 0.137 0.132 0.127 0.122 0.117 So what is the interpretation of 0.0523, which is the average transaction rate. It means that on average the customers have 0.05 transactions per period (week). So the customer buys a CD every 20 week # Overall fit - Calibration data # Aggregate plots pnbd.PlotFrequencyInCalibration(params2, data$cal$cbs, 7) ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 freq.6 freq.7+ ## n.x.actual 1411 439 214 100 62.0 38.0 29.0 64.0 ## n.x.expected 1434 397 194 112 70.6 46.4 31.5 70.8 Using the holdout period # Fit Holdout period # Plot of performance in hold-out period T.star &lt;- 39 # length of the holdout period censor &lt;- 7 # This censor serves the same purpose as above x.star &lt;- data$holdout$cbs[,&quot;x.star&quot;] comp &lt;- pnbd.PlotFreqVsConditionalExpectedFrequency(params2, T.star, data$cal$cbs, x.star, censor) rownames(comp) &lt;- c(&quot;act&quot;, &quot;exp&quot;, &quot;bin&quot;) comp ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 freq.6 freq.7+ ## act 0.237 0.697 1.39 1.56 2.53 2.95 3.86 6.36 ## exp 0.138 0.600 1.20 1.71 2.40 2.91 3.82 6.40 ## bin 1411.000 439.000 214.00 100.00 62.00 38.00 29.00 64.00 The plot compares what we have observed and what we expect from the model. The following calculates the expected number of transactions in a given period. # A randomly selected individual pnbd.Expectation(params2,78) ## [1] 1.91 We see that for any given customer we expect around 2 purchases for the period. #Probability that a randomly selected customer makes 0 purchases in the hold-out period pnbd.pmf.General(params2,T.star,t.end = 78,0) ## [1] 0.811 That is a probability of 81% # Analysing a specific customer - No of transactions data$cal$cbs[&quot;1516&quot;,] ## x t.x T.cal ## 26.0 30.9 31.0 x &lt;- data$cal$cbs[&quot;1516&quot;, &quot;x&quot;] t.x &lt;- data$cal$cbs[&quot;1516&quot;, &quot;t.x&quot;] T.cal &lt;- data$cal$cbs[&quot;1516&quot;, &quot;T.cal&quot;] # x # t.x # T.cal We see that the customer has made 26 purchases in the calibration period. He bought something in period 30.8 and we see the holdout period is 31 periods, hence we expect him to be alive in the end of the hold out period. Now we want to calculate the number of transactions n period ahead given the information that we have about the customer. # T.star no of periods ahead df &lt;- pnbd.ConditionalExpectedTransactions(params2,T.star=1:5,x,t.x,T.cal) t(t(df)) ## [,1] ## [1,] 0.633 ## [2,] 1.257 ## [3,] 1.873 ## [4,] 2.480 ## [5,] 3.080 Notice, that the table is accumulated, hence in period 1 we expect 0.63 transactions, then in period 1 and 2 we expect 1.25 transactions and lastly in period etc. The following show how much it changes from period to period (week). res &lt;- data.frame(diff(as.matrix(df))) print(res) ## diff.as.matrix.df.. ## 1 0.624 ## 2 0.616 ## 3 0.608 ## 4 0.600 # DERT discounted expected residual no. of transactions # 15% compounded annually has been converted to 0.0027 compounded continuously, # as we are dealing with weekly data and not annual data. d &lt;- 0.0027 pnbd.DERT(params2,x,t.x,T.cal,d) ## [1] 86 We see that the DERT = 86, hence we see that the later we get the transactions, that amount is not as valuable today. # calculate the discounted expected residual transactions of a customer # who made 2 repeat transactions in a calibration period that was 38.86 # weeks long, with the last transaction occurring in the middle of # the 31th week. pnbd.DERT(params2, x=2, t.x=30.43, T.cal=38.86, d) ## [1] 6.48 # We can also use vectors to compute DERT for several customers: pnbd.DERT(params2, x=1:10, t.x = 30.43, T.cal=38.86, d) ## [1] 3.99 6.48 8.90 11.22 13.44 15.54 17.51 19.33 20.98 22.45 We see that x iterates form 1 to 10, hence customers with 10 repeatative purchases in the calibration has a discounted expected residual no. of transactions that is increasing. pnbd.Plot.DERT(params2, x=0:14, t.x=0:38, T.cal=38.86, d, type=&quot;contour&quot;) ## 0 1 2 3 4 5 6 7 8 9 ## 0 0.477 0.578 0.320 0.129 0.0446 0.0140 0.00411 0.00116 3.16e-04 8.38e-05 ## 1 0.518 0.683 0.417 0.186 0.0702 0.0241 0.00776 0.00239 7.14e-04 2.07e-04 ## 2 0.559 0.795 0.530 0.258 0.1064 0.0397 0.01391 0.00466 1.51e-03 4.76e-04 ## 3 0.599 0.912 0.660 0.349 0.1561 0.0630 0.02385 0.00862 3.02e-03 1.03e-03 ## 4 0.639 1.033 0.807 0.462 0.2226 0.0968 0.03935 0.01528 5.74e-03 2.10e-03 ## 5 0.677 1.158 0.970 0.598 0.3098 0.1443 0.06280 0.02608 1.05e-02 4.09e-03 ## 6 0.715 1.286 1.150 0.760 0.4216 0.2098 0.09734 0.04306 1.84e-02 7.65e-03 ## 7 0.753 1.416 1.344 0.950 0.5622 0.2980 0.14697 0.06902 3.13e-02 1.38e-02 ## 8 0.789 1.547 1.553 1.167 0.7358 0.4144 0.21671 0.10773 5.16e-02 2.41e-02 ## 9 0.825 1.679 1.773 1.414 0.9466 0.5652 0.31270 0.16419 8.30e-02 4.08e-02 ## 10 0.860 1.811 2.005 1.689 1.1980 0.7570 0.44229 0.24479 1.30e-01 6.74e-02 ## 11 0.894 1.942 2.245 1.992 1.4930 0.9967 0.61399 0.35761 2.00e-01 1.09e-01 ## 12 0.928 2.073 2.492 2.320 1.8334 1.2908 0.83734 0.51255 3.01e-01 1.71e-01 ## 13 0.961 2.202 2.744 2.672 2.2199 1.6454 1.12262 0.72141 4.44e-01 2.64e-01 ## 14 0.993 2.329 2.999 3.044 2.6515 2.0653 1.48030 0.99777 6.43e-01 4.00e-01 ## 15 1.025 2.454 3.254 3.432 3.1256 2.5535 1.92033 1.35654 9.14e-01 5.95e-01 ## 16 1.056 2.576 3.509 3.832 3.6381 3.1104 2.45105 1.81323 1.28e+00 8.68e-01 ## 17 1.086 2.696 3.762 4.241 4.1834 3.7337 3.07813 2.38262 1.76e+00 1.24e+00 ## 18 1.116 2.812 4.011 4.653 4.7544 4.4179 3.80333 3.07706 2.37e+00 1.75e+00 ## 19 1.145 2.926 4.255 5.065 5.3436 5.1545 4.62353 3.90439 3.14e+00 2.42e+00 ## 20 1.174 3.036 4.493 5.472 5.9425 5.9324 5.53010 4.86577 4.08e+00 3.28e+00 ## 21 1.202 3.144 4.723 5.871 6.5431 6.7382 6.50899 5.95400 5.20e+00 4.36e+00 ## 22 1.229 3.248 4.947 6.260 7.1376 7.5579 7.54146 7.15262 6.49e+00 5.66e+00 ## 23 1.256 3.348 5.161 6.636 7.7190 8.3770 8.60554 8.43647 7.94e+00 7.20e+00 ## 24 1.282 3.446 5.368 6.996 8.2812 9.1822 9.67791 9.77366 9.50e+00 8.93e+00 ## 25 1.308 3.540 5.565 7.340 8.8195 9.9616 10.73584 11.12873 1.11e+01 1.08e+01 ## 26 1.333 3.630 5.754 7.667 9.3302 10.7055 11.75906 12.46636 1.28e+01 1.28e+01 ## 27 1.358 3.718 5.934 7.975 9.8108 11.4068 12.73091 13.75483 1.45e+01 1.48e+01 ## 28 1.382 3.802 6.104 8.266 10.2600 12.0605 13.63911 14.96851 1.60e+01 1.68e+01 ## 29 1.406 3.884 6.267 8.538 10.6771 12.6640 14.47577 16.08925 1.75e+01 1.86e+01 ## 30 1.429 3.962 6.420 8.792 11.0624 13.2164 15.23707 17.10641 1.88e+01 2.03e+01 ## 31 1.452 4.037 6.566 9.029 11.4167 13.7185 15.92256 18.01606 2.00e+01 2.18e+01 ## 32 1.474 4.110 6.704 9.249 11.7412 14.1721 16.53441 18.81965 2.10e+01 2.31e+01 ## 33 1.496 4.180 6.834 9.454 12.0376 14.5800 17.07664 19.52246 2.19e+01 2.42e+01 ## 34 1.518 4.247 6.956 9.644 12.3076 14.9453 17.55440 20.13226 2.27e+01 2.52e+01 ## 35 1.539 4.312 7.072 9.820 12.5530 15.2714 17.97347 20.65808 2.33e+01 2.60e+01 ## 36 1.560 4.374 7.181 9.982 12.7757 15.5618 18.33981 21.10938 2.39e+01 2.66e+01 ## 37 1.580 4.433 7.284 10.132 12.9776 15.8199 18.65923 21.49540 2.43e+01 2.72e+01 ## 38 1.600 4.491 7.381 10.271 13.1604 16.0491 18.93728 21.82485 2.47e+01 2.76e+01 ## 10 11 12 13 14 ## 0 2.18e-05 5.57e-06 1.40e-06 3.49e-07 8.58e-08 ## 1 5.90e-05 1.65e-05 4.55e-06 1.24e-06 3.33e-07 ## 2 1.47e-04 4.47e-05 1.34e-05 3.96e-06 1.16e-06 ## 3 3.43e-04 1.12e-04 3.63e-05 1.16e-05 3.66e-06 ## 4 7.52e-04 2.65e-04 9.18e-05 3.15e-05 1.07e-05 ## 5 1.57e-03 5.89e-04 2.18e-04 8.00e-05 2.90e-05 ## 6 3.12e-03 1.25e-03 4.92e-04 1.92e-04 7.40e-05 ## 7 5.96e-03 2.53e-03 1.06e-03 4.37e-04 1.79e-04 ## 8 1.10e-02 4.93e-03 2.18e-03 9.53e-04 4.12e-04 ## 9 1.97e-02 9.29e-03 4.33e-03 1.99e-03 9.08e-04 ## 10 3.41e-02 1.70e-02 8.31e-03 4.02e-03 1.92e-03 ## 11 5.77e-02 3.01e-02 1.55e-02 7.84e-03 3.94e-03 ## 12 9.53e-02 5.20e-02 2.80e-02 1.48e-02 7.80e-03 ## 13 1.54e-01 8.77e-02 4.93e-02 2.73e-02 1.50e-02 ## 14 2.43e-01 1.45e-01 8.48e-02 4.90e-02 2.81e-02 ## 15 3.77e-01 2.34e-01 1.43e-01 8.60e-02 5.12e-02 ## 16 5.73e-01 3.71e-01 2.35e-01 1.47e-01 9.13e-02 ## 17 8.57e-01 5.76e-01 3.80e-01 2.48e-01 1.59e-01 ## 18 1.26e+00 8.79e-01 6.03e-01 4.08e-01 2.72e-01 ## 19 1.81e+00 1.32e+00 9.38e-01 6.58e-01 4.55e-01 ## 20 2.55e+00 1.93e+00 1.43e+00 1.04e+00 7.47e-01 ## 21 3.53e+00 2.78e+00 2.14e+00 1.62e+00 1.20e+00 ## 22 4.78e+00 3.92e+00 3.13e+00 2.46e+00 1.90e+00 ## 23 6.32e+00 5.39e+00 4.48e+00 3.65e+00 2.92e+00 ## 24 8.14e+00 7.22e+00 6.24e+00 5.28e+00 4.39e+00 ## 25 1.02e+01 9.41e+00 8.45e+00 7.43e+00 6.41e+00 ## 26 1.25e+01 1.19e+01 1.11e+01 1.01e+01 9.06e+00 ## 27 1.49e+01 1.46e+01 1.41e+01 1.33e+01 1.24e+01 ## 28 1.72e+01 1.74e+01 1.73e+01 1.68e+01 1.62e+01 ## 29 1.95e+01 2.01e+01 2.05e+01 2.05e+01 2.03e+01 ## 30 2.16e+01 2.27e+01 2.35e+01 2.41e+01 2.45e+01 ## 31 2.35e+01 2.50e+01 2.63e+01 2.75e+01 2.84e+01 ## 32 2.51e+01 2.70e+01 2.88e+01 3.04e+01 3.18e+01 ## 33 2.65e+01 2.87e+01 3.08e+01 3.28e+01 3.47e+01 ## 34 2.76e+01 3.01e+01 3.24e+01 3.48e+01 3.70e+01 ## 35 2.86e+01 3.12e+01 3.38e+01 3.63e+01 3.88e+01 ## 36 2.94e+01 3.21e+01 3.48e+01 3.75e+01 4.02e+01 ## 37 3.00e+01 3.28e+01 3.56e+01 3.84e+01 4.12e+01 ## 38 3.05e+01 3.34e+01 3.63e+01 3.91e+01 4.20e+01 # P(alive) data$cal$cbs[&quot;1516&quot;,] ## x t.x T.cal ## 26.0 30.9 31.0 x &lt;- data$cal$cbs[&quot;1516&quot;, &quot;x&quot;] t.x &lt;- data$cal$cbs[&quot;1516&quot;, &quot;t.x&quot;] T.cal &lt;- data$cal$cbs[&quot;1516&quot;, &quot;T.cal&quot;] # x # t.x # T.cal x = 26 t.x = 30.85 T.cal = 31 pnbd.PAlive(params2, x, t.x, T.cal) ## [1] 0.998 We see that he has a probability of 99% of being alive at the end of the calibration period. palive.pnbd &lt;- pnbd.PAlive(params2, x = 1:5, t.x = 12, T.cal = 39) for (i in 1:5) { cat(&quot;x =&quot;, i, &quot;:&quot;, sprintf(&quot;%5.2f %%&quot;, 100*palive.pnbd[i]), &quot;\\n&quot;) } ## x = 1 : 45.49 % ## x = 2 : 33.19 % ## x = 3 : 22.14 % ## x = 4 : 13.61 % ## x = 5 : 7.83 % We see that as number of transactions inceases the probability of being alive decrease. The reason for this is that we se that the t.x. is in period 12 (the most recent purchase). We see that the customer who has bought five times before period 12, hence i bought a lot. is pretty certain that he is not a customer anymore. Where the customer who only made one purchase before period 12, may still be a customer, bu his pattern just tells that he does not buy that often. # P(Alive) across customers p.alives &lt;- pnbd.PAlive(params2,data$cal$cbs[,&quot;x&quot;],data$cal$cbs[,&quot;t.x&quot;],data$cal$cbs[,&quot;T.cal&quot;]) plot(density(p.alives)) The figure explains where the customers mainly lie of being alive. Hence in the region between 0.3 and 0.4, the most customers lie. Now we can try to calculate the number of active customers. # Number of active customers library(tidyverse) no_of_active &lt;- sum(p.alives) no_of_active ## [1] 1052 That is the sum of the probability of p.alives. This is an estimate of how many customers that are actually alive. One could also have made a cutoff saying that we will not accumulate on customer that have less than a certain threshold, e.g., 20%. This is what he does in the following, where he does: Creating a data frame Sorting the customers Plotting the spread Calculating how many active customers there are in the specific scenario. p.alive1 &lt;- as.data.frame(p.alives) head(p.alive1) p.alives 0.869 0.168 0.295 0.295 0.295 0.749 p.alive2 &lt;- tibble::rowid_to_column(p.alive1, &quot;ID&quot;) head(p.alive2) ID p.alives 1 0.869 2 0.168 3 0.295 4 0.295 5 0.295 6 0.749 newalive &lt;- arrange(p.alive2,desc(p.alives)) newalive2 &lt;- tibble::rowid_to_column(newalive, &quot;ID1&quot;) head(newalive2,6) ID1 ID p.alives 1 59 1 2 495 1 3 519 1 4 584 1 5 720 1 6 761 1 plot(newalive2$ID1,newalive2$p.alives,xlab=&quot;Sorted customers&quot;, ylab=&quot;P(Alive)&quot;) abline(h =0.5) We see that there are approximately 500 active customers. 1.3.5.2 BG/NBG example Notice, this is showing how this can be applied to any data set. Remember that that column names should be x, t.x, T.cal Notice, that this is not a super good files, as it is with customer with a loyalty cards, to they make many repeatitive purchases library(BTYD) cal1 &lt;- read.csv(&quot;Data/BTYD/cal2010.csv&quot;, header=TRUE,sep=&#39;;&#39;) cal2010 &lt;- as.matrix(cal1,nrow=3084,ncol=3) colnames(cal2010) &lt;- c(&quot;x&quot;,&quot;t.x&quot;,&quot;T.cal&quot;) head(cal2010,10) ## x t.x T.cal ## [1,] 77 85 85 ## [2,] 83 85 85 ## [3,] 56 83 85 ## [4,] 65 85 85 ## [5,] 77 85 85 ## [6,] 78 85 85 ## [7,] 78 85 85 ## [8,] 57 83 85 ## [9,] 83 85 85 ## [10,] 79 85 85 params &lt;- bgnbd.EstimateParameters(cal2010,par.start = c(1, 1, 1, 1), max.param.value = 10000) params ## p1 p2 p3 p4 ## 11.9792 14.7594 0.0047 6.5050 LL &lt;- bgnbd.cbs.LL(params, cal2010) LL ## [1] -251854 p.matrix &lt;- c(params, LL); p.matrix ## p1 p2 p3 p4 ## 1.20e+01 1.48e+01 4.70e-03 6.51e+00 -2.52e+05 for (i in 1:2){ params &lt;- bgnbd.EstimateParameters(cal2010, params); LL &lt;- bgnbd.cbs.LL(params, cal2010); p.matrix.row &lt;- c(params, LL); p.matrix &lt;- rbind(p.matrix, p.matrix.row); } colnames(p.matrix) &lt;- c(&quot;r&quot;, &quot;alpha&quot;, &quot;a&quot;, &quot;b&quot;, &quot;LL&quot;); rownames(p.matrix) &lt;- 1:3; p.matrix; ## r alpha a b LL ## 1 12 14.8 0.0047 6.51 -251854 ## 2 12 14.8 0.0047 6.51 -251854 ## 3 12 14.8 0.0047 6.51 -251854 bgnbd.PlotTransactionRateHeterogeneity(params) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## x.axis.ticks 0 1.47e-02 2.94e-02 4.41e-02 5.88e-02 7.34e-02 8.81e-02 ## heterogeneity 0 1.61e-14 2.61e-11 1.80e-09 3.42e-08 3.19e-07 1.90e-06 ## [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## x.axis.ticks 1.03e-01 0.117505 0.132193 0.146881 0.161569 0.17626 0.19095 ## heterogeneity 8.31e-06 0.000029 0.000085 0.000218 0.000499 0.00104 0.00203 ## [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] ## x.axis.ticks 0.20563 0.22032 0.2350 0.2497 0.2644 0.2791 0.2938 0.3085 0.3231 ## heterogeneity 0.00368 0.00632 0.0103 0.0162 0.0244 0.0356 0.0503 0.0692 0.0928 ## [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] ## x.axis.ticks 0.338 0.353 0.367 0.382 0.397 0.411 0.426 0.441 0.455 0.47 0.485 ## heterogeneity 0.122 0.156 0.197 0.244 0.297 0.357 0.422 0.494 0.569 0.65 0.733 ## [,35] [,36] [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] ## x.axis.ticks 0.499 0.514 0.529 0.543 0.558 0.573 0.588 0.602 0.617 0.632 0.646 ## heterogeneity 0.819 0.907 0.995 1.082 1.167 1.250 1.329 1.403 1.472 1.534 1.590 ## [,46] [,47] [,48] [,49] [,50] [,51] [,52] [,53] [,54] [,55] [,56] ## x.axis.ticks 0.661 0.676 0.69 0.705 0.72 0.734 0.749 0.764 0.778 0.793 0.808 ## heterogeneity 1.638 1.679 1.71 1.736 1.75 1.762 1.763 1.757 1.743 1.723 1.697 ## [,57] [,58] [,59] [,60] [,61] [,62] [,63] [,64] [,65] [,66] [,67] ## x.axis.ticks 0.823 0.837 0.852 0.867 0.881 0.896 0.911 0.925 0.94 0.955 0.969 ## heterogeneity 1.665 1.628 1.587 1.541 1.492 1.441 1.386 1.331 1.27 1.216 1.157 ## [,68] [,69] [,70] [,71] [,72] [,73] [,74] [,75] [,76] [,77] [,78] ## x.axis.ticks 0.984 0.999 1.013 1.028 1.043 1.058 1.072 1.087 1.102 1.116 1.131 ## heterogeneity 1.099 1.041 0.984 0.928 0.873 0.819 0.767 0.717 0.669 0.623 0.579 ## [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86] [,87] [,88] [,89] ## x.axis.ticks 1.146 1.160 1.18 1.190 1.204 1.219 1.23 1.248 1.263 1.278 1.293 ## heterogeneity 0.537 0.497 0.46 0.424 0.391 0.359 0.33 0.303 0.277 0.253 0.231 ## [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98] [,99] ## x.axis.ticks 1.307 1.322 1.337 1.351 1.366 1.38 1.395 1.410 1.4247 1.4394 ## heterogeneity 0.211 0.192 0.174 0.158 0.143 0.13 0.117 0.106 0.0957 0.0862 ## [,100] ## x.axis.ticks 1.4541 ## heterogeneity 0.0776 bgnbd.PlotDropoutRateHeterogeneity(params) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## x.axis.ticks 0 1.19e-04 2.37e-04 3.56e-04 0.000475 0.000594 0.000712 ## heterogeneity Inf 3.83e+01 1.92e+01 1.28e+01 9.624085 7.702304 6.419886 ## [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] ## x.axis.ticks 0.000831 0.00095 0.00107 0.00119 0.00131 0.00142 0.00154 0.00166 ## heterogeneity 5.503145 4.81512 4.27968 3.85109 3.50027 3.20779 2.96022 2.74793 ## [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] ## x.axis.ticks 0.00178 0.0019 0.00202 0.00214 0.00226 0.00237 0.00249 0.00261 ## heterogeneity 2.56389 2.4028 2.26062 2.13421 2.02107 1.91922 1.82705 1.74324 ## [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] ## x.axis.ticks 0.00273 0.00285 0.00297 0.00309 0.00321 0.00332 0.00344 0.00356 ## heterogeneity 1.66670 1.59653 1.53196 1.47234 1.41713 1.36586 1.31811 1.27354 ## [,32] [,33] [,34] [,35] [,36] [,37] [,38] [,39] ## x.axis.ticks 0.00368 0.0038 0.00392 0.00404 0.00415 0.00427 0.00439 0.00451 ## heterogeneity 1.23184 1.1927 1.15601 1.12143 1.08882 1.05802 1.02889 1.00128 ## [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] ## x.axis.ticks 0.00463 0.00475 0.00487 0.00499 0.0051 0.00522 0.00534 0.00546 ## heterogeneity 0.97508 0.95019 0.92652 0.90397 0.8825 0.86193 0.84231 0.82355 ## [,48] [,49] [,50] [,51] [,52] [,53] [,54] [,55] ## x.axis.ticks 0.00558 0.0057 0.00582 0.00594 0.00605 0.00617 0.00629 0.00641 ## heterogeneity 0.80558 0.7884 0.77183 0.75597 0.74073 0.72607 0.71197 0.69839 ## [,56] [,57] [,58] [,59] [,60] [,61] [,62] [,63] ## x.axis.ticks 0.00653 0.00665 0.00677 0.00689 0.007 0.00712 0.00724 0.00736 ## heterogeneity 0.68530 0.67267 0.66049 0.64873 0.637 0.62638 0.61575 0.60547 ## [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] ## x.axis.ticks 0.00748 0.0076 0.00772 0.00783 0.00795 0.00807 0.00819 0.00831 ## heterogeneity 0.59551 0.5859 0.57651 0.56745 0.55865 0.55011 0.54182 0.53376 ## [,72] [,73] [,74] [,75] [,76] [,77] [,78] [,79] ## x.axis.ticks 0.00843 0.00855 0.00867 0.00878 0.0089 0.00902 0.00914 0.00926 ## heterogeneity 0.52593 0.51832 0.51091 0.50371 0.4967 0.48987 0.48322 0.47674 ## [,80] [,81] [,82] [,83] [,84] [,85] [,86] [,87] ## x.axis.ticks 0.00938 0.0095 0.00962 0.00973 0.00985 0.00997 0.0101 0.0102 ## heterogeneity 0.47042 0.4643 0.45825 0.45239 0.44667 0.44109 0.4356 0.4303 ## [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] ## x.axis.ticks 0.0103 0.0104 0.0106 0.0107 0.0108 0.0109 0.011 0.0112 0.0113 ## heterogeneity 0.4251 0.4200 0.4150 0.4102 0.4054 0.4008 0.396 0.3918 0.3874 ## [,97] [,98] [,99] [,100] ## x.axis.ticks 0.0114 0.0115 0.0116 0.0118 ## heterogeneity 0.3831 0.3790 0.3749 0.3709 bgnbd.Expectation(params, t=52) ## p3 ## 41.9 bgnbd.PlotFrequencyInCalibration(params, cal2010, 85) ## freq.0 freq.1 freq.2 freq.3 freq.4 freq.5 freq.6 freq.7 freq.8 ## n.x.actual 0.00e+00 4.00 2.00 4.0 2.00 4.00 2.00 3.00 6.00 ## n.x.expected 3.53e-07 2.23 1.93 1.7 1.52 1.38 1.26 1.16 1.08 ## freq.9 freq.10 freq.11 freq.12 freq.13 freq.14 freq.15 freq.16 ## n.x.actual 4.00 5.000 5.000 3.000 3.000 3.000 5.000 6.00 ## n.x.expected 1.01 0.955 0.915 0.892 0.886 0.902 0.944 1.02 ## freq.17 freq.18 freq.19 freq.20 freq.21 freq.22 freq.23 freq.24 ## n.x.actual 10.00 5.00 12.00 9.00 9.00 6.00 10.00 4.00 ## n.x.expected 1.13 1.28 1.48 1.75 2.08 2.48 2.97 3.55 ## freq.25 freq.26 freq.27 freq.28 freq.29 freq.30 freq.31 freq.32 ## n.x.actual 7.00 4 12.00 5.00 16 10.00 7.0 14 ## n.x.expected 4.22 5 5.89 6.89 8 9.23 10.6 12 ## freq.33 freq.34 freq.35 freq.36 freq.37 freq.38 freq.39 freq.40 ## n.x.actual 7.0 15.0 9 17.0 12.0 15.0 17.0 11 ## n.x.expected 13.6 15.3 17 18.9 20.8 22.8 24.9 27 ## freq.41 freq.42 freq.43 freq.44 freq.45 freq.46 freq.47 freq.48 ## n.x.actual 18.0 16.0 36.0 15.0 22.0 22.0 16.0 19.0 ## n.x.expected 29.1 31.3 33.4 35.5 37.6 39.7 41.7 43.6 ## freq.49 freq.50 freq.51 freq.52 freq.53 freq.54 freq.55 freq.56 ## n.x.actual 20.0 23.0 25.0 17.0 30.0 22.0 24.0 27.0 ## n.x.expected 45.5 47.3 48.9 50.5 51.9 53.2 54.3 55.4 ## freq.57 freq.58 freq.59 freq.60 freq.61 freq.62 freq.63 freq.64 ## n.x.actual 29.0 31 33.0 23 31.0 31.0 35.0 28.0 ## n.x.expected 56.3 57 57.6 58 58.3 58.5 58.5 58.4 ## freq.65 freq.66 freq.67 freq.68 freq.69 freq.70 freq.71 freq.72 ## n.x.actual 34.0 51.0 47.0 47.0 41 62.0 61.0 45.0 ## n.x.expected 58.2 57.8 57.3 56.7 56 55.2 54.3 53.3 ## freq.73 freq.74 freq.75 freq.76 freq.77 freq.78 freq.79 freq.80 ## n.x.actual 53.0 74.0 73.0 78.0 93.0 109 94.0 123.0 ## n.x.expected 52.2 51.1 49.9 48.7 47.4 46 44.7 43.3 ## freq.81 freq.82 freq.83 freq.84 freq.85+ ## n.x.actual 161.0 184.0 205 225.0 327.0 ## n.x.expected 41.9 40.5 39 37.6 36.2 "],["customer-satisfaction-and-loyalty.html", "2 Customer Satisfaction and Loyalty 2.1 Customer surveys 2.2 NPS 2.3 CFA and SEM Application Lavann 2.4 PLS 2.5 Segmentation (Clustering)", " 2 Customer Satisfaction and Loyalty I’ll start of with some notes on constructing a survey. 2.1 Customer surveys The following sections are separated into five subsections: Concepts and examples Sampling and response rate Pretesting and measurement validity Questionnaire design Question types Scale labeling Question style Social desirability bias Optimizing versus satisfaction Simple size 2.1.1 (1) Concepts and examples We have six types of data: Customer identification data - information on the customer and who they are. Demographic data - information on a more macroscale, where they are from, ethnicities, age etc. Psychographic or lifestyle data - psychological characteristics and traits such as values, desires, goals, interests, and lifestyle choices. Transaction data - Do we have transactional information on the customer? What should it be used for and should it be preprocessed. Marketing action data Other types of data - e.g., financial and competitive data To get such data we can have three different sources. Internal secondary data External secondary data Primary data - survey, focus groups, in depth-interviews, observational techniques. Notice that this is the most costly and time consuming, that is also why getting external secondary data can be extremely expensive. Another thing that one must be aware of is innovation in survey designs, for instance, are we able to ask about what a customer wants in the future and if they answer. If you could rely on such answers, then I guess it would be very easy to be running a business. In addition of this, it is often experienced, that people are reluctant to answer honestly in surveys, this can be due to several things, for instance you are: Forced to answer - hence no motivation You don’t care about the purpose - hence no motivation. 2.1.2 (2) Sampling and response rate We need the sample to be representative of the population, e.g., equal amount of males and females to reflect real life. Naturally that comes down to what the purpose of the survey is. We often get a problem with low response rate. And those that actually respond might share similarities, e.g., they are having a lot of free time, they need the rewards that you may get from answering etc. So you want to be cautous about how to get them to answer your survey. We can overcome this by assigning statistical weigts to the underrepresented groups. Often we see that young and old adults, males and people with the highest income are underrepresented. Also there is a tendency to have few responses from busy people and those living in bigger cities. On the other hand then people with low education and income is often overrepresented. Then how to do the sampling? We have the following two approaches: Non-probability sampling - this is pretty much taking whom ever you feel like, for instance sharing a questionnaire on facebook, hence it will be distributed to your friends etc. This is not representative Probability sampling - this is the go to approach. Here we have four approaches. This will be explained following the picture. Sampling techniques Probability sampling techniques Simple random sampling: that is just a computer randomly selecting numbers. And then you select these observations. Systematic sampling: we do 1) selecting a suitable sampling frame, 2) Each element is assigned a number from 1 to N (Population for instance Denmark), 3) Determine sampling interval, 4) Pcik every ith element in succession from the sampling frame. Stratified sampling: where you group people in different stratas. They can be assigned on one or more parameters. This is widely used. See an illustration below Cluster sampling. Stratified sampling Subconclusion In the end, you always ask youself, is it representative. When you have the probabilty sampling methods, you randomly select a number of people that should answer, e.g., you selesct 1000, but only 543 answers, then do you know what people that actually replied, hence, do you know who answered and is it still representative? If you don’t know, then the data is very likely to be representative. Also often people will answer because you ‘forced’ them to answer, hence they are not really reliable, as we don’t know how honest and thorough they where. That is also elaborated in the following slide. So again, if you need 1000 responses, and you force all 1000 to respond, then they will often just rush through and thus they are not reliable. Hence that is not better than not letting them answer. 2.1.3 (3) Pretesting and measurement validity We need to pretest the sampling, the tradition way, conventional way. How we do it, is just giving the test to a couple of people and see if they understand the questions. This is key, because if they do not understand then you cannot expect that the mass will understand the questions. In general there are two approaches: Conventaional way: That is giving the questions to persons and they tell you their oppinion. One must be aware that this is very subjective and also a bit random. Moden pretesting way: This has two alternatives Behavior coding: you have a third person, an observer, will take notes about the responends actions. Cognitive pretesting: This is an alternative, the respondent must think out loud, hence verbalize whatever comes to mind when reading the questions. Subconclusion Conventional way has low reliability. And the behavior coding is the most reliable. An complete alternative can be to have conversational interviews although this is very time consuming and often not applied. 2.1.4 (4) Questionnaire design The following subsections will cover this. (4a) Question types Are we going to make open or closed questions. It has the following advantages and disadvantages. Closed questions: Respondents answer what they are given, henve we need to cover the different outcomes to have their experience reflected correctly. Open questions: Some respondents have difficulty of expressing their feelings or people just get lazy and not answer faithfully. Also you will have a ton of different answers. Hence they both have pros and cons, so one must be aware of when to choose what. In general we must be cautious about how we use strong words, like very, extreme etc. because it can have several meanings to each individual. To avoid mistakes, we can setup a questionnaire with questions that are essentially the same, but framed different, to get an idea of what the person intent to ask and also how consistent he is (that being for instance a weak satisfier). (4b) Scale labeling It is very good to have both a range defined by words, e.g., going from very bad to very good. Criticism of showing the numbers, let us say that it goes from -3 to 3, then the distance between -2 and -3 is one, but the numerical distance between two categories it not necessarily between -1 and -2. To solve this, people sometimes puts a line, e.g., between 0 and 100, then the repsonded is able to point out where he is on the range. Also, even though we produce numbers, people tend to do quantitative regression, while it is in fact a categorical variable. There are much criticism on using numbers. Also, the middle point, what does it mean, to the respondend it can be many many things. (4c) Question style Will it be an open or a closed question. (4d) Social desirability bias Imagine a questionnaire where you are asked about what happens in the past, for instance over the last year, how many times pr. month on average have you eaten a burger? 1 time, 2 - 3 times, 4 - 5 times, more than five times, 0 times This is often leading to over- and underreporting, as nobody can remember this. Theory has lead to stages when answering questinons (this should be taken with a grain of salt), see the following. Stages of answering questions In short, one must look up what they have in their memory and then try to fit it into the boxes thzat you give them. It is different how much effort a person is putting into the considerations, it can be generalized with: Optimizers: A thourough person really considering what to answer, these you want the most of. Weak satisfaction: This is just an optimizer which is less considerate. Strong satisficing: A person which will just select what he thinks is the most appropriate for the interviewer or researcher, these you do not want any of. This leads to the next section. (4e) Optimizing versus satisfaction Conditions to foster satisfaction. We need to make the questions as short as possible and use common words. It must be easy for them to understand to avoid them not to understand. This can be cooked down to: Task difficulty Respondents inability Respondents demotivation to optimize We now that the order of questions does matter: it creates the following effects. Primacy effects: That being prone to select the first question, e.g., weak satisfiers seem to go for this one, because they are just not too considerate Recency effects: Often when solutions is presented orally then they go for the last option. If we have a loooong list of questions, then people tend to either just agree or disagree. In general, people who want to save time, they tend to answer the same in each question, if they are similar. 2.1.5 (5) Simple size We can infer this statistically. Notice this example assume only one predictor variable. Sample size estimate We see that: z = the confidence level, we go for 5% hence 1.96. D = The interval you want to be within sigma = Variance, notice that this must be estimated, as to get true sigma you need characteristics on the population, and we don’t really have that. And now we can plug it into the function Sample size inference example Conducting a survey to estimate the monthly amount invested in savings schemes so that the estimate will be within 5 EUR, what would the sample size be? And we see that we need 496 respondents. Then what happens if we have multiple predictors? Then we can infer for each predictor and take the largest value. A rule of thumb is that we need at least 5 observations for a predictor 2.2 NPS # x &lt;- ##The ratings go here # library(NPS) # prop.table(table(x)) # nps.se(x) # nps.var(x) 2.3 CFA and SEM Application Lavann Data source: Hair et al., Multivariate Data Analysis, Pearson Education 2.3.1 Business Problem &amp; Objectives HBAT is a manufacturer of paper products who sells products to two market segments: the newsprint industry and the magazine industry. The current market is very competitive, so the manufacturer wants to understand how its customers perceive the company and make purchasing decisions, in order to enforce customers loyalty. The manufacturer commissioned a study asking its customers to complete a questionnaire on a secure website. In total, 100 customers - purchasing managers from different firms - buying from HBAT completed the questionnaire. The data consist of three main pieces of information: • A 1st type of information is available from HBAT ́s data warehouse and includes information on: customer type in terms of length of purchase relationship (X1) industry type(X2) size of the customer(X3) region of the customer(X4) distribution system(X5) • The 2nd type of information is collected based on the online questionnaire and includes consumers’ perceptions of HBAT ́s performance on 13 attributes using a continuous 0-10 (line) scale with 10 being “Excellent” and 0 being “Poor”. The 13 attributes are: X6 Product quality X7 E-commerce X8 Technical support X9 Complaint resolution X10 Advertising X11 Product line X12 Salesforce image X13 Competitive pricing X14 Warranty and claims X15 Packaging X16 Order and billing X17 Price flexibility X18 Delivery speed • The 3rd type of information relates to purchase outcomes and business relationships: satisfaction with HBAT, future purchase intention etc. (X19-X22) whether the firm would consider a strategic alliance/partnership with HBAT (X23). 2.3.2 Data The dataset (HBAT.sav) consists of data for n = 100 customers. Each observation contains information on 23 variables described above. Consistent with the marketing theory, there is an underlying factor structure in the data. When designing the study, the company has clearly 4 types of factors in their mind. They expect that the customer satisfaction is determined by the following four type of perceptions: perceptions about the product value, perceptions about the marketing actions, perceptions about the customer service and perceptions about the technical support.These factors are abstract constructs that can be measured in a survey using multi-item scales. The following items define each construct: X18 Delivery Speed X9 Complain resolution X16 Order and Billing, to express “Customer service” X11Product line X6 Product quality X13 Competing pricing, to express “Product value” X12 Salesforce image X7 E-commerce X10 Advertising, to express “Marketing” X8 Technical support X14 Warranty and claims, to express “Technical support” library(lavaan) library(foreign) data &lt;- read.spss(&quot;Data/CFA and SEM/HBAT.sav&quot;, to.data.frame=TRUE) 2.3.2.1 EFA In the following we are going to identify the factors and what variables that are related with the factors. # Exploratory factor analysis (EFA): explicitly assumes the existence of latent factors underlying the observed data. fit1 &lt;- factanal(~ x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18 ,factors = 4 ,data = data ,lower = 0.1 ,rotation = &quot;varimax&quot;) print(fit1) ## ## Call: ## factanal(x = ~x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18, factors = 4, data = data, rotation = &quot;varimax&quot;, lower = 0.1) ## ## Uniquenesses: ## x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 ## 0.623 0.305 0.285 0.183 0.663 0.100 0.100 0.595 0.100 0.987 0.358 0.100 0.100 ## ## Loadings: ## Factor1 Factor2 Factor3 Factor4 ## x6 0.609 ## x7 0.826 ## x8 0.113 0.838 ## x9 0.879 0.153 0.140 ## x10 0.199 -0.128 0.530 ## x11 0.500 0.814 0.115 ## x12 0.125 -0.140 0.929 ## x13 -0.561 0.219 -0.198 ## x14 0.146 0.931 ## x15 0.109 ## x16 0.784 0.109 0.111 ## x17 0.553 -0.750 0.204 -0.132 ## x18 0.940 0.122 0.183 ## ## Factor1 Factor2 Factor3 Factor4 ## SS loadings 2.910 2.030 1.990 1.657 ## Proportion Var 0.224 0.156 0.153 0.127 ## Cumulative Var 0.224 0.380 0.533 0.661 ## ## Test of the hypothesis that 4 factors are sufficient. ## The chi square statistic is 163 on 32 degrees of freedom. ## The p-value is 1.83e-19 #We see that the p-value is highly signinficant, hence we reject the model. A first exploration of the factors using the EFA reveals the factor loadings for each observable variable. High loadings (&gt;0.6 or &gt; 0.7) are expected for the items that theoretically define each construct. Items with cross-loadings should be removed. One can observe: x9, x16, x17, x18 load high on Factor 1 (Customer service) x6, x11, x13, x17 load high on Factor 2 (Product value) x7, x10?, x12 load high on Factor 3 (Marketing) x8 and x14 load high on Factor 4 (Technical support) x17 load high simultaneusly on two factors (Factor 1 and 2). This phenomenon is called “cross-loading”. one needs to remove the items with cross-loading from the measurement model. It means they do not measure a single construct. x11 might also be a candidate for deletion (loading high on factor 1 and 2); keep it for the moment. one can delete x15 which does not have high loadings on ANY of the four factors. Very low loadings (&lt;.10) are not displayed. # Run EFA without x15 and x17 fit2 &lt;- factanal(~ x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x16 + x18 ,factors = 4 ,data = data ,lower = 0.1 ,rotation = &quot;varimax&quot;) print(fit2,sort = T) ## ## Call: ## factanal(x = ~x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x16 + x18, factors = 4, data = data, rotation = &quot;varimax&quot;, lower = 0.1) ## ## Uniquenesses: ## x6 x7 x8 x9 x10 x11 x12 x13 x14 x16 x18 ## 0.635 0.305 0.285 0.163 0.668 0.100 0.100 0.599 0.100 0.342 0.100 ## ## Loadings: ## Factor1 Factor2 Factor3 Factor4 ## x9 0.895 0.135 0.128 ## x16 0.796 0.105 0.108 ## x18 0.918 0.193 0.154 ## x7 0.827 ## x10 0.180 0.537 ## x12 0.127 0.928 -0.144 ## x8 0.838 0.107 ## x14 0.932 0.140 ## x6 0.598 ## x11 0.519 0.120 0.786 ## x13 0.223 -0.202 -0.553 ## ## Factor1 Factor2 Factor3 Factor4 ## SS loadings 2.613 1.962 1.645 1.391 ## Proportion Var 0.238 0.178 0.150 0.126 ## Cumulative Var 0.238 0.416 0.565 0.692 ## ## Test of the hypothesis that 4 factors are sufficient. ## The chi square statistic is 26.7 on 17 degrees of freedom. ## The p-value is 0.0626 We want the cumulative variance to be above 60%, we see that we meet that criteria Test of the hypothesis that 4 factors are sufficient. The chi square statistic is 26.7 on 17 degrees of freedom. The p-value is 0.0626 (n.s.) hence we cannot reject, that there is a relationship. Using eigen value to assess amount of factors to select. # NOTE library(nFactors) ev &lt;- eigen(cor(data[,c(7:19)])) ev ## eigen() decomposition ## $values ## [1] 3.56707 2.99764 1.73808 1.28723 1.00524 0.61861 0.55143 0.44699 0.28074 ## [10] 0.20071 0.16620 0.13106 0.00901 ## ## $vectors ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] -0.00845 0.3621 0.0468 0.4703 -0.0501 0.05322 0.6659 -0.3323 -0.2246 ## [2,] -0.24427 -0.2589 -0.3590 0.3422 0.0977 0.37672 -0.2079 -0.1810 0.0433 ## [3,] -0.06909 0.2741 -0.5250 -0.3649 -0.0142 -0.05246 0.1286 -0.0214 -0.3623 ## [4,] -0.45714 0.1231 0.2005 -0.1024 0.0860 0.05697 0.0189 0.0449 -0.3091 ## [5,] -0.25522 -0.2097 -0.1830 0.3264 -0.1255 -0.78069 0.1380 0.2470 0.1262 ## [6,] -0.25245 0.4220 0.1037 0.2175 0.0321 0.16339 -0.1441 0.5145 0.0778 ## [7,] -0.29099 -0.2728 -0.3587 0.2956 0.0131 0.19591 -0.0905 -0.0613 -0.1245 ## [8,] 0.00825 -0.4322 0.0141 -0.1538 -0.0233 0.35802 0.6275 0.4831 0.1301 ## [9,] -0.12846 0.2680 -0.5228 -0.3228 -0.1301 0.00157 0.1071 0.0736 0.2955 ## [10,] -0.06569 -0.0120 0.0950 0.0153 -0.9701 0.14594 -0.1047 -0.0452 -0.0289 ## [11,] -0.43050 0.1017 0.1684 -0.1489 0.0566 0.02876 0.1377 -0.4002 0.6684 ## [12,] -0.27855 -0.3751 0.1499 -0.3457 -0.0136 -0.15476 0.0941 -0.2816 -0.3011 ## [13,] -0.47871 0.0728 0.2165 -0.0839 0.0377 0.02089 -0.0669 0.2112 -0.2023 ## [,10] [,11] [,12] [,13] ## [1,] 0.00787 0.1892 0.00495 0.00326 ## [2,] -0.56804 0.1257 0.24275 -0.02490 ## [3,] -0.31786 -0.3782 -0.34065 0.00771 ## [4,] 0.14360 -0.4215 0.64667 -0.01415 ## [5,] -0.13266 -0.0879 0.06437 -0.01273 ## [6,] -0.09583 0.0794 -0.18383 0.57585 ## [7,] 0.66320 -0.1459 -0.30866 0.04986 ## [8,] -0.07431 -0.1026 -0.00955 -0.01785 ## [9,] 0.26442 0.4612 0.36090 -0.01302 ## [10,] -0.05604 -0.0820 -0.00397 -0.01143 ## [11,] -0.05576 -0.2641 -0.22587 0.00806 ## [12,] -0.05326 0.3794 -0.06753 0.53420 ## [13,] -0.06681 0.3857 -0.29611 -0.61547 # EV = a measure of how much of varaince each factor explain. # According to this, we select 5 factors, equal to the no. of values # that is above 1. We defined the individual constructs based on the theory (Stage 1) and developed the overall measurement model making also a first exploration with EFA. The next stages (4 and 5) would be to assess the measurement model validity (Confirmatory Factor Analsysis). Lastly, stages 5 and 6 implies to specify the structural model (SEM) and to asssess the structural model validity. Hence, the next two main operational tasks are: Set up a confirmatory factor analysis to confirm the measurement model Given the measurement model has been examined and validated in the CFA analysis, set up a SEM model, to test the structural relationships between the four constructs identified and the customers´ likelihood to continue doing business with HBAT (X19-Satisfaction, X20-Likelihood of recommendation and X21-Likelihood of future purchase). variable.names(data) ## [1] &quot;id&quot; &quot;x1&quot; &quot;x2&quot; &quot;x3&quot; &quot;x4&quot; &quot;x5&quot; &quot;x6&quot; &quot;x7&quot; &quot;x8&quot; &quot;x9&quot; &quot;x10&quot; &quot;x11&quot; ## [13] &quot;x12&quot; &quot;x13&quot; &quot;x14&quot; &quot;x15&quot; &quot;x16&quot; &quot;x17&quot; &quot;x18&quot; &quot;x19&quot; &quot;x20&quot; &quot;x21&quot; &quot;x22&quot; &quot;x23&quot; VariableLabels &lt;- unname(attr(data, &quot;variable.labels&quot;)) # data.label.table &lt;- attr(sav, &quot;label.table&quot;) # if you load it with read_sav() summary(data) ## id x1 x2 ## Min. : 1.0 Less than 1 year:32 Magazine industry :52 ## 1st Qu.: 25.8 1 to 5 years :35 Newsprint industry:48 ## Median : 50.5 Over 5 years :33 ## Mean : 50.5 ## 3rd Qu.: 75.2 ## Max. :100.0 ## x3 x4 x5 ## Small (0 to 499):49 USA/North America :39 Indirect through broker:57 ## Large (500+) :51 Outside North America:61 Direct to customer :43 ## ## ## ## ## x6 x7 x8 x9 x10 ## Min. : 5.00 Min. :2.20 Min. :1.30 Min. :2.60 Min. :1.90 ## 1st Qu.: 6.58 1st Qu.:3.28 1st Qu.:4.25 1st Qu.:4.60 1st Qu.:3.17 ## Median : 8.00 Median :3.60 Median :5.40 Median :5.45 Median :4.00 ## Mean : 7.81 Mean :3.67 Mean :5.36 Mean :5.44 Mean :4.01 ## 3rd Qu.: 9.10 3rd Qu.:3.92 3rd Qu.:6.62 3rd Qu.:6.32 3rd Qu.:4.80 ## Max. :10.00 Max. :5.70 Max. :8.50 Max. :7.80 Max. :6.50 ## x11 x12 x13 x14 x15 ## Min. :2.30 Min. :2.90 Min. :3.70 Min. :4.10 Min. :1.70 ## 1st Qu.:4.70 1st Qu.:4.50 1st Qu.:5.88 1st Qu.:5.40 1st Qu.:4.10 ## Median :5.75 Median :4.90 Median :7.10 Median :6.10 Median :5.00 ## Mean :5.80 Mean :5.12 Mean :6.97 Mean :6.04 Mean :5.15 ## 3rd Qu.:6.80 3rd Qu.:5.80 3rd Qu.:8.40 3rd Qu.:6.60 3rd Qu.:6.30 ## Max. :8.40 Max. :8.20 Max. :9.90 Max. :8.10 Max. :9.50 ## x16 x17 x18 x19 x20 ## Min. :2.00 Min. :2.60 Min. :1.60 Min. :4.70 Min. :4.60 ## 1st Qu.:3.70 1st Qu.:3.70 1st Qu.:3.40 1st Qu.:6.00 1st Qu.:6.30 ## Median :4.40 Median :4.35 Median :3.90 Median :7.05 Median :7.00 ## Mean :4.28 Mean :4.61 Mean :3.89 Mean :6.92 Mean :7.02 ## 3rd Qu.:4.80 3rd Qu.:5.60 3rd Qu.:4.42 3rd Qu.:7.62 3rd Qu.:7.60 ## Max. :6.70 Max. :7.30 Max. :5.50 Max. :9.90 Max. :9.90 ## x21 x22 x23 ## Min. :5.50 Min. :37.1 No, would not consider:55 ## 1st Qu.:7.10 1st Qu.:51.1 Yes, would consider :45 ## Median :7.70 Median :58.6 ## Mean :7.71 Mean :58.4 ## 3rd Qu.:8.40 3rd Qu.:65.3 ## Max. :9.90 Max. :77.1 2.3.2.2 CFA model We use CFA to iterate through different models. 2.3.2.2.1 Initial model CS = Customer Service PV = Product value MK = Marketing TS = Technical support CFA.model &lt;- &#39;CS =~ x18 + x9 + x16 PV =~ x11 + x6 + x13 MK =~ x12 + x7 + x10 TS =~ x8 + x14 # Correlations between exogeneous constructs are optional because # by default, all exogenous latent variables in a CFA model are allowed to correlate CS ~~ PV CS ~~ MK CS ~~ TS PV ~~ MK PV ~~ TS MK ~~ TS&#39; # fit the model fit &lt;- cfa(CFA.model, data = data) # display summary output summary(fit , fit.measures=TRUE , standardized = TRUE , modindices = FALSE) #If TRUE, then we would get modificantion index ## lavaan 0.6-7 ended normally after 53 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 28 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 61.835 ## Degrees of freedom 38 ## P-value (Chi-square) 0.009 ## ## Model Test Baseline Model: ## ## Test statistic 655.315 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.960 ## Tucker-Lewis Index (TLI) 0.943 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1349.559 ## Loglikelihood unrestricted model (H1) -1318.642 ## ## Akaike (AIC) 2755.119 ## Bayesian (BIC) 2828.064 ## Sample-size adjusted Bayesian (BIC) 2739.632 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.079 ## 90 Percent confidence interval - lower 0.040 ## 90 Percent confidence interval - upper 0.114 ## P-value RMSEA &lt;= 0.05 0.097 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.087 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS =~ ## x18 1.000 0.703 0.961 ## x9 1.549 0.099 15.657 0.000 1.088 0.905 ## x16 1.035 0.092 11.198 0.000 0.727 0.787 ## PV =~ ## x11 1.000 1.547 1.182 ## x6 0.360 0.103 3.515 0.000 0.558 0.401 ## x13 -0.385 0.112 -3.444 0.001 -0.596 -0.388 ## MK =~ ## x12 1.000 1.071 1.003 ## x7 0.514 0.064 8.009 0.000 0.550 0.789 ## x10 0.566 0.104 5.434 0.000 0.605 0.540 ## TS =~ ## x8 1.000 1.014 0.666 ## x14 0.962 0.391 2.461 0.014 0.976 1.197 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS ~~ ## PV 0.608 0.113 5.406 0.000 0.560 0.560 ## MK 0.204 0.080 2.558 0.011 0.271 0.271 ## TS 0.089 0.075 1.190 0.234 0.125 0.125 ## PV ~~ ## MK 0.006 0.136 0.042 0.967 0.003 0.003 ## TS 0.265 0.176 1.506 0.132 0.169 0.169 ## MK ~~ ## TS 0.120 0.106 1.126 0.260 0.110 0.110 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x18 0.040 0.018 2.242 0.025 0.040 0.076 ## .x9 0.261 0.056 4.675 0.000 0.261 0.181 ## .x16 0.325 0.051 6.396 0.000 0.325 0.381 ## .x11 -0.681 0.427 -1.596 0.111 -0.681 -0.398 ## .x6 1.619 0.229 7.065 0.000 1.619 0.839 ## .x13 2.008 0.283 7.096 0.000 2.008 0.850 ## .x12 -0.008 0.112 -0.070 0.944 -0.008 -0.007 ## .x7 0.183 0.039 4.665 0.000 0.183 0.378 ## .x10 0.891 0.131 6.806 0.000 0.891 0.708 ## .x8 1.290 0.440 2.929 0.003 1.290 0.556 ## .x14 -0.288 0.373 -0.770 0.441 -0.288 -0.432 ## CS 0.494 0.077 6.392 0.000 1.000 1.000 ## PV 2.394 0.472 5.076 0.000 1.000 1.000 ## MK 1.146 0.196 5.843 0.000 1.000 1.000 ## TS 1.029 0.485 2.122 0.034 1.000 1.000 # Check the model fit (see slides for references). # NOTE: we get &quot;lavaan WARNING: some estimated ov variances are negative&quot;. # This is called in the literature &quot;Heywood case&quot;. Heywood cases or negative variance estimates, are a common occurrence in factor analysis and latent variable structural equation models. # There are several potential causes (https://journals.sagepub.com/doi/10.1177/0049124112442138). Here,eliminating the problematic item x11, will solve the problem. We see that we want to maximize TLI and CFI towards 1. Atm it looks well. We want the RMSEA (Root Mean Square Error of Approximation) to be below the 5% level. This we obtain in this example with pointe estimate of 0.079 where the 5% level is at 0.097, hence we are below. She writes in the slides (54) that we just need to be below 8%, this we also meet. she also writes that above 10% is a poor fit. We see that we get a warning, that is because there are negative variances, this is because of variable 11, that we ended up leaving in the model. Now we can ask for the modification indexes. # Ask for the modification indiceslin modificationindices(fit ,sort = T ,minimum.value = 10 ,op = &quot;~~&quot;) lhs op rhs mi epc sepc.lv sepc.all sepc.nox 68 x18 ~~ x11 15.7 0.128 0.128 0.773 0.773 69 x18 ~~ x6 11.2 -0.135 -0.135 -0.526 -0.526 85 x16 ~~ x11 10.7 -0.154 -0.154 -0.327 -0.327 # MI reveal that x11 is correlated with x16 and x18; it means that x11 has substantial cross-loading on two factors (we also found this in EFA). Cross-loading goes against one of the principles of unidimensionality in SEM. We delete x11 from the analysis and re-run CFA. This confirms that we must do something with x11 (it should have been done in the exploratory assessment, but it was kept to show it) 2.3.2.2.2 Second model Model without x11 Notice that the EFA already suggested this, but we wanted to explore how it affected the overall model. And we end up seeing the consensus that we need to do something with V11. # CFA model after deleting x11 set.seed(1234) CFA.model &lt;- &#39;CS =~ x18 + x9 + x16 PV =~ x6 + x13 MK =~ x12 + x7 + x10 TS =~ x8 + x14&#39; # fit the model fit &lt;- cfa(CFA.model, data = data) # display summary output summary(fit, fit.measures=TRUE, standardized = TRUE, modindices = FALSE) ## lavaan 0.6-7 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 26 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 18.183 ## Degrees of freedom 29 ## P-value (Chi-square) 0.940 ## ## Model Test Baseline Model: ## ## Test statistic 530.377 ## Degrees of freedom 45 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.035 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1221.406 ## Loglikelihood unrestricted model (H1) -1212.314 ## ## Akaike (AIC) 2494.812 ## Bayesian (BIC) 2562.546 ## Sample-size adjusted Bayesian (BIC) 2480.432 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.016 ## P-value RMSEA &lt;= 0.05 0.989 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.039 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS =~ ## x18 1.000 0.677 0.926 ## x9 1.658 0.113 14.664 0.000 1.123 0.934 ## x16 1.107 0.098 11.294 0.000 0.749 0.811 ## PV =~ ## x6 1.000 0.625 0.450 ## x13 -2.197 0.962 -2.284 0.022 -1.372 -0.893 ## MK =~ ## x12 1.000 1.066 0.999 ## x7 0.518 0.063 8.249 0.000 0.552 0.792 ## x10 0.571 0.103 5.531 0.000 0.609 0.543 ## TS =~ ## x8 1.000 1.280 0.841 ## x14 0.604 0.135 4.467 0.000 0.774 0.948 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS ~~ ## PV 0.055 0.054 1.022 0.307 0.131 0.131 ## MK 0.191 0.077 2.471 0.013 0.265 0.265 ## TS 0.123 0.097 1.272 0.203 0.142 0.142 ## PV ~~ ## MK -0.199 0.112 -1.774 0.076 -0.299 -0.299 ## TS 0.239 0.144 1.665 0.096 0.299 0.299 ## MK ~~ ## TS 0.127 0.145 0.880 0.379 0.093 0.093 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x18 0.076 0.022 3.434 0.001 0.076 0.142 ## .x9 0.185 0.059 3.137 0.002 0.185 0.128 ## .x16 0.292 0.048 6.108 0.000 0.292 0.342 ## .x6 1.540 0.268 5.750 0.000 1.540 0.798 ## .x13 0.480 0.756 0.635 0.525 0.480 0.203 ## .x12 0.003 0.106 0.026 0.979 0.003 0.002 ## .x7 0.181 0.038 4.714 0.000 0.181 0.372 ## .x10 0.887 0.130 6.813 0.000 0.887 0.705 ## .x8 0.681 0.362 1.882 0.060 0.681 0.294 ## .x14 0.067 0.128 0.523 0.601 0.067 0.100 ## CS 0.458 0.077 5.936 0.000 1.000 1.000 ## PV 0.390 0.227 1.721 0.085 1.000 1.000 ## MK 1.136 0.193 5.886 0.000 1.000 1.000 ## TS 1.638 0.469 3.494 0.000 1.000 1.000 2.3.2.2.2.1 Intepretation of the model 2.3.2.2.2.1.1 1.) Examine the MODEL FIT a much better fit than we obtained before A decent model requires: CFI &gt;.90, TLI&gt;.90, RMSEA&lt; 0.08, SRMR &lt;.0.08. Check these indexes of model fit in your summary. 2.3.2.2.2.1.2 2). Examine the LOADINGS significance, size and sign The (std.) loadings should be at least +-.40. It is desirable to have high and significant loadings - it reflects items convergent validity. In one factor 2, competitive pricing (x13) and product quality (x6) have opposite signs. It means that the product quality and competitive pricing vary together, but move in direction opposite to each other. Perceptions are more positive whether product quality increases or price decreases. This trade-off leads to naming the factor product value. When variables have different signs, we need to be careful to reverse one when creating summated scales or using further in SEM analysis. Reverse scoring is the process by which the data values for a variable are reversed so that its correlation with the other variables are reversed (go from negative to positive). The purpose of reversing is to prevent a canceling out of variables with positive and negative loading. Reverse coding is typically required if we have some negatively phrased statement items in our questionnaire. For a categorical variable e.g. 1-5, the reversing implies: 1-&gt;5, 2-&gt;4, 3-&gt;2, 4-&gt;1, 5-&gt;1 For a continous variable: newvar = oldvar * (-1) summary(data$x13) str(data$x13) # reversing X13 : library(dplyr) library(tidyr) data &lt;- data %&gt;% mutate(x13r = x13 * (-1)) str(data$x13r) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.70 5.88 7.10 6.97 8.40 9.90 ## num [1:100] 6.8 5.3 4.5 8.8 6.8 8.5 8.9 6.9 9.3 8.4 ... ## num [1:100] -6.8 -5.3 -4.5 -8.8 -6.8 -8.5 -8.9 -6.9 -9.3 -8.4 ... 2.3.2.2.2.1.3 3). Examine RELIABILITY of the factors Reliability = assessment of degree of consistency between multiple measurements of a variable (back to the slides to refer to this concept). library(semTools) semTools::reliability(fit) ## CS PV MK TS ## alpha 0.897 -1.329 0.783 0.798 ## omega 0.921 0.217 0.822 0.849 ## omega2 0.921 0.217 0.822 0.849 ## omega3 0.922 0.217 0.823 0.849 ## avevar 0.805 0.529 0.629 0.750 # alpha = coefficient alpha (Cronbach, 1951) - should be &gt; than 0.5 or 0.6 (some textbooks) # omega = is similar to composite reliability index (CR) (Fornell &amp; Larcker (1981) - should be &gt; 0.7 # avevar = average variance extracted (AVE) (Fornell &amp; Larcker (1981)) - should be &gt; than 0.5. # For PV factor, reliability was calculed with the non-reversed item; let´s change that: We want the avevar to be above 0.60. The omega is the composite reliability, it should be higher than 0.7. We want the alpha to be close to 1. We see that PV is not meeting the two above. that is because we have a negative variable. Now we are going to reverse x13 to make the construct PV meet the requirements (or at least get closer to) set.seed(1234) CFA.model &lt;- &#39; # Measurement model CS =~ x18 + x9 + x16 PV =~ x6 + x13r MK =~ x12 + x7 + x10 TS =~ x8 + x14&#39; fit &lt;- cfa(CFA.model, data = data) summary(fit, fit.measures=TRUE, standardized = TRUE, modindices = FALSE) ## lavaan 0.6-7 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 26 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 18.183 ## Degrees of freedom 29 ## P-value (Chi-square) 0.940 ## ## Model Test Baseline Model: ## ## Test statistic 530.377 ## Degrees of freedom 45 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.035 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1221.406 ## Loglikelihood unrestricted model (H1) -1212.314 ## ## Akaike (AIC) 2494.812 ## Bayesian (BIC) 2562.546 ## Sample-size adjusted Bayesian (BIC) 2480.432 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.016 ## P-value RMSEA &lt;= 0.05 0.989 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.039 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS =~ ## x18 1.000 0.677 0.926 ## x9 1.658 0.113 14.664 0.000 1.123 0.934 ## x16 1.107 0.098 11.294 0.000 0.749 0.811 ## PV =~ ## x6 1.000 0.625 0.450 ## x13r 2.197 0.962 2.284 0.022 1.372 0.893 ## MK =~ ## x12 1.000 1.066 0.999 ## x7 0.518 0.063 8.249 0.000 0.552 0.792 ## x10 0.571 0.103 5.531 0.000 0.609 0.543 ## TS =~ ## x8 1.000 1.280 0.841 ## x14 0.604 0.135 4.467 0.000 0.774 0.948 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS ~~ ## PV 0.055 0.054 1.022 0.307 0.131 0.131 ## MK 0.191 0.077 2.471 0.013 0.265 0.265 ## TS 0.123 0.097 1.272 0.203 0.142 0.142 ## PV ~~ ## MK -0.199 0.112 -1.774 0.076 -0.299 -0.299 ## TS 0.239 0.144 1.665 0.096 0.299 0.299 ## MK ~~ ## TS 0.127 0.145 0.880 0.379 0.093 0.093 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x18 0.076 0.022 3.434 0.001 0.076 0.142 ## .x9 0.185 0.059 3.137 0.002 0.185 0.128 ## .x16 0.292 0.048 6.108 0.000 0.292 0.342 ## .x6 1.540 0.268 5.750 0.000 1.540 0.798 ## .x13r 0.480 0.756 0.635 0.525 0.480 0.203 ## .x12 0.003 0.106 0.026 0.979 0.003 0.002 ## .x7 0.181 0.038 4.714 0.000 0.181 0.372 ## .x10 0.887 0.130 6.813 0.000 0.887 0.705 ## .x8 0.681 0.362 1.882 0.060 0.681 0.294 ## .x14 0.067 0.128 0.523 0.601 0.067 0.100 ## CS 0.458 0.077 5.936 0.000 1.000 1.000 ## PV 0.390 0.227 1.721 0.085 1.000 1.000 ## MK 1.136 0.193 5.886 0.000 1.000 1.000 ## TS 1.638 0.469 3.494 0.000 1.000 1.000 semTools::reliability(fit) ## CS PV MK TS ## alpha 0.897 0.571 0.783 0.798 ## omega 0.921 0.664 0.822 0.849 ## omega2 0.921 0.664 0.822 0.849 ## omega3 0.922 0.664 0.823 0.849 ## avevar 0.805 0.529 0.629 0.750 given alpha, omega and avevar values overpass the recommended values, one can conclude that all factors display good reliability. Although it does not appear as if the PV avevar does not meet the 60% goal. With x13 reversed, we see that the model is far better. Although it does not entirely meet the requirements. 2.3.2.2.2.1.4 4). Examine DISCRIMINANT VALIDITY of the factors each pair of latent correlations (correlations between the principal components) should be sufficiently below 1 (in absolute value), that the latent variables can be thought of representing two distinct contructs. (to recall, back to the slides to refer to this concept). discriminantValidity(fit, merge=TRUE) lhs op rhs est ci.lower ci.upper Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) CS ~~ PV 0.131 -0.096 0.357 32 2522 2582 51.5 33.3 3 0 CS ~~ MK 0.265 0.074 0.456 32 2624 2684 153.1 134.9 3 0 CS ~~ TS 0.142 -0.066 0.350 32 2597 2657 126.2 108.0 3 0 PV ~~ MK -0.299 -0.527 -0.072 32 2514 2574 43.6 25.5 3 0 PV ~~ TS 0.299 0.062 0.536 32 2518 2577 46.9 28.8 3 0 MK ~~ TS 0.093 -0.110 0.297 32 2599 2659 128.0 109.8 3 0 Output: The first set are factor correlation estimates and their confidence intervals. Are these correlations sufficiently low to claim discriminant validity of the four constructs? Based on Fornell &amp; Larcker (1981), the square root of each construct´s AVE should have a greater value than the inter-constructs corelations (alternatitvely, AVE &gt; corr^2). Let us check that: reliability_out = reliability (fit) AVEs = reliability_out[5,] sqrtAVEs = sqrt(AVEs) sqrtAVEs ## CS PV MK TS ## 0.897 0.728 0.793 0.866 Comparing the inter-constructs correlations (see “est”\" column in the output of discriminantValidity(fit, merge=TRUE)) with the sqrtAVEs, we conclude that cf. Fornell &amp; Larcker (1981) criterion, the four constructss display significant discriminat validity. Now we can plot the CFA. notice that the arrows are not yet directed hence you have not yet imposed the structure that we are going to do in SEM. 2.3.2.3 SEM NOTE: Three variables were not included in the CFA (x11, x15, x17) as these did not meet the assumptions, reasoning both in the EFA and CFA for x11. Reason: These variables did not load high on any of the main constructs If they are important, they can be treated as separate explanatory variables in SEM last DV in our SEM model will be x19-Satisfaction. in other words, we build a model to explain x19 Notice that we are also continuing to use the reversed version of x13r. SEM.model1 &lt;- &#39; # Measurement model CS =~ x18 + x9 + x16 PV =~ x6 + x13r MK =~ x12 + x7 + x10 TS =~ x8 + x14 # Structural model x19 ~ CS + PV + MK + TS&#39; Now we can fit the model fitSEM1 &lt;- sem(SEM.model1 ,data=data ,se=&quot;robust&quot; ,estimator = &quot;ML&quot;) #Maximum Likelihood, that is the standard summary(fitSEM1, fit.measures=TRUE, standardized = TRUE, rsquare=TRUE) ## lavaan 0.6-7 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 31 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 32.225 ## Degrees of freedom 35 ## P-value (Chi-square) 0.603 ## ## Model Test Baseline Model: ## ## Test statistic 688.965 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.007 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1308.074 ## Loglikelihood unrestricted model (H1) -1291.961 ## ## Akaike (AIC) 2678.148 ## Bayesian (BIC) 2758.908 ## Sample-size adjusted Bayesian (BIC) 2661.002 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.064 ## P-value RMSEA &lt;= 0.05 0.877 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.064 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS =~ ## x18 1.000 0.678 0.928 ## x9 1.652 0.127 12.995 0.000 1.120 0.932 ## x16 1.106 0.102 10.858 0.000 0.750 0.812 ## PV =~ ## x6 1.000 1.126 0.810 ## x13r 0.676 0.125 5.422 0.000 0.761 0.495 ## MK =~ ## x12 1.000 1.150 1.078 ## x7 0.447 0.048 9.309 0.000 0.514 0.737 ## x10 0.466 0.063 7.340 0.000 0.535 0.478 ## TS =~ ## x8 1.000 1.057 0.694 ## x14 0.886 0.452 1.960 0.050 0.936 1.148 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## x19 ~ ## CS 0.787 0.116 6.758 0.000 0.534 0.450 ## PV 0.663 0.110 6.022 0.000 0.746 0.629 ## MK 0.518 0.078 6.671 0.000 0.595 0.502 ## TS -0.039 0.045 -0.870 0.384 -0.042 -0.035 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS ~~ ## PV 0.090 0.099 0.912 0.362 0.118 0.118 ## MK 0.181 0.083 2.194 0.028 0.232 0.232 ## TS 0.099 0.091 1.088 0.277 0.138 0.138 ## PV ~~ ## MK -0.269 0.146 -1.838 0.066 -0.208 -0.208 ## TS 0.131 0.142 0.920 0.357 0.110 0.110 ## MK ~~ ## TS 0.137 0.131 1.046 0.296 0.112 0.112 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x18 0.074 0.017 4.328 0.000 0.074 0.139 ## .x9 0.191 0.045 4.274 0.000 0.191 0.132 ## .x16 0.291 0.041 7.080 0.000 0.291 0.341 ## .x6 0.663 0.203 3.261 0.001 0.663 0.343 ## .x13r 1.784 0.206 8.662 0.000 1.784 0.755 ## .x12 -0.184 0.085 -2.161 0.031 -0.184 -0.162 ## .x7 0.222 0.037 6.005 0.000 0.222 0.456 ## .x10 0.971 0.113 8.580 0.000 0.971 0.772 ## .x8 1.201 0.576 2.083 0.037 1.201 0.518 ## .x14 -0.212 0.424 -0.499 0.618 -0.212 -0.318 ## .x19 0.170 0.087 1.963 0.050 0.170 0.121 ## CS 0.460 0.077 5.942 0.000 1.000 1.000 ## PV 1.267 0.261 4.861 0.000 1.000 1.000 ## MK 1.322 0.186 7.119 0.000 1.000 1.000 ## TS 1.118 0.645 1.734 0.083 1.000 1.000 ## ## R-Square: ## Estimate ## x18 0.861 ## x9 0.868 ## x16 0.659 ## x6 0.657 ## x13r 0.245 ## x12 NA ## x7 0.544 ## x10 0.228 ## x8 0.482 ## x14 NA ## x19 0.879 If the message “lavaan WARNING: some estimated ov variances are negative” shows up. In this case, the problematic items are x12 and x14. It reflects that we would need more quality data and more items per construct to run this model. We set se=“robust” to produce robust standard errors; setting se=“boot” or se=“bootstrap” will produce bootstrap standard errors. Now we can check the information critera / indexes as we did in the CFA. The following is a summary of all important criteria. fitmeasures(fit) # alternative summary ## npar fmin chisq df ## 26.000 0.091 18.183 29.000 ## pvalue baseline.chisq baseline.df baseline.pvalue ## 0.940 530.377 45.000 0.000 ## cfi tli nnfi rfi ## 1.000 1.035 1.035 0.947 ## nfi pnfi ifi rni ## 0.966 0.622 1.022 1.022 ## logl unrestricted.logl aic bic ## -1221.406 -1212.314 2494.812 2562.546 ## ntotal bic2 rmsea rmsea.ci.lower ## 100.000 2480.432 0.000 0.000 ## rmsea.ci.upper rmsea.pvalue rmr rmr_nomean ## 0.016 0.989 0.048 0.048 ## srmr srmr_bentler srmr_bentler_nomean crmr ## 0.039 0.039 0.039 0.043 ## crmr_nomean srmr_mplus srmr_mplus_nomean cn_05 ## 0.043 0.039 0.039 235.044 ## cn_01 gfi agfi pgfi ## 273.711 0.966 0.935 0.509 ## mfi ecvi ## 1.056 0.702 Next, check the structural coeficients in summary(). Output partially reproduced below: Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all x19 ~ CS 0.787 0.116 6.758 0.000 0.534 0.450 PV 0.663 0.110 6.022 0.000 0.746 0.629 MK 0.518 0.078 6.671 0.000 0.595 0.502 TS -0.039 0.045 -0.870 0.384 -0.042 -0.035 Concl.: Customers perceptions about CS, PV and MK are positively and significantly correlated with satisfaction. TS (Technical Service) perceptions is not significantly related to customer satisfaction. check modification indices if relevant summary(fitSEM1, fit.measures=TRUE, standardized = TRUE, rsquare=TRUE, modindices=TRUE) modificationindices(fitSEM1, sort = T, minimum.value = 10, op = &quot;~~&quot;) ## lavaan 0.6-7 ended normally after 60 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 31 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 32.225 ## Degrees of freedom 35 ## P-value (Chi-square) 0.603 ## ## Model Test Baseline Model: ## ## Test statistic 688.965 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.007 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1308.074 ## Loglikelihood unrestricted model (H1) -1291.961 ## ## Akaike (AIC) 2678.148 ## Bayesian (BIC) 2758.908 ## Sample-size adjusted Bayesian (BIC) 2661.002 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.064 ## P-value RMSEA &lt;= 0.05 0.877 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.064 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS =~ ## x18 1.000 0.678 0.928 ## x9 1.652 0.127 12.995 0.000 1.120 0.932 ## x16 1.106 0.102 10.858 0.000 0.750 0.812 ## PV =~ ## x6 1.000 1.126 0.810 ## x13r 0.676 0.125 5.422 0.000 0.761 0.495 ## MK =~ ## x12 1.000 1.150 1.078 ## x7 0.447 0.048 9.309 0.000 0.514 0.737 ## x10 0.466 0.063 7.340 0.000 0.535 0.478 ## TS =~ ## x8 1.000 1.057 0.694 ## x14 0.886 0.452 1.960 0.050 0.936 1.148 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## x19 ~ ## CS 0.787 0.116 6.758 0.000 0.534 0.450 ## PV 0.663 0.110 6.022 0.000 0.746 0.629 ## MK 0.518 0.078 6.671 0.000 0.595 0.502 ## TS -0.039 0.045 -0.870 0.384 -0.042 -0.035 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS ~~ ## PV 0.090 0.099 0.912 0.362 0.118 0.118 ## MK 0.181 0.083 2.194 0.028 0.232 0.232 ## TS 0.099 0.091 1.088 0.277 0.138 0.138 ## PV ~~ ## MK -0.269 0.146 -1.838 0.066 -0.208 -0.208 ## TS 0.131 0.142 0.920 0.357 0.110 0.110 ## MK ~~ ## TS 0.137 0.131 1.046 0.296 0.112 0.112 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x18 0.074 0.017 4.328 0.000 0.074 0.139 ## .x9 0.191 0.045 4.274 0.000 0.191 0.132 ## .x16 0.291 0.041 7.080 0.000 0.291 0.341 ## .x6 0.663 0.203 3.261 0.001 0.663 0.343 ## .x13r 1.784 0.206 8.662 0.000 1.784 0.755 ## .x12 -0.184 0.085 -2.161 0.031 -0.184 -0.162 ## .x7 0.222 0.037 6.005 0.000 0.222 0.456 ## .x10 0.971 0.113 8.580 0.000 0.971 0.772 ## .x8 1.201 0.576 2.083 0.037 1.201 0.518 ## .x14 -0.212 0.424 -0.499 0.618 -0.212 -0.318 ## .x19 0.170 0.087 1.963 0.050 0.170 0.121 ## CS 0.460 0.077 5.942 0.000 1.000 1.000 ## PV 1.267 0.261 4.861 0.000 1.000 1.000 ## MK 1.322 0.186 7.119 0.000 1.000 1.000 ## TS 1.118 0.645 1.734 0.083 1.000 1.000 ## ## R-Square: ## Estimate ## x18 0.861 ## x9 0.868 ## x16 0.659 ## x6 0.657 ## x13r 0.245 ## x12 NA ## x7 0.544 ## x10 0.228 ## x8 0.482 ## x14 NA ## x19 0.879 ## ## Modification Indices: ## ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 36 CS =~ x6 0.356 -0.197 -0.133 -0.096 -0.096 ## 37 CS =~ x13r 0.356 0.133 0.090 0.059 0.059 ## 38 CS =~ x12 0.455 -0.092 -0.062 -0.058 -0.058 ## 39 CS =~ x7 0.001 0.002 0.001 0.002 0.002 ## 40 CS =~ x10 2.401 0.234 0.159 0.142 0.142 ## 41 CS =~ x8 0.158 -0.079 -0.054 -0.035 -0.035 ## 42 CS =~ x14 0.158 0.070 0.048 0.058 0.058 ## 43 PV =~ x18 2.442 -0.055 -0.062 -0.085 -0.085 ## 44 PV =~ x9 1.008 0.058 0.066 0.055 0.055 ## 45 PV =~ x16 0.612 0.043 0.049 0.053 0.053 ## 46 PV =~ x12 0.806 0.077 0.087 0.081 0.081 ## 47 PV =~ x7 1.280 -0.049 -0.055 -0.079 -0.079 ## 48 PV =~ x10 0.058 0.023 0.026 0.023 0.023 ## 49 PV =~ x8 1.002 0.095 0.107 0.070 0.070 ## 50 PV =~ x14 1.002 -0.084 -0.095 -0.116 -0.116 ## 51 MK =~ x18 0.376 0.017 0.020 0.027 0.027 ## 52 MK =~ x9 0.064 -0.012 -0.014 -0.011 -0.011 ## 53 MK =~ x16 0.274 -0.024 -0.028 -0.030 -0.030 ## 54 MK =~ x6 3.377 0.329 0.378 0.272 0.272 ## 55 MK =~ x13r 3.377 -0.222 -0.256 -0.166 -0.166 ## 56 MK =~ x8 0.708 -0.083 -0.095 -0.063 -0.063 ## 57 MK =~ x14 0.708 0.073 0.084 0.104 0.104 ## 58 TS =~ x18 0.404 -0.018 -0.019 -0.026 -0.026 ## 59 TS =~ x9 0.291 -0.025 -0.026 -0.022 -0.022 ## 60 TS =~ x16 2.951 0.078 0.083 0.090 0.090 ## 61 TS =~ x6 2.902 -0.287 -0.304 -0.219 -0.219 ## 62 TS =~ x13r 2.902 0.194 0.206 0.134 0.134 ## 63 TS =~ x12 0.211 0.031 0.032 0.030 0.030 ## 64 TS =~ x7 0.159 -0.014 -0.014 -0.021 -0.021 ## 65 TS =~ x10 0.071 -0.020 -0.021 -0.019 -0.019 ## 66 x18 ~~ x9 0.036 0.011 0.011 0.091 0.091 ## 67 x18 ~~ x16 0.045 -0.006 -0.006 -0.044 -0.044 ## 68 x18 ~~ x6 1.441 -0.041 -0.041 -0.185 -0.185 ## 69 x18 ~~ x13r 0.029 -0.008 -0.008 -0.022 -0.022 ## 70 x18 ~~ x12 0.427 -0.011 -0.011 -0.092 -0.092 ## 71 x18 ~~ x7 0.559 0.011 0.011 0.083 0.083 ## 72 x18 ~~ x10 2.482 0.051 0.051 0.192 0.192 ## 73 x18 ~~ x8 1.139 -0.034 -0.034 -0.114 -0.114 ## 74 x18 ~~ x14 0.091 0.005 0.005 0.041 0.041 ## 75 x18 ~~ x19 0.051 0.005 0.005 0.046 0.046 ## 76 x9 ~~ x16 0.004 0.003 0.003 0.014 0.014 ## 77 x9 ~~ x6 0.481 0.039 0.039 0.109 0.109 ## 78 x9 ~~ x13r 0.630 0.062 0.062 0.106 0.106 ## 79 x9 ~~ x12 0.473 0.018 0.018 0.099 0.099 ## 80 x9 ~~ x7 0.507 -0.017 -0.017 -0.080 -0.080 ## 81 x9 ~~ x10 0.916 -0.051 -0.051 -0.119 -0.119 ## 82 x9 ~~ x8 2.939 0.089 0.089 0.186 0.186 ## 83 x9 ~~ x14 2.066 -0.040 -0.040 -0.201 -0.201 ## 84 x9 ~~ x19 0.058 -0.009 -0.009 -0.050 -0.050 ## 85 x16 ~~ x6 0.031 0.009 0.009 0.021 0.021 ## 86 x16 ~~ x13r 0.049 0.017 0.017 0.024 0.024 ## 87 x16 ~~ x12 0.560 -0.020 -0.020 -0.084 -0.084 ## 88 x16 ~~ x7 0.836 0.022 0.022 0.085 0.085 ## 89 x16 ~~ x10 0.004 0.003 0.003 0.006 0.006 ## 90 x16 ~~ x8 1.439 -0.063 -0.063 -0.107 -0.107 ## 91 x16 ~~ x14 3.275 0.051 0.051 0.203 0.203 ## 92 x16 ~~ x19 0.000 0.001 0.001 0.003 0.003 ## 94 x6 ~~ x12 0.161 0.039 0.039 0.111 0.111 ## 95 x6 ~~ x7 0.256 0.026 0.026 0.067 0.067 ## 96 x6 ~~ x10 0.046 -0.021 -0.021 -0.026 -0.026 ## 97 x6 ~~ x8 0.021 0.014 0.014 0.016 0.016 ## 98 x6 ~~ x14 0.865 -0.067 -0.067 -0.179 -0.179 ## 99 x6 ~~ x19 7.958 1.408 1.408 4.191 4.191 ## 100 x13r ~~ x12 0.784 -0.081 -0.081 -0.141 -0.141 ## 101 x13r ~~ x7 0.004 -0.004 -0.004 -0.006 -0.006 ## 102 x13r ~~ x10 0.080 -0.037 -0.037 -0.028 -0.028 ## 103 x13r ~~ x8 1.321 0.146 0.146 0.100 0.100 ## 104 x13r ~~ x14 0.197 0.031 0.031 0.050 0.050 ## 105 x13r ~~ x19 7.958 -0.952 -0.952 -1.728 -1.728 ## 106 x12 ~~ x7 0.494 0.082 0.082 0.405 0.405 ## 107 x12 ~~ x10 2.507 -0.136 -0.136 -0.322 -0.322 ## 108 x12 ~~ x8 0.019 -0.007 -0.007 -0.014 -0.014 ## 109 x12 ~~ x14 0.125 0.012 0.012 0.059 0.059 ## 110 x12 ~~ x19 2.831 0.158 0.158 0.894 0.894 ## 111 x7 ~~ x10 1.855 0.066 0.066 0.142 0.142 ## 112 x7 ~~ x8 0.029 0.006 0.006 0.013 0.013 ## 113 x7 ~~ x14 0.114 -0.007 -0.007 -0.032 -0.032 ## 114 x7 ~~ x19 3.051 -0.066 -0.066 -0.341 -0.341 ## 115 x10 ~~ x8 0.446 -0.058 -0.058 -0.054 -0.054 ## 116 x10 ~~ x14 0.023 0.007 0.007 0.015 0.015 ## 117 x10 ~~ x19 0.124 0.021 0.021 0.052 0.052 ## 119 x8 ~~ x19 0.000 0.000 0.000 0.001 0.001 ## 120 x14 ~~ x19 0.000 0.000 0.000 -0.002 -0.002 lhs op rhs mi epc sepc.lv sepc.all sepc.nox First we see the summary, and we can assess for improvements, there does not appear to be any. no suggestion for improvement # If required, the bootstrap model parameters are available with: # PAR.boot &lt;- bootstrapLavaan(fitSEM1, R=10, type=&quot;ordinary&quot;,FUN=&quot;coef&quot;) # T.boot &lt;- bootstrapLavaan(fitSEM1, R=10, type=&quot;bollen.stine&quot;,FUN=fitMeasures, fit.measures=&quot;chisq&quot;) 2.3.3 Plotting the model and this is another plot This also show what variables that should be exlcuded. Note: to improve the model, exclude the items with the negative variance. hence the two items, x12 and x14 2.3.4 Consider a more complex SEM model involving a mediating effect. Consistent with the theory, Sem.model2 proposed x19 (Satisfaction) as mediator between the four latent constructs and Likelihood of future purchase (x21). SEM.model2 &lt;- &#39; # Measurement model CS =~ x18 + x9 + x16 PV =~ x6 + x13r MK =~ x12 + x7 + x10 TS =~ x8 + x14 # Structural model x19 ~ CS + PV + MK + TS x21 ~ x19&#39; # fit the model fitSEM2 &lt;- sem(SEM.model2, data=data, se=&quot;robust&quot;) summary(fitSEM2, fit.measures=TRUE) ## lavaan 0.6-7 ended normally after 55 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 33 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 45.798 ## Degrees of freedom 45 ## P-value (Chi-square) 0.439 ## ## Model Test Baseline Model: ## ## Test statistic 778.988 ## Degrees of freedom 66 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.999 ## Tucker-Lewis Index (TLI) 0.998 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1404.638 ## Loglikelihood unrestricted model (H1) -1381.738 ## ## Akaike (AIC) 2875.275 ## Bayesian (BIC) 2961.246 ## Sample-size adjusted Bayesian (BIC) 2857.023 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.013 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.068 ## P-value RMSEA &lt;= 0.05 0.814 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.065 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## CS =~ ## x18 1.000 ## x9 1.652 0.127 12.995 0.000 ## x16 1.106 0.102 10.858 0.000 ## PV =~ ## x6 1.000 ## x13r 0.676 0.125 5.422 0.000 ## MK =~ ## x12 1.000 ## x7 0.447 0.048 9.309 0.000 ## x10 0.466 0.063 7.340 0.000 ## TS =~ ## x8 1.000 ## x14 0.886 0.452 1.960 0.050 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## x19 ~ ## CS 0.787 0.116 6.758 0.000 ## PV 0.663 0.110 6.022 0.000 ## MK 0.518 0.078 6.671 0.000 ## TS -0.039 0.045 -0.870 0.384 ## x21 ~ ## x19 0.574 0.055 10.450 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## CS ~~ ## PV 0.090 0.099 0.912 0.362 ## MK 0.181 0.083 2.194 0.028 ## TS 0.099 0.091 1.088 0.277 ## PV ~~ ## MK -0.269 0.146 -1.838 0.066 ## TS 0.131 0.142 0.920 0.357 ## MK ~~ ## TS 0.137 0.131 1.046 0.296 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x18 0.074 0.017 4.328 0.000 ## .x9 0.191 0.045 4.274 0.000 ## .x16 0.291 0.041 7.080 0.000 ## .x6 0.663 0.203 3.261 0.001 ## .x13r 1.784 0.206 8.662 0.000 ## .x12 -0.184 0.085 -2.161 0.031 ## .x7 0.222 0.037 6.005 0.000 ## .x10 0.971 0.113 8.580 0.000 ## .x8 1.201 0.576 2.083 0.037 ## .x14 -0.212 0.424 -0.499 0.618 ## .x19 0.170 0.087 1.963 0.050 ## .x21 0.404 0.047 8.674 0.000 ## CS 0.460 0.077 5.942 0.000 ## PV 1.267 0.261 4.861 0.000 ## MK 1.322 0.186 7.119 0.000 ## TS 1.118 0.645 1.734 0.083 summary(fitSEM2, fit.measures=TRUE, standardized = TRUE, rsquare=TRUE, modindices=TRUE) modificationindices(fitSEM2, sort = T, minimum.value = 10, op = &quot;~~&quot;) ## lavaan 0.6-7 ended normally after 55 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 33 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 45.798 ## Degrees of freedom 45 ## P-value (Chi-square) 0.439 ## ## Model Test Baseline Model: ## ## Test statistic 778.988 ## Degrees of freedom 66 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.999 ## Tucker-Lewis Index (TLI) 0.998 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1404.638 ## Loglikelihood unrestricted model (H1) -1381.738 ## ## Akaike (AIC) 2875.275 ## Bayesian (BIC) 2961.246 ## Sample-size adjusted Bayesian (BIC) 2857.023 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.013 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.068 ## P-value RMSEA &lt;= 0.05 0.814 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.065 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS =~ ## x18 1.000 0.678 0.928 ## x9 1.652 0.127 12.995 0.000 1.120 0.932 ## x16 1.106 0.102 10.858 0.000 0.750 0.812 ## PV =~ ## x6 1.000 1.126 0.810 ## x13r 0.676 0.125 5.422 0.000 0.761 0.495 ## MK =~ ## x12 1.000 1.150 1.078 ## x7 0.447 0.048 9.309 0.000 0.514 0.737 ## x10 0.466 0.063 7.340 0.000 0.535 0.478 ## TS =~ ## x8 1.000 1.057 0.694 ## x14 0.886 0.452 1.960 0.050 0.936 1.148 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## x19 ~ ## CS 0.787 0.116 6.758 0.000 0.534 0.450 ## PV 0.663 0.110 6.022 0.000 0.746 0.629 ## MK 0.518 0.078 6.671 0.000 0.595 0.502 ## TS -0.039 0.045 -0.870 0.384 -0.042 -0.035 ## x21 ~ ## x19 0.574 0.055 10.450 0.000 0.574 0.731 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## CS ~~ ## PV 0.090 0.099 0.912 0.362 0.118 0.118 ## MK 0.181 0.083 2.194 0.028 0.232 0.232 ## TS 0.099 0.091 1.088 0.277 0.138 0.138 ## PV ~~ ## MK -0.269 0.146 -1.838 0.066 -0.208 -0.208 ## TS 0.131 0.142 0.920 0.357 0.110 0.110 ## MK ~~ ## TS 0.137 0.131 1.046 0.296 0.112 0.112 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x18 0.074 0.017 4.328 0.000 0.074 0.139 ## .x9 0.191 0.045 4.274 0.000 0.191 0.132 ## .x16 0.291 0.041 7.080 0.000 0.291 0.341 ## .x6 0.663 0.203 3.261 0.001 0.663 0.343 ## .x13r 1.784 0.206 8.662 0.000 1.784 0.755 ## .x12 -0.184 0.085 -2.161 0.031 -0.184 -0.162 ## .x7 0.222 0.037 6.005 0.000 0.222 0.456 ## .x10 0.971 0.113 8.580 0.000 0.971 0.772 ## .x8 1.201 0.576 2.083 0.037 1.201 0.518 ## .x14 -0.212 0.424 -0.499 0.618 -0.212 -0.318 ## .x19 0.170 0.087 1.963 0.050 0.170 0.121 ## .x21 0.404 0.047 8.674 0.000 0.404 0.466 ## CS 0.460 0.077 5.942 0.000 1.000 1.000 ## PV 1.267 0.261 4.861 0.000 1.000 1.000 ## MK 1.322 0.186 7.119 0.000 1.000 1.000 ## TS 1.118 0.645 1.734 0.083 1.000 1.000 ## ## R-Square: ## Estimate ## x18 0.861 ## x9 0.868 ## x16 0.659 ## x6 0.657 ## x13r 0.245 ## x12 NA ## x7 0.544 ## x10 0.228 ## x8 0.482 ## x14 NA ## x19 0.879 ## x21 0.534 ## ## Modification Indices: ## ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 38 CS =~ x6 0.356 -0.197 -0.133 -0.096 -0.096 ## 39 CS =~ x13r 0.356 0.133 0.090 0.059 0.059 ## 40 CS =~ x12 0.455 -0.092 -0.062 -0.058 -0.058 ## 41 CS =~ x7 0.001 0.002 0.001 0.002 0.002 ## 42 CS =~ x10 2.401 0.234 0.159 0.142 0.142 ## 43 CS =~ x8 0.158 -0.079 -0.054 -0.035 -0.035 ## 44 CS =~ x14 0.158 0.070 0.048 0.058 0.058 ## 45 PV =~ x18 2.442 -0.055 -0.062 -0.085 -0.085 ## 46 PV =~ x9 1.008 0.058 0.066 0.055 0.055 ## 47 PV =~ x16 0.612 0.043 0.049 0.053 0.053 ## 48 PV =~ x12 0.806 0.077 0.087 0.081 0.081 ## 49 PV =~ x7 1.280 -0.049 -0.055 -0.079 -0.079 ## 50 PV =~ x10 0.058 0.023 0.026 0.023 0.023 ## 51 PV =~ x8 1.002 0.095 0.107 0.070 0.070 ## 52 PV =~ x14 1.002 -0.084 -0.095 -0.116 -0.116 ## 53 MK =~ x18 0.376 0.017 0.020 0.027 0.027 ## 54 MK =~ x9 0.064 -0.012 -0.014 -0.011 -0.011 ## 55 MK =~ x16 0.274 -0.024 -0.028 -0.030 -0.030 ## 56 MK =~ x6 3.377 0.329 0.378 0.272 0.272 ## 57 MK =~ x13r 3.377 -0.222 -0.256 -0.166 -0.166 ## 58 MK =~ x8 0.708 -0.083 -0.095 -0.063 -0.063 ## 59 MK =~ x14 0.708 0.073 0.084 0.104 0.104 ## 60 TS =~ x18 0.404 -0.018 -0.019 -0.026 -0.026 ## 61 TS =~ x9 0.291 -0.025 -0.026 -0.022 -0.022 ## 62 TS =~ x16 2.951 0.078 0.083 0.090 0.090 ## 63 TS =~ x6 2.901 -0.287 -0.304 -0.219 -0.219 ## 64 TS =~ x13r 2.902 0.194 0.206 0.134 0.134 ## 65 TS =~ x12 0.211 0.031 0.032 0.030 0.030 ## 66 TS =~ x7 0.159 -0.014 -0.014 -0.021 -0.021 ## 67 TS =~ x10 0.071 -0.020 -0.021 -0.019 -0.019 ## 68 x18 ~~ x9 0.036 0.011 0.011 0.091 0.091 ## 69 x18 ~~ x16 0.045 -0.006 -0.006 -0.044 -0.044 ## 70 x18 ~~ x6 1.441 -0.041 -0.041 -0.185 -0.185 ## 71 x18 ~~ x13r 0.029 -0.008 -0.008 -0.022 -0.022 ## 72 x18 ~~ x12 0.427 -0.011 -0.011 -0.092 -0.092 ## 73 x18 ~~ x7 0.559 0.011 0.011 0.083 0.083 ## 74 x18 ~~ x10 2.482 0.051 0.051 0.192 0.192 ## 75 x18 ~~ x8 1.139 -0.034 -0.034 -0.114 -0.114 ## 76 x18 ~~ x14 0.091 0.005 0.005 0.041 0.041 ## 77 x18 ~~ x19 0.051 0.005 0.005 0.046 0.046 ## 78 x18 ~~ x21 0.001 0.000 0.000 -0.003 -0.003 ## 79 x9 ~~ x16 0.004 0.003 0.003 0.014 0.014 ## 80 x9 ~~ x6 0.481 0.039 0.039 0.109 0.109 ## 81 x9 ~~ x13r 0.630 0.062 0.062 0.106 0.106 ## 82 x9 ~~ x12 0.473 0.018 0.018 0.099 0.099 ## 83 x9 ~~ x7 0.507 -0.017 -0.017 -0.080 -0.080 ## 84 x9 ~~ x10 0.916 -0.051 -0.051 -0.119 -0.119 ## 85 x9 ~~ x8 2.939 0.089 0.089 0.186 0.186 ## 86 x9 ~~ x14 2.066 -0.040 -0.040 -0.201 -0.201 ## 87 x9 ~~ x19 0.058 -0.009 -0.009 -0.050 -0.050 ## 88 x9 ~~ x21 0.685 0.029 0.029 0.106 0.106 ## 89 x16 ~~ x6 0.031 0.009 0.009 0.021 0.021 ## 90 x16 ~~ x13r 0.049 0.017 0.017 0.024 0.024 ## 91 x16 ~~ x12 0.560 -0.020 -0.020 -0.084 -0.084 ## 92 x16 ~~ x7 0.836 0.022 0.022 0.085 0.085 ## 93 x16 ~~ x10 0.004 0.003 0.003 0.006 0.006 ## 94 x16 ~~ x8 1.439 -0.063 -0.063 -0.107 -0.107 ## 95 x16 ~~ x14 3.275 0.051 0.051 0.203 0.203 ## 96 x16 ~~ x19 0.000 0.001 0.001 0.003 0.003 ## 97 x16 ~~ x21 2.088 -0.053 -0.053 -0.154 -0.154 ## 99 x6 ~~ x12 0.161 0.039 0.039 0.111 0.111 ## 100 x6 ~~ x7 0.256 0.026 0.026 0.067 0.067 ## 101 x6 ~~ x10 0.046 -0.021 -0.021 -0.026 -0.026 ## 102 x6 ~~ x8 0.021 0.014 0.014 0.016 0.016 ## 103 x6 ~~ x14 0.865 -0.067 -0.067 -0.179 -0.179 ## 104 x6 ~~ x19 7.958 1.408 1.408 4.191 4.191 ## 105 x6 ~~ x21 2.959 0.100 0.100 0.193 0.193 ## 106 x13r ~~ x12 0.784 -0.081 -0.081 -0.141 -0.141 ## 107 x13r ~~ x7 0.004 -0.004 -0.004 -0.006 -0.006 ## 108 x13r ~~ x10 0.080 -0.037 -0.037 -0.028 -0.028 ## 109 x13r ~~ x8 1.321 0.146 0.146 0.100 0.100 ## 110 x13r ~~ x14 0.197 0.031 0.031 0.050 0.050 ## 111 x13r ~~ x19 7.958 -0.952 -0.952 -1.728 -1.728 ## 112 x13r ~~ x21 0.058 0.021 0.021 0.025 0.025 ## 113 x12 ~~ x7 0.494 0.082 0.082 0.405 0.405 ## 114 x12 ~~ x10 2.507 -0.136 -0.136 -0.322 -0.322 ## 115 x12 ~~ x8 0.019 -0.007 -0.007 -0.014 -0.014 ## 116 x12 ~~ x14 0.125 0.012 0.012 0.059 0.059 ## 117 x12 ~~ x19 2.831 0.158 0.158 0.894 0.894 ## 118 x12 ~~ x21 2.442 0.045 0.045 0.164 0.164 ## 119 x7 ~~ x10 1.855 0.066 0.066 0.142 0.142 ## 120 x7 ~~ x8 0.029 0.006 0.006 0.013 0.013 ## 121 x7 ~~ x14 0.114 -0.007 -0.007 -0.032 -0.032 ## 122 x7 ~~ x19 3.051 -0.066 -0.066 -0.341 -0.341 ## 123 x7 ~~ x21 3.033 -0.045 -0.045 -0.152 -0.152 ## 124 x10 ~~ x8 0.446 -0.058 -0.058 -0.054 -0.054 ## 125 x10 ~~ x14 0.023 0.007 0.007 0.015 0.015 ## 126 x10 ~~ x19 0.124 0.021 0.021 0.052 0.052 ## 127 x10 ~~ x21 0.288 -0.032 -0.032 -0.052 -0.052 ## 129 x8 ~~ x19 0.000 0.000 0.000 0.001 0.001 ## 130 x8 ~~ x21 5.305 0.134 0.134 0.193 0.193 ## 131 x14 ~~ x19 0.000 0.000 0.000 -0.002 -0.002 ## 132 x14 ~~ x21 4.602 -0.066 -0.066 -0.225 -0.225 ## 133 x19 ~~ x21 0.678 -0.032 -0.032 -0.122 -0.122 ## 134 x19 ~ x21 0.678 -0.079 -0.079 -0.062 -0.062 ## 135 x21 ~ CS 1.460 -0.155 -0.105 -0.113 -0.113 ## 136 x21 ~ PV 1.200 0.082 0.093 0.100 0.100 ## 137 x21 ~ MK 0.003 0.003 0.003 0.004 0.004 ## 138 x21 ~ TS 1.302 -0.057 -0.060 -0.065 -0.065 ## 140 CS ~ x21 1.192 -0.128 -0.188 -0.175 -0.175 ## 145 PV ~ x21 3.038 0.432 0.384 0.358 0.358 ## 150 MK ~ x21 0.432 0.111 0.096 0.090 0.090 ## 155 TS ~ x21 1.366 -0.157 -0.148 -0.138 -0.138 lhs op rhs mi epc sepc.lv sepc.all sepc.nox Model has a good fit Plotting the model library(lavaanPlot) labels &lt;- list(x19 = &quot;SATISFACTION&quot;, x21 = &quot;FUTURE PURCHASE&quot;) lavaanPlot(model = fitSEM2 , node_options = list(shape = &quot;box&quot; , fontname = &quot;Helvetica&quot;) , edge_options = list(color = &quot;grey&quot;) , coefs = TRUE , covs=TRUE , stand=TRUE , sig=.05 , stars=&quot;regress&quot; , labels = labels) We see that satisfaction is the mediator, where the future purchase is the final construct that we want to measure. # now # - summarize the findings # - are the all structural paths in the sem model significant? (if so, which hypotheses are supported?) # - which is the most important determinant of customer satisfaction? (check std. path coefficients and conclude) # - does satisfaction act as a sigificant mediator? (check the sig. of mediating patterns and conclude) # - how much variance in x21 (Likelihood of future purchase) the model explains? (check R^2 associated) 2.4 PLS 2.4.1 Definition of PLS We see that partial least squares can be used to make exploratiry analysis as well as prediction. Where SEM is used in a more confirmatory way of looking into relationships, where you assume some structure. Hence we also assume normality in SEM, this is not the case with PLS. See more information in the following illustration. SEMVSPLS 2.4.2 When to use PLS and when to use SEM PLS if: You have a goal of predicting a target construct The structural model is complex (many constructs and indicators) If the sample size is small, and then again, what is small?? Or if the data is not normally distributed. SEM if: The goal is theory testin or comparison of different theories Error terms requires additional specificatio such as the covariation The structural model is non-recursive The research requires a global goodness-of-fit criterion 2.4.3 Model construction We have two types of variables: Latent: These cannot be directly observed. These come in two classes: Exogenous latent variables: are independent variables Endogenous latent variable: are dependent variables. Manifest: This is a directly observable. These come in two classes: Exogenous: reveal exogenious latent variables Endogenous: reveal endogenious latent variables PLS is based on OLS and dont assume linearity and observations does not have to be independent. 2.4.4 Model Estimation It is called partial least squares as some of the model is changed while others is not changed, hence you change it partially. That comes in different procedures. The iterative process is where you first estimate the outer variables and then you move onwards. 2.4.5 Model evaluation Look at the following criteria: Loadings - items reliability. We want the above 0.7 as they indicate that the constrct explains more than 50% of the indicators variance. Thus proving reliability. Internal reliability. Discriminant validity The significance of the inner coefficients The models ability to explain the endogenous latent variables 2.4.6 Example for the lecture PLS is estimated based upon OLS It is always converging It provides an output although the constructs are poorly measured It works with less assumptions than covariance based SEM It has no overall fit criterion it is very popular expecially in consulting Description of the following example: In epsi_pls.R the purpose was to estimate the EPSI model in a slightly modified form as given below. The only diference to the standard EPSI model was that all questions related to quality aspects were items for an overall Quality construct and not divided into Quality with respect to service (Quality soft ware)and Quality with respect to Product (Quality Hard ware) EPSI If we take a look at the dataset (help(mobi) we can see that there are 24 items all in all. One question is about complaints (CUSCO). Complaints is not part of the model, so we do not include that one. We see that the measurement model is all the relationships between the questions (the yellow boxes) to the constructs. They are called the measurement model, as these are the variables that you can measure. Next step is to formulate the model. In semPLS we use a matrix format and not equations like in Lavaan. We describe the measurement model and the structural model separately and in this case we should end up with two matrices like the following: Mapping questions and latentvariables library(boot) #Bootstrapping of standard deviations to outer loadings and path coefficients library(semPLS) library(psych) # help(mobi) data(mobi) #names(mobi) Before we run the model it is always a good idea to have an overview of the dataset. We can use the function describe from library psych to give this overview. #The dataset summary(mobi) describe(mobi) #from library(psych) ## CUEX1 CUEX2 CUEX3 CUSA1 ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 4.00 ## 1st Qu.: 7.00 1st Qu.: 7.00 1st Qu.: 6.00 1st Qu.: 7.00 ## Median : 8.00 Median : 8.00 Median : 8.00 Median : 8.00 ## Mean : 7.58 Mean : 7.53 Mean : 7.42 Mean : 7.99 ## 3rd Qu.: 8.00 3rd Qu.: 9.00 3rd Qu.: 9.00 3rd Qu.: 9.00 ## Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 ## CUSA2 CUSA3 CUSCO CUSL1 ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 1.00 ## 1st Qu.: 6.00 1st Qu.: 7.00 1st Qu.: 6.00 1st Qu.: 6.00 ## Median : 7.00 Median : 7.00 Median : 7.00 Median : 8.00 ## Mean : 7.13 Mean : 7.32 Mean : 7.07 Mean : 7.45 ## 3rd Qu.: 8.00 3rd Qu.: 8.00 3rd Qu.: 9.00 3rd Qu.:10.00 ## Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 ## CUSL2 CUSL3 IMAG1 IMAG2 ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 1.00 ## 1st Qu.: 3.00 1st Qu.: 7.00 1st Qu.: 7.00 1st Qu.: 7.00 ## Median : 4.00 Median : 8.00 Median : 8.00 Median : 8.00 ## Mean : 4.99 Mean : 7.67 Mean : 7.64 Mean : 7.78 ## 3rd Qu.: 6.75 3rd Qu.:10.00 3rd Qu.: 9.00 3rd Qu.: 9.00 ## Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 ## IMAG3 IMAG4 IMAG5 PERQ1 ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 2.00 ## 1st Qu.: 5.00 1st Qu.: 7.00 1st Qu.: 7.00 1st Qu.: 7.00 ## Median : 7.00 Median : 8.00 Median : 8.00 Median : 8.00 ## Mean : 6.74 Mean : 7.59 Mean : 7.93 Mean : 7.94 ## 3rd Qu.: 8.00 3rd Qu.: 9.00 3rd Qu.: 9.00 3rd Qu.: 9.00 ## Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 ## PERQ2 PERQ3 PERQ4 PERQ5 PERQ6 ## Min. : 1.00 Min. : 1.0 Min. : 1.00 Min. : 3.00 Min. : 1.00 ## 1st Qu.: 6.00 1st Qu.: 7.0 1st Qu.: 7.00 1st Qu.: 7.00 1st Qu.: 7.00 ## Median : 7.00 Median : 8.0 Median : 8.00 Median : 8.00 Median : 8.00 ## Mean : 7.19 Mean : 7.7 Mean : 7.92 Mean : 7.87 Mean : 7.78 ## 3rd Qu.: 8.00 3rd Qu.: 9.0 3rd Qu.: 9.00 3rd Qu.: 9.00 3rd Qu.: 9.00 ## Max. :10.00 Max. :10.0 Max. :10.00 Max. :10.00 Max. :10.00 ## PERQ7 PERV1 PERV2 ## Min. : 1.00 Min. : 1.00 Min. : 1.00 ## 1st Qu.: 7.00 1st Qu.: 5.00 1st Qu.: 6.00 ## Median : 8.00 Median : 6.00 Median : 7.00 ## Mean : 7.59 Mean : 6.16 Mean : 6.92 ## 3rd Qu.: 9.00 3rd Qu.: 8.00 3rd Qu.: 8.00 ## Max. :10.00 Max. :10.00 Max. :10.00 vars n mean sd median trimmed mad min max range skew kurtosis se CUEX1 1 250 7.58 1.62 8 7.68 1.48 1 10 9 -0.663 0.829 0.103 CUEX2 2 250 7.53 1.79 8 7.67 1.48 1 10 9 -0.800 1.447 0.113 CUEX3 3 250 7.42 2.10 8 7.63 1.48 1 10 9 -0.849 0.615 0.133 CUSA1 4 250 7.99 1.23 8 8.03 1.48 4 10 6 -0.220 0.153 0.078 CUSA2 5 250 7.13 1.76 7 7.18 1.48 1 10 9 -0.547 0.558 0.112 CUSA3 6 250 7.32 1.75 7 7.41 1.48 1 10 9 -0.667 0.962 0.110 CUSCO 7 250 7.07 2.27 7 7.29 2.22 1 10 9 -0.710 0.238 0.144 CUSL1 8 250 7.45 2.66 8 7.86 2.96 1 10 9 -0.995 0.077 0.168 CUSL2 9 250 4.99 2.84 4 4.80 2.96 1 10 9 0.545 -0.906 0.180 CUSL3 10 250 7.67 2.22 8 7.96 2.96 1 10 9 -1.059 0.864 0.140 IMAG1 11 250 7.64 1.70 8 7.76 1.48 1 10 9 -0.901 1.689 0.108 IMAG2 12 250 7.78 1.69 8 7.93 1.48 1 10 9 -0.793 0.962 0.107 IMAG3 13 250 6.74 2.13 7 6.91 1.48 1 10 9 -0.793 0.657 0.135 IMAG4 14 250 7.59 1.84 8 7.75 1.48 1 10 9 -0.923 1.321 0.116 IMAG5 15 250 7.93 1.56 8 8.07 1.48 1 10 9 -1.069 2.682 0.098 PERQ1 16 250 7.94 1.42 8 8.04 1.48 2 10 8 -0.720 1.179 0.090 PERQ2 17 250 7.19 1.89 7 7.33 1.48 1 10 9 -0.819 0.729 0.120 PERQ3 18 250 7.70 1.82 8 7.88 1.48 1 10 9 -0.906 0.831 0.115 PERQ4 19 250 7.92 1.65 8 8.11 1.48 1 10 9 -1.070 1.790 0.104 PERQ5 20 250 7.87 1.45 8 7.97 1.48 3 10 7 -0.677 0.558 0.092 PERQ6 21 250 7.78 1.63 8 7.92 1.48 1 10 9 -0.974 1.787 0.103 PERQ7 22 250 7.59 1.84 8 7.78 1.48 1 10 9 -0.959 1.053 0.117 PERV1 23 250 6.16 2.18 6 6.26 1.48 1 10 9 -0.397 -0.214 0.138 PERV2 24 250 6.92 1.84 7 6.98 1.48 1 10 9 -0.645 0.971 0.117 Here we can see that all items apart from CUSL2 has a negative skewness. If skewness is negative then it means that the tail in the distribution is to the left. Less than -1 then highly skewed, between -0,5 and -1 then moderately skewed. Kurtosis is the degree of peakedness in a distribution (how heavy tails). Negative kurtosis is an indicator of a more even distribution across possible outcomes. It can be seen that CUSL2 has a positive skewness (more even distribution) and a negative kurtosis. Answers to this question behaves completely different compared to the rest. The question is also compared to the other questions that are more straightforward to answer. We can plot the distribution of the CUSL1 and 2. hist(mobi$CUSL1) Figure 2.1: Histogram CUSL1 hist(mobi$CUSL2) Figure 2.2: Histogram CUSL2 We see that they are not at all normally distributed. Notice that we are going to build the following model using in R. We see that we are going to use the arrows which are non-dotted. We see that in the package we have image that is the only variable that are not described by other constructs. Now we are going to define the structural model, by describing the varaibles that are linked to them, hence identifying the indicators of each of the latent variables. Now we can ‘draw’ the model. We saw in SEM that we wrote it as a function, here we make it as a matrix. 2.4.6.1 Making structural- and measurement model We relate the constructs and the varaibles in the following. #The structural model EPSIsm &lt;- matrix(c(&quot;IMAGE&quot;, &quot;VALUE&quot; ,&quot;EXP&quot;,&quot;VALUE&quot; ,&quot;QUAL&quot;,&quot;VALUE&quot; ,&quot;VALUE&quot;,&quot;SATISF&quot; ,&quot;SATISF&quot;,&quot;LOYAL&quot; ,&quot;IMAGE&quot;,&quot;LOYAL&quot;),byrow=TRUE,ncol=2) colnames(EPSIsm)=c(&quot;source&quot;,&quot;target&quot;) EPSIsm ## source target ## [1,] &quot;IMAGE&quot; &quot;VALUE&quot; ## [2,] &quot;EXP&quot; &quot;VALUE&quot; ## [3,] &quot;QUAL&quot; &quot;VALUE&quot; ## [4,] &quot;VALUE&quot; &quot;SATISF&quot; ## [5,] &quot;SATISF&quot; &quot;LOYAL&quot; ## [6,] &quot;IMAGE&quot; &quot;LOYAL&quot; #The measurement model EPSImm1 &lt;-matrix(c(&quot;IMAGE&quot;, &quot;IMAG1&quot;,&quot;IMAGE&quot;,&quot;IMAG2&quot;,&quot;IMAGE&quot;,&quot;IMAG3&quot;,&quot;IMAGE&quot;,&quot;IMAG4&quot;),byrow=TRUE,ncol=2) colnames(EPSImm1)=c(&quot;source&quot;,&quot;target&quot;) # EPSImm1 EPSImm2 &lt;-matrix(c(&quot;EXP&quot;, &quot;CUEX1&quot;,&quot;EXP&quot;,&quot;CUEX2&quot;,&quot;EXP&quot;,&quot;CUEX3&quot;),byrow=TRUE,ncol=2) colnames(EPSImm2)=c(&quot;source&quot;,&quot;target&quot;) EPSImm3 &lt;-matrix(c(&quot;QUAL&quot;, &quot;PERQ1&quot;,&quot;QUAL&quot;,&quot;PERQ2&quot;,&quot;QUAL&quot;,&quot;PERQ3&quot;,&quot;QUAL&quot;,&quot;PERQ4&quot;,&quot;QUAL&quot;,&quot;PERQ5&quot;, &quot;QUAL&quot;,&quot;PERQ6&quot;,&quot;QUAL&quot;,&quot;PERQ7&quot;),byrow=TRUE,ncol=2) colnames(EPSImm3)=c(&quot;source&quot;,&quot;target&quot;) head(EPSImm3,7) EPSImm4 &lt;-matrix(c(&quot;VALUE&quot;, &quot;PERV1&quot;,&quot;VALUE&quot;,&quot;PERV2&quot;),byrow=TRUE,ncol=2) colnames(EPSImm4)=c(&quot;source&quot;,&quot;target&quot;) EPSImm5 &lt;-matrix(c(&quot;SATISF&quot;, &quot;CUSA1&quot;,&quot;SATISF&quot;,&quot;CUSA2&quot;,&quot;SATISF&quot;,&quot;CUSA3&quot;),byrow=TRUE,ncol=2) colnames(EPSImm5)=c(&quot;source&quot;,&quot;target&quot;) EPSImm6 &lt;-matrix(c(&quot;LOYAL&quot;, &quot;CUSL1&quot;,&quot;LOYAL&quot;,&quot;CUSL2&quot;,&quot;LOYAL&quot;,&quot;CUSL3&quot;),byrow=TRUE,ncol=2) colnames(EPSImm6)=c(&quot;source&quot;,&quot;target&quot;) ## source target ## [1,] &quot;QUAL&quot; &quot;PERQ1&quot; ## [2,] &quot;QUAL&quot; &quot;PERQ2&quot; ## [3,] &quot;QUAL&quot; &quot;PERQ3&quot; ## [4,] &quot;QUAL&quot; &quot;PERQ4&quot; ## [5,] &quot;QUAL&quot; &quot;PERQ5&quot; ## [6,] &quot;QUAL&quot; &quot;PERQ6&quot; ## [7,] &quot;QUAL&quot; &quot;PERQ7&quot; EPSImm &lt;-rbind(EPSImm1,EPSImm2,EPSImm3,EPSImm4,EPSImm5,EPSImm6) EPSImm ## source target ## [1,] &quot;IMAGE&quot; &quot;IMAG1&quot; ## [2,] &quot;IMAGE&quot; &quot;IMAG2&quot; ## [3,] &quot;IMAGE&quot; &quot;IMAG3&quot; ## [4,] &quot;IMAGE&quot; &quot;IMAG4&quot; ## [5,] &quot;EXP&quot; &quot;CUEX1&quot; ## [6,] &quot;EXP&quot; &quot;CUEX2&quot; ## [7,] &quot;EXP&quot; &quot;CUEX3&quot; ## [8,] &quot;QUAL&quot; &quot;PERQ1&quot; ## [9,] &quot;QUAL&quot; &quot;PERQ2&quot; ## [10,] &quot;QUAL&quot; &quot;PERQ3&quot; ## [11,] &quot;QUAL&quot; &quot;PERQ4&quot; ## [12,] &quot;QUAL&quot; &quot;PERQ5&quot; ## [13,] &quot;QUAL&quot; &quot;PERQ6&quot; ## [14,] &quot;QUAL&quot; &quot;PERQ7&quot; ## [15,] &quot;VALUE&quot; &quot;PERV1&quot; ## [16,] &quot;VALUE&quot; &quot;PERV2&quot; ## [17,] &quot;SATISF&quot; &quot;CUSA1&quot; ## [18,] &quot;SATISF&quot; &quot;CUSA2&quot; ## [19,] &quot;SATISF&quot; &quot;CUSA3&quot; ## [20,] &quot;LOYAL&quot; &quot;CUSL1&quot; ## [21,] &quot;LOYAL&quot; &quot;CUSL2&quot; ## [22,] &quot;LOYAL&quot; &quot;CUSL3&quot; 2.4.6.2 Now we can estimate the model estimating and looking at the matrices reflecting the relationships. #The whole model EPSI &lt;- plsm(data=mobi ,strucmod=EPSIsm ,measuremod=EPSImm) EPSI[[&quot;D&quot;]] #The structural model (inner relations) EPSI[[&quot;M&quot;]] #The measurement model ## EXP IMAGE QUAL VALUE SATISF LOYAL ## EXP 0 0 0 1 0 0 ## IMAGE 0 0 0 1 0 1 ## QUAL 0 0 0 1 0 0 ## VALUE 0 0 0 0 1 0 ## SATISF 0 0 0 0 0 1 ## LOYAL 0 0 0 0 0 0 ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 1 0 0 0 0 0 ## CUEX2 1 0 0 0 0 0 ## CUEX3 1 0 0 0 0 0 ## IMAG1 0 1 0 0 0 0 ## IMAG2 0 1 0 0 0 0 ## IMAG3 0 1 0 0 0 0 ## IMAG4 0 1 0 0 0 0 ## PERQ1 0 0 1 0 0 0 ## PERQ2 0 0 1 0 0 0 ## PERQ3 0 0 1 0 0 0 ## PERQ4 0 0 1 0 0 0 ## PERQ5 0 0 1 0 0 0 ## PERQ6 0 0 1 0 0 0 ## PERQ7 0 0 1 0 0 0 ## PERV1 0 0 0 1 0 0 ## PERV2 0 0 0 1 0 0 ## CUSA1 0 0 0 0 1 0 ## CUSA2 0 0 0 0 1 0 ## CUSA3 0 0 0 0 1 0 ## CUSL1 0 0 0 0 0 1 ## CUSL2 0 0 0 0 0 1 ## CUSL3 0 0 0 0 0 1 #Estimation of model epsi &lt;- sempls(model=EPSI,data=mobi) epsi ## All 250 observations are valid. ## Converged after 8 iterations. ## Tolerance: 1e-07 ## Scheme: centroid ## Path Estimate ## lam_1_1 EXP -&gt; CUEX1 0.786 ## lam_1_2 EXP -&gt; CUEX2 0.583 ## lam_1_3 EXP -&gt; CUEX3 0.682 ## lam_2_1 IMAGE -&gt; IMAG1 0.751 ## lam_2_2 IMAGE -&gt; IMAG2 0.593 ## lam_2_3 IMAGE -&gt; IMAG3 0.628 ## lam_2_4 IMAGE -&gt; IMAG4 0.821 ## lam_3_1 QUAL -&gt; PERQ1 0.796 ## lam_3_2 QUAL -&gt; PERQ2 0.626 ## lam_3_3 QUAL -&gt; PERQ3 0.789 ## lam_3_4 QUAL -&gt; PERQ4 0.762 ## lam_3_5 QUAL -&gt; PERQ5 0.764 ## lam_3_6 QUAL -&gt; PERQ6 0.769 ## lam_3_7 QUAL -&gt; PERQ7 0.791 ## lam_4_1 VALUE -&gt; PERV1 0.903 ## lam_4_2 VALUE -&gt; PERV2 0.939 ## lam_5_1 SATISF -&gt; CUSA1 0.784 ## lam_5_2 SATISF -&gt; CUSA2 0.845 ## lam_5_3 SATISF -&gt; CUSA3 0.865 ## lam_6_1 LOYAL -&gt; CUSL1 0.825 ## lam_6_2 LOYAL -&gt; CUSL2 0.209 ## lam_6_3 LOYAL -&gt; CUSL3 0.911 ## beta_1_4 EXP -&gt; VALUE 0.036 ## beta_2_4 IMAGE -&gt; VALUE 0.232 ## beta_3_4 QUAL -&gt; VALUE 0.403 ## beta_4_5 VALUE -&gt; SATISF 0.610 ## beta_2_6 IMAGE -&gt; LOYAL 0.140 ## beta_5_6 SATISF -&gt; LOYAL 0.563 We see that we get the estimates, which are the coefficients. Just as in the covariance based approach it is important to have a good measurement model and the criteria are the same. Firstly we have to make sure that the items are good indicators for the latent constructs. Here we look at the loadings. The loadings are produced with the following line: # Evaluation of estimated model plsLoadings(epsi) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 0.79 . . . . . ## CUEX2 0.58 . . . . . ## CUEX3 0.68 . . . . . ## IMAG1 . 0.75 . . . . ## IMAG2 . 0.59 0.49 . . . ## IMAG3 . 0.63 . . . . ## IMAG4 . 0.82 . . . . ## PERQ1 . . 0.80 . 0.68 . ## PERQ2 . . 0.63 . . . ## PERQ3 . . 0.79 . 0.65 . ## PERQ4 . . 0.76 . . . ## PERQ5 . . 0.76 . . . ## PERQ6 . . 0.77 . . . ## PERQ7 . . 0.79 . 0.70 . ## PERV1 . . . 0.90 . . ## PERV2 . . . 0.94 . . ## CUSA1 . . 0.64 . 0.78 . ## CUSA2 . . . . 0.85 . ## CUSA3 . . . . 0.86 . ## CUSL1 . . . . . 0.83 ## CUSL2 . . . . . 0.21 ## CUSL3 . . . . . 0.91 We see that expectations are influenced by CUEX, and Image is defined by image etc. Although it is rather interesting to see that the satisfaction is defined by several variable. When we look on the loyalty, we see that question no. 2 (CUSL2) that there is a very low relationship. The rule of thumb, is that the relationship needs to be at least 0.7. So we also have other variables that are questionable. 2.4.6.3 Measuring quality of the constructs We can can compute the composite reliability by the following: #Composite reliability dgrho(epsi) ## Dillon-Goldstein&#39;s rho reflective MVs ## EXP 0.73 3 ## IMAGE 0.79 4 ## QUAL 0.90 7 ## VALUE 0.92 2 ## SATISF 0.87 3 ## LOYAL 0.72 3 We want the coefficients be higher than 0.8 (rule of thumb), hence we see that loyalty for instance is low. that is because of the question no. 2, that was very weak. # AVE is average of communality for a construct communality(epsi) ## communality reflective MVs ## EXP 0.47 3 ## IMAGE 0.50 4 ## QUAL 0.58 7 ## VALUE 0.85 2 ## SATISF 0.69 3 ## LOYAL 0.52 3 ## ## Average communality: 0.58 now we can look at the inner relationtions. #Inner relations pC &lt;- pathCoeff(epsi) print(pC,abbreviate=TRUE,minlength=3) ## EXP IMA QUA VAL SAT LOY ## EXP . . . 0.036 . . ## IMA . . . 0.232 . 0.140 ## QUA . . . 0.403 . . ## VAL . . . . 0.610 . ## SAT . . . . . 0.563 ## LOY . . . . . . we see that the coefficients are standardized. #The total effects tE&lt;- totalEffects(epsi) print(tE,abbreviate=TRUE,minlength=3) ## EXP IMA QUA VAL SAT LOY ## EXP . . . 0.036 0.022 0.012 ## IMA . . . 0.232 0.141 0.219 ## QUA . . . 0.403 0.246 0.139 ## VAL . . . . 0.610 0.344 ## SAT . . . . . 0.563 ## LOY . . . . . . The total effect includes the indirect effects, e.g., going between latent variables. #R2 rSquared(epsi) ## R-squared ## EXP . ## IMAGE . ## QUAL . ## VALUE 0.38 ## SATISF 0.37 ## LOYAL 0.44 We see that the \\(R^2\\) is rather low. It is common to say, that it needs to be at least 0.65, i.e., 65%. 2.4.6.4 Are the coefficients significant? We can run a bootstrap to assess whether the coefficients are significant. #Bootstrapping - estimation of standard errors set.seed(123) epsiBoot &lt;- bootsempls(epsi,nboot=500,start=&quot;ones&quot;,verbose=FALSE) epsiBoot ## Call: bootsempls(object = epsi, nboot = 500, start = &quot;ones&quot;, verbose = FALSE) ## ## Estimate Bias Std.Error ## EXP -&gt; CUEX1 0.7865 -0.015459 0.0716 ## EXP -&gt; CUEX2 0.5827 -0.019156 0.1474 ## EXP -&gt; CUEX3 0.6817 -0.003128 0.1026 ## IMAGE -&gt; IMAG1 0.7505 -0.002769 0.0417 ## IMAGE -&gt; IMAG2 0.5927 -0.006763 0.0764 ## IMAGE -&gt; IMAG3 0.6281 -0.002382 0.0632 ## IMAGE -&gt; IMAG4 0.8215 -0.001675 0.0355 ## QUAL -&gt; PERQ1 0.7962 -0.000784 0.0264 ## QUAL -&gt; PERQ2 0.6256 -0.006429 0.0539 ## QUAL -&gt; PERQ3 0.7893 -0.000843 0.0306 ## QUAL -&gt; PERQ4 0.7622 -0.001921 0.0467 ## QUAL -&gt; PERQ5 0.7644 -0.000716 0.0342 ## QUAL -&gt; PERQ6 0.7694 0.001351 0.0582 ## QUAL -&gt; PERQ7 0.7906 0.001880 0.0295 ## VALUE -&gt; PERV1 0.9035 -0.001996 0.0208 ## VALUE -&gt; PERV2 0.9386 0.000559 0.0071 ## SATISF -&gt; CUSA1 0.7841 -0.003195 0.0347 ## SATISF -&gt; CUSA2 0.8451 0.000466 0.0230 ## SATISF -&gt; CUSA3 0.8648 0.001230 0.0164 ## LOYAL -&gt; CUSL1 0.8253 -0.005565 0.0420 ## LOYAL -&gt; CUSL2 0.2087 0.001986 0.1127 ## LOYAL -&gt; CUSL3 0.9107 -0.002258 0.0138 ## EXP -&gt; VALUE 0.0356 0.019026 0.0822 ## IMAGE -&gt; VALUE 0.2317 -0.005509 0.1146 ## QUAL -&gt; VALUE 0.4035 -0.004219 0.1216 ## VALUE -&gt; SATISF 0.6105 -0.001294 0.0552 ## IMAGE -&gt; LOYAL 0.1396 0.013019 0.0762 ## SATISF -&gt; LOYAL 0.5635 -0.007305 0.0873 E.g., we see that the relationship between the expectations (EXP) and the value (VALUE) is rather low and there does not appear to be any relationship. # Structural model coefficients parallelplot(epsiBoot,reflinesAt=0, alpha=0.8, varnames=attr(epsiBoot$t,&quot;path&quot;)[23:28]) We see that the dotted line = 0. This gives an idea of the stability of the estimation that we have done. #Constructing observations plsWeights(epsi) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 0.56 . . . . . ## CUEX2 0.34 . . . . . ## CUEX3 0.52 . . . . . ## IMAG1 . 0.36 . . . . ## IMAG2 . 0.28 . . . . ## IMAG3 . 0.31 . . . . ## IMAG4 . 0.45 . . . . ## PERQ1 . . 0.20 . . . ## PERQ2 . . 0.13 . . . ## PERQ3 . . 0.20 . . . ## PERQ4 . . 0.17 . . . ## PERQ5 . . 0.20 . . . ## PERQ6 . . 0.17 . . . ## PERQ7 . . 0.24 . . . ## PERV1 . . . 0.48 . . ## PERV2 . . . 0.60 . . ## CUSA1 . . . . 0.35 . ## CUSA2 . . . . 0.38 . ## CUSA3 . . . . 0.47 . ## CUSL1 . . . . . 0.47 ## CUSL2 . . . . . 0.12 ## CUSL3 . . . . . 0.64 These are the factor scores. #Standardised observations for (constructs) epsi$factor_scores densityplot(epsi) densityplot(epsi,use=&quot;prediction&quot;) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## [1,] -0.65829 -1.4798 -1.60130 -2.19569 -1.3198 -0.740413 ## [2,] 1.70506 1.7653 1.58015 1.85556 1.3705 0.999667 ## [3,] -0.40880 -1.0365 -0.61002 0.21417 -0.1089 -0.579037 ## [4,] -0.33339 -0.0201 -0.30737 -0.88010 1.9074 1.085356 ## [5,] 0.68649 0.7122 0.99610 -0.33296 0.9417 0.462692 ## [6,] 1.01463 0.8530 1.02713 1.85556 -0.1089 1.042512 ## [7,] -1.15401 -1.0338 -0.03974 -0.22849 0.3740 0.059495 ## [8,] -2.79085 0.2982 -0.31383 -0.22849 -0.6071 -0.252149 ## [9,] -0.40880 -0.4310 -0.50772 -0.22849 -0.3928 -0.536193 ## [10,] -1.05990 -0.3383 -0.32752 -0.77562 -1.3584 -0.074283 ## [11,] -0.35078 0.1237 0.19910 0.21417 0.2445 0.505537 ## [12,] 1.70506 0.3354 0.30195 -0.22849 0.1055 -1.336101 ## [13,] 0.24554 0.5611 0.40540 -0.55429 0.1055 -0.069039 ## [14,] 1.95455 1.9300 1.57784 0.53997 0.9417 1.085356 ## [15,] -0.73695 -0.2513 0.00808 -1.54408 -0.3232 0.113448 ## [16,] -0.60352 0.6913 1.13277 -0.89247 -0.0394 0.328293 ## [17,] -0.11999 0.7200 0.53090 0.74893 0.3894 0.999667 ## [18,] -1.10248 -1.9176 -0.78280 0.21417 -1.0745 -2.227563 ## [19,] 0.87795 0.9191 0.64648 0.43550 0.9263 0.950957 ## [20,] -1.35197 -0.9063 -0.34353 0.21417 -0.1630 -0.557304 ## [21,] -1.44932 0.4152 0.19472 0.21417 0.5883 0.440959 ## [22,] 0.12624 -0.1450 -0.14689 -0.88010 -1.5728 -0.939391 ## [23,] -0.79173 -0.3704 -0.74790 -0.22849 -0.9297 -0.401794 ## [24,] -0.15932 -0.0598 -0.03592 1.30843 0.3740 0.355270 ## [25,] -0.06197 -0.3039 -1.44458 -1.32275 -2.1638 -2.850228 ## [26,] 0.76190 1.9300 1.78546 0.76130 1.4787 1.256735 ## [27,] 0.37898 1.0040 0.07132 0.21417 -0.5376 -0.358949 ## [28,] 0.87795 -1.3138 -2.18045 1.85556 -0.5376 0.762604 ## [29,] -0.75564 -0.0598 -0.21519 0.53997 -0.1089 0.371138 ## [30,] 0.91403 1.6382 1.41095 1.41291 1.6389 1.256735 ## [31,] -0.56418 0.0913 0.61777 1.08710 0.6579 0.688024 ## [32,] -0.65829 -0.3704 -0.21575 -0.88010 -1.0745 -1.583166 ## [33,] -1.02707 0.7861 -0.36333 0.74893 -0.2846 0.956823 ## [34,] 0.33965 0.7387 1.12529 0.43550 0.5575 0.462692 ## [35,] 0.99724 -0.4123 -2.96841 -3.06862 -2.8610 -3.076182 ## [36,] 1.57162 0.9982 0.22322 0.43550 -0.3928 -0.111884 ## [37,] -0.10130 -0.4310 -0.05373 -1.32275 -1.4666 -2.388940 ## [38,] -0.56093 -0.6465 -0.35186 -0.65877 -0.5376 -0.294371 ## [39,] -0.06197 0.4681 1.14268 1.30843 0.9958 1.052514 ## [40,] -0.15932 -0.4365 -0.10852 0.31864 -0.1089 0.795447 ## [41,] 1.95455 -0.0413 0.98504 0.76130 1.4246 0.688024 ## [42,] 0.47633 0.2825 0.50878 0.43550 0.4281 0.585361 ## [43,] -1.60145 1.0118 1.29648 1.41291 1.3705 0.999667 ## [44,] -0.06197 -0.7471 -0.47749 -1.10142 -0.1243 0.000782 ## [45,] 1.06941 1.6382 1.54212 1.41291 1.9074 0.859402 ## [46,] 0.87795 0.8335 1.47610 0.43550 0.6579 -1.969874 ## [47,] 0.68649 0.3828 0.45251 -1.54408 -0.3232 -0.756281 ## [48,] -0.71631 -0.1263 0.09735 0.53997 0.1596 0.279583 ## [49,] -0.68023 -2.5138 -2.28057 -1.64856 -1.9108 -1.846099 ## [50,] -0.67373 -1.6642 0.41701 -2.74282 -1.3043 -0.933525 ## [51,] 0.87795 1.0681 0.43575 -0.44981 0.8027 0.462692 ## [52,] -0.69762 0.0858 -0.78670 -0.55429 -0.6071 -0.579037 ## [53,] 0.05408 -0.0124 0.33301 0.76130 0.3894 0.510780 ## [54,] 1.07266 -0.7731 -0.06744 0.43550 -0.1089 0.236738 ## [55,] -0.90777 -0.2057 -0.25733 0.43550 -0.3232 -0.316104 ## [56,] 0.22685 -0.6997 -0.10895 0.43550 -0.1089 -0.401794 ## [57,] -0.54224 -0.5585 -1.89561 -0.88010 -2.3008 -0.993344 ## [58,] 0.09016 0.7732 0.95068 0.76130 0.6579 0.151049 ## [59,] 1.07266 1.2802 0.75825 1.08710 1.4787 1.128201 ## [60,] -1.06315 -0.1263 -0.10796 -0.33296 -0.1089 -0.810235 ## [61,] -0.82911 -0.8773 -1.07472 -1.42723 -1.5882 -1.271523 ## [62,] -0.79173 -0.1873 0.37609 0.21417 0.1750 0.279583 ## [63,] 0.78384 0.0197 -0.01059 0.98263 0.1596 0.220871 ## [64,] -0.50616 -1.3573 -1.28046 -0.55429 -0.8215 0.054251 ## [65,] 0.53436 0.1843 -0.07625 -0.00716 -0.6612 -2.319118 ## [66,] 0.12950 0.5134 0.74117 -1.21827 -0.0934 -0.868947 ## [67,] 0.76190 -0.1164 0.87262 0.21417 0.1055 -0.391791 ## [68,] 0.37898 -0.3704 -0.62625 -0.88010 -2.7683 -2.120763 ## [69,] 0.09341 -0.0601 0.26260 -0.00716 0.1596 0.456827 ## [70,] 0.18752 -0.2060 0.28814 -0.33296 -0.6071 -0.069039 ## [71,] -0.15932 0.2031 0.56473 0.21417 0.4435 0.220871 ## [72,] 1.95455 0.7122 1.24284 0.21417 0.7815 -0.380682 ## [73,] 0.51242 0.5478 1.50449 -3.06862 1.9074 -3.076182 ## [74,] -1.00188 -1.2827 -0.94312 0.21417 -1.1440 -0.810235 ## [75,] 1.26087 -0.1583 -0.21957 0.21417 -1.6790 -0.321348 ## [76,] -0.52030 -2.0973 -0.44867 -1.42723 -0.8756 -0.385926 ## [77,] 1.41625 -0.3704 0.05861 0.76130 -0.0702 0.440959 ## [78,] -1.63753 -0.6012 -1.23851 -0.88010 -0.3773 0.591226 ## [79,] -0.25343 0.1843 -0.11684 -0.33296 -0.3232 0.236738 ## [80,] -0.21734 -0.4024 -0.23630 0.21417 -0.1784 0.022516 ## [81,] 0.97530 0.5611 1.09403 0.21417 0.5883 0.987936 ## [82,] -1.00188 0.3299 -0.17202 0.53997 -0.4835 0.413982 ## [83,] 0.37898 0.6593 0.81923 0.53997 1.4091 1.171046 ## [84,] -0.65829 -0.6810 0.03098 -0.00716 -0.5917 -0.606014 ## [85,] -0.00394 -0.5640 -0.13766 -0.00716 -0.3773 -0.718680 ## [86,] -1.44282 0.0671 0.45503 -0.12401 0.0360 -0.541436 ## [87,] 1.22478 0.4813 0.77991 1.08710 1.1253 1.042512 ## [88,] 1.10874 0.7067 0.92307 1.30843 0.4281 0.676915 ## [89,] 0.18752 0.1366 0.08118 0.53997 0.6579 0.999667 ## [90,] -0.40880 0.5266 0.99678 0.53997 0.3045 0.655182 ## [91,] 0.37898 1.2297 1.19211 0.76130 0.6733 0.762604 ## [92,] -0.06197 -0.1395 -0.64099 -0.00716 0.1055 -0.246283 ## [93,] 0.32096 0.6405 0.71410 0.53997 0.8568 0.709758 ## [94,] -0.81367 -0.4178 -0.16783 0.10969 -0.1784 -0.461613 ## [95,] -0.79173 -2.6513 -3.01981 -1.42723 -3.1062 -3.204715 ## [96,] 0.53111 1.5533 0.72298 1.41291 0.9109 0.966825 ## [97,] -0.19540 0.2185 0.69350 0.98263 0.6424 0.703892 ## [98,] -0.35078 -0.5163 -0.52050 0.21417 -0.3232 -0.069039 ## [99,] -1.09923 -0.9067 -0.52281 -0.22849 -0.1089 -0.938768 ## [100,] 0.09016 1.4923 1.23873 0.21417 0.8568 0.810692 ## [101,] -0.19540 -0.0413 0.02266 0.53997 -0.5376 -0.761525 ## [102,] 0.12950 0.2505 -0.09964 0.21417 0.1055 -0.230415 ## [103,] 0.18752 -0.2196 0.07143 0.53997 0.3740 0.822423 ## [104,] -0.65829 -0.5350 -0.76165 -0.00716 0.5130 -0.209304 ## [105,] 0.28487 1.1500 0.85278 0.76130 0.7274 1.171046 ## [106,] -1.11663 -2.0055 -0.24111 -0.88010 -1.3584 -0.713437 ## [107,] 1.95455 -0.8435 0.20577 1.20395 0.9417 0.108205 ## [108,] 0.03214 0.3170 0.00211 -0.67114 0.6424 0.398114 ## [109,] -0.51705 0.8560 -0.09534 0.76130 0.1596 1.085356 ## [110,] 0.57044 0.2505 0.27038 0.53997 1.4787 1.213890 ## [111,] -0.36947 -1.0069 0.26053 -0.22849 0.3894 0.242604 ## [112,] 0.97530 1.0650 0.50714 0.86578 -0.0548 0.709758 ## [113,] 1.35822 0.8526 0.92307 0.98263 1.1407 1.342424 ## [114,] 0.87795 1.2140 1.78546 0.76130 1.4787 1.085356 ## [115,] 0.41506 1.1629 1.29687 0.76130 0.9109 0.816558 ## [116,] 0.22035 0.4762 -0.85286 -1.54408 -1.0745 0.698026 ## [117,] -1.75228 1.4738 0.87712 1.85556 1.3396 1.171046 ## [118,] 1.95455 1.9300 1.27715 0.76130 0.7274 0.999667 ## [119,] 1.57162 0.8052 1.24108 1.30843 0.6424 0.472695 ## [120,] 0.82317 1.5059 0.45466 0.76130 1.3396 0.762604 ## [121,] 1.22478 0.7920 0.43695 0.53997 0.2812 -0.579037 ## [122,] 0.22685 -0.7979 -0.46010 -1.86988 -1.4974 -1.566676 ## [123,] 1.45558 1.9300 1.78546 1.85556 1.9074 1.256735 ## [124,] -0.42750 -1.4423 -1.39506 -0.55429 -1.3043 0.193894 ## [125,] -0.63960 -0.6997 -1.70576 -0.00716 -0.5376 -0.471615 ## [126,] -0.25343 -2.1269 0.18189 -0.88010 -0.6766 -2.114898 ## [127,] 1.51360 1.5720 1.06356 1.30843 1.3551 1.052514 ## [128,] -0.84975 -0.5030 -0.00622 -0.55429 0.1055 -0.181705 ## [129,] -0.06197 -0.4877 -0.24736 -0.88010 -0.8756 -0.832590 ## [130,] -0.15932 0.3964 -0.20758 0.21417 -0.1089 -0.111884 ## [131,] 0.49827 -0.3509 -0.61031 0.74893 -0.1551 0.377003 ## [132,] 0.14819 -1.4047 -0.43875 -1.42723 -1.3584 -1.170588 ## [133,] -2.02500 -1.7393 -2.50897 -0.88010 -1.6423 1.342424 ## [134,] 0.45764 1.0176 1.11052 1.20395 -0.1938 0.419848 ## [135,] -0.90777 -0.1293 -0.24584 -0.00716 -0.1089 -0.224550 ## [136,] -0.82781 0.1711 0.60450 0.76130 0.3740 1.128201 ## [137,] 1.95455 1.9300 1.78546 1.85556 1.9074 1.342424 ## [138,] 0.12950 0.3170 0.24391 0.31864 0.9417 0.987936 ## [139,] 0.91728 0.7070 0.96680 -0.46219 1.6389 0.220871 ## [140,] 0.57044 0.3828 0.14135 1.19158 0.3740 0.591226 ## [141,] 0.28487 1.3276 0.58067 0.33102 0.9263 0.688024 ## [142,] 0.12950 0.4152 0.16474 0.21417 0.1055 1.342424 ## [143,] -1.75683 -1.3305 -1.04426 -1.10142 -1.9262 -1.089036 ## [144,] 0.41506 1.3088 -0.07526 -0.56667 0.3740 0.591226 ## [145,] 1.95455 1.9300 1.78546 1.85556 1.9074 0.999667 ## [146,] 0.87795 -0.9217 0.92307 1.30843 0.9417 -0.155351 ## [147,] -0.66154 -1.3809 -2.12061 -1.20590 -1.9108 -1.497477 ## [148,] -1.69556 -0.2854 0.30633 -0.00716 0.1055 -0.316104 ## [149,] -2.12106 -1.1132 -1.91094 -0.88010 -0.0161 -1.336101 ## [150,] 0.91403 0.3678 0.49049 -0.22849 -0.1630 1.085356 ## [151,] 0.87795 1.2481 1.03370 0.76130 0.4435 1.171046 ## [152,] 0.37898 0.3964 0.06068 -0.22849 0.3740 0.236738 ## [153,] -0.33339 0.5778 -0.42720 -1.43960 -0.4835 0.865268 ## [154,] 1.95455 -1.4609 0.47433 0.74893 0.8181 1.171046 ## [155,] 0.53111 0.1391 0.11504 -0.00716 0.1055 -0.444638 ## [156,] 0.97530 0.2505 -0.00622 -0.00716 -0.1784 -0.053171 ## [157,] 0.37898 -0.2248 -0.57800 0.21417 -0.1089 -1.003969 ## [158,] -0.50616 0.9511 0.44244 0.76130 0.6579 0.762604 ## [159,] -1.81355 -1.8664 -1.30324 -0.77562 -0.6612 -0.735170 ## [160,] 0.53436 0.2031 -0.01327 0.33102 0.1596 0.456827 ## [161,] 0.28162 1.2140 0.20117 0.53997 -0.1089 0.377003 ## [162,] 0.18752 -0.0789 -0.13838 -0.55429 -0.1243 0.215627 ## [163,] 0.12950 0.3170 0.75727 0.31864 0.9263 0.779579 ## [164,] -0.60027 -1.2224 -0.80560 -0.21611 -1.6655 -1.158857 ## [165,] 0.91403 -0.3171 -2.45521 -2.84730 -2.0402 0.999667 ## [166,] -0.47333 0.3115 0.78246 -0.22849 0.9109 -1.336101 ## [167,] 1.26087 1.3088 1.45006 0.21417 0.3740 0.999667 ## [168,] 0.22360 -1.1904 -0.52244 0.31864 -0.8756 0.295451 ## [169,] -1.54668 0.4469 -0.27241 0.09732 -0.3232 0.236738 ## [170,] -0.02263 -1.4641 -1.47140 -0.22849 -0.3232 0.059495 ## [171,] -0.00719 -0.8266 -1.01140 -0.33296 -0.8756 -0.171703 ## [172,] 0.87795 -1.6642 1.58197 1.85556 0.9109 1.042512 ## [173,] -0.40880 0.0568 0.38275 0.53997 0.1596 0.526648 ## [174,] 0.18752 -0.6302 -0.06193 -0.88010 -1.1981 -0.456369 ## [175,] 0.62846 1.1820 0.85617 0.09732 1.1561 -0.466994 ## [176,] 0.91403 0.5799 0.14366 0.43550 0.6733 -1.079033 ## [177,] -0.06197 0.1994 0.03011 1.85556 1.1253 0.278961 ## [178,] 1.03332 0.4626 0.25433 0.21417 0.3199 0.371138 ## [179,] 0.33965 -0.3704 -0.46029 0.21417 0.5342 1.342424 ## [180,] 0.12950 0.6883 -0.89949 -0.00716 -0.1089 -0.756281 ## [181,] 0.14819 0.0064 -0.00550 -0.33296 -0.1089 -0.224550 ## [182,] -1.02707 -0.6486 -2.65563 -1.85751 -1.0900 -0.257392 ## [183,] 1.38016 1.9300 0.91968 0.09732 0.1596 0.956823 ## [184,] 0.37898 0.3964 0.12989 0.76130 0.1055 -0.982235 ## [185,] -1.25461 -0.4975 -1.26909 -0.33296 -1.6577 -1.223435 ## [186,] 1.45558 1.6194 1.57784 1.85556 1.9074 1.342424 ## [187,] -0.60352 -0.1075 0.96285 0.76130 -0.0548 0.371138 ## [188,] -3.41236 -1.2360 -2.77382 -3.06862 -2.6466 -0.450504 ## [189,] -1.25461 -1.1184 -1.64307 -0.55429 -0.8756 -0.938768 ## [190,] 0.28162 0.7732 0.26830 0.21417 -0.1089 0.505537 ## [191,] 1.70506 -0.0997 -0.57117 -1.76541 1.0866 0.462692 ## [192,] 0.51242 1.9300 1.39149 1.19158 0.8355 -0.424149 ## [193,] 0.91728 1.3091 1.43807 -0.00716 0.8027 -0.424149 ## [194,] 1.95455 -0.7047 1.41928 1.52976 0.8722 1.042512 ## [195,] -0.60027 -0.2377 -0.46264 -0.77562 -0.8910 -0.993344 ## [196,] 0.57369 -1.4419 -1.38590 -0.88010 -1.1054 -0.734548 ## [197,] 1.95455 0.6167 1.78546 0.74893 1.9074 1.128201 ## [198,] 1.95455 1.0681 1.29200 1.85556 1.3705 1.042512 ## [199,] 0.12950 -0.0124 0.01314 -0.55429 -0.3773 0.709758 ## [200,] 1.16676 0.0671 0.24441 -0.00716 0.9417 0.526648 ## [201,] 0.37898 0.3964 0.61394 -0.22849 0.4281 -0.542681 ## [202,] -0.21734 0.1840 0.56187 0.43550 1.0557 0.762604 ## [203,] -0.31145 -1.8757 -1.95653 -1.42723 0.3740 -0.032060 ## [204,] -0.88908 -2.3315 -1.13819 -1.32275 -1.3584 0.408117 ## [205,] -0.06197 0.4118 -0.97584 -0.88010 -0.0548 0.591226 ## [206,] -1.23592 -1.4157 -0.61765 0.21417 -1.6964 -3.204715 ## [207,] 0.57044 0.4759 0.53694 1.30843 0.3585 1.342424 ## [208,] -0.11999 0.1843 0.62134 0.86578 1.1020 0.645180 ## [209,] -0.67698 0.5103 0.08847 0.21417 -0.1784 0.456827 ## [210,] -0.73695 0.2181 0.15535 1.20395 0.9109 0.113448 ## [211,] -0.15932 -0.9404 -0.94312 0.21417 -0.6458 -0.632991 ## [212,] -0.40880 0.4438 0.01927 0.21417 -0.3928 -1.293878 ## [213,] -0.98644 -0.6776 -0.37564 -0.99695 -0.8061 -0.138861 ## [214,] -0.84975 -0.0601 -0.45699 1.08710 0.1596 0.285449 ## [215,] -1.69556 -1.0390 -1.79154 -0.11164 -1.1595 -0.987479 ## [216,] -2.48334 -1.3570 -1.07146 -1.21827 -0.2170 -2.045076 ## [217,] -1.40674 0.3964 0.02815 0.76130 -0.1243 1.171046 ## [218,] 0.97530 0.9511 0.74762 -0.88010 0.8722 0.532514 ## [219,] 0.32096 0.0197 0.41711 -0.33296 0.5883 0.532514 ## [220,] -3.21310 -3.5037 -3.60918 -1.97436 -2.9073 -2.582051 ## [221,] -0.40880 -0.5504 -0.14020 -0.88010 -0.7153 -0.895924 ## [222,] 0.01345 0.5740 -0.16942 0.31864 0.4435 0.956823 ## [223,] -0.06197 0.7865 0.31622 0.21417 0.3740 0.661047 ## [224,] -0.75239 -0.0785 -0.15925 -1.32275 -0.8061 -0.316104 ## [225,] 0.32421 -0.8299 -0.98806 -1.10142 0.6270 -0.283884 ## [226,] 0.14819 0.5553 -0.41520 -0.11164 -0.1089 0.301316 ## [227,] -1.00513 -0.6203 -0.95134 -2.09121 -1.2136 -1.427656 ## [228,] 0.26293 1.2106 0.70561 0.43550 0.8568 0.585361 ## [229,] -0.84975 -0.2854 -0.29328 -1.86988 -0.3773 -0.547302 ## [230,] 1.95455 1.9300 1.57784 1.85556 1.9074 1.342424 ## [231,] -2.25125 -1.0199 -1.17320 -1.10142 -1.6423 -2.404807 ## [232,] -0.36947 -0.0413 -0.18425 0.09732 0.0147 -1.250412 ## [233,] -0.17801 0.5706 0.90735 1.30843 0.8722 0.548382 ## [234,] -0.56093 -0.6145 -0.37162 0.76130 -0.3928 -0.181705 ## [235,] -0.60352 -1.5054 -2.20057 -0.99695 -1.1440 -1.293256 ## [236,] -0.90777 -0.7945 -0.59409 0.21417 -0.9605 -0.059037 ## [237,] -0.88908 -1.2489 -0.53535 -0.22849 -0.3232 0.285449 ## [238,] -0.11999 -0.4365 -1.03781 -0.77562 -0.3773 -0.478103 ## [239,] -0.02263 -0.3670 -2.05235 0.10969 -1.5574 -2.898938 ## [240,] -0.75564 -0.2718 -0.72680 -0.22849 0.3740 -0.010949 ## [241,] -0.87494 1.2239 0.84739 0.43550 1.6235 1.171046 ## [242,] 1.95455 -3.4890 0.48482 -3.06862 0.1364 1.171046 ## [243,] -1.36741 0.8710 1.01103 -0.88010 1.4787 0.999667 ## [244,] 1.45558 1.3276 0.92758 0.09732 0.6965 1.342424 ## [245,] -1.60145 -0.9909 -0.20292 -0.43744 -0.6071 0.230873 ## [246,] -0.40880 -0.9916 -1.46093 0.09732 -1.7118 -1.702320 ## [247,] 1.16676 0.0282 0.72290 -0.88010 0.6579 -0.005083 ## [248,] 0.12950 -0.2534 0.06530 0.76130 0.1596 0.230873 ## [249,] -0.60027 0.1046 -0.77941 -0.88010 -0.5376 -0.187571 ## [250,] 0.32421 0.9699 0.82703 0.42312 0.0901 1.042512 ## attr(,&quot;scaled:center&quot;) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## -6.22e-17 5.58e-17 2.16e-17 -7.31e-17 -1.76e-16 -4.49e-17 ## attr(,&quot;scaled:scale&quot;) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## 0.523 1.189 2.349 1.051 1.491 0.986 #Unstandardised solution # Function to get unstandardized LV unstandardized_LV &lt;- function(data, sempls_model){ # Save object to hold outer weights which will be scaled in loop outer_weights_resc &lt;- sempls_model$outer_weights # Find standard deviation of indicators std_indicators &lt;- unlist(lapply(data, FUN = sd)) for (lv in colnames(sempls_model$model$M)) { # Check estimation mode of LV mode &lt;- attr(sempls_model$model$blocks[[lv]], &quot;mode&quot;) if (mode == &quot;A&quot;) { tmp_indicator_names &lt;- sempls_model$model$measuremod[sempls_model$model$measuremod[,1]==lv,2] } else { tmp_indicator_names &lt;- sempls_model$model$measuremod[sempls_model$model$measuremod[,2]==lv,1] } tmp_weights &lt;- outer_weights_resc[tmp_indicator_names,lv] tmp_weights &lt;- tmp_weights/std_indicators[tmp_indicator_names] outer_weights_resc[tmp_indicator_names,lv] &lt;- tmp_weights/sum(tmp_weights) } # Data as matrix in order to perform matrix multiplication data_mat &lt;- as.matrix(data[,rownames(outer_weights_resc)]) unstandardized_LV &lt;- data_mat %*% outer_weights_resc return(unstandardized_LV) } # Unstandardized LV LV_unstand &lt;- unstandardized_LV(data = mobi, sempls_model = epsi) head(LV_unstand) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## [1,] 6.68 5.55 5.73 2.6 5.79 5.92 ## [2,] 9.68 9.79 9.74 10.0 9.30 9.33 ## [3,] 7.00 6.13 6.98 7.0 7.37 6.23 ## [4,] 7.10 7.46 7.36 5.0 10.00 9.50 ## [5,] 8.39 8.41 9.00 6.0 8.74 8.28 ## [6,] 8.81 8.60 9.04 10.0 7.37 9.41 plot(LV_unstand[,5:6]) pairs(LV_unstand,pch=19,cex=0.7,cex.axis=0.8,col.axis=&quot;gray70&quot;,gap=0.5) summary(LV_unstand) ## EXP IMAGE QUAL VALUE ## Min. : 3.19 Min. : 2.91 Min. : 3.20 Min. : 1.00 ## 1st Qu.: 6.68 1st Qu.: 6.69 1st Qu.: 7.15 1st Qu.: 5.58 ## Median : 7.52 Median : 7.54 Median : 7.79 Median : 7.00 ## Mean : 7.52 Mean : 7.48 Mean : 7.75 Mean : 6.61 ## 3rd Qu.: 8.30 3rd Qu.: 8.37 3rd Qu.: 8.66 3rd Qu.: 7.98 ## Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 ## SATISF LOYAL ## Min. : 3.46 Min. : 1.08 ## 1st Qu.: 6.81 1st Qu.: 6.30 ## Median : 7.64 Median : 7.82 ## Mean : 7.51 Mean : 7.37 ## 3rd Qu.: 8.39 3rd Qu.: 8.86 ## Max. :10.00 Max. :10.00 2.4.7 Revised code He made some changes, e.g., removing the question that was not relevant for the customer loyalty. In this setup the only difference is that I have made some modifications to the model formulation. I have deleted CUSL2 for the reasons mentioned above I have included IMAGE5 that I actually forgot to include in the first setup (it does not have any real influence I found out) I have modified the Quality construct. Now it only has 2 items So it is now a contruct that measures the perception of the service in an attempt to avoid it to be mixed up with SAT If you run this model, you will see that the problem with EXP still remains and the effect of EXP is still insignificant, so it could be an idea simply to drop EXP from the model. The construct IMAGE still have a relative low AVE, but the effects from IMAGE are now significant. library(boot) #Bootstrapping of standard deviations to outer loadings and path coefficients library(semPLS) library(psych) help(mobi) data(mobi) names(mobi) ## [1] &quot;CUEX1&quot; &quot;CUEX2&quot; &quot;CUEX3&quot; &quot;CUSA1&quot; &quot;CUSA2&quot; &quot;CUSA3&quot; &quot;CUSCO&quot; &quot;CUSL1&quot; &quot;CUSL2&quot; ## [10] &quot;CUSL3&quot; &quot;IMAG1&quot; &quot;IMAG2&quot; &quot;IMAG3&quot; &quot;IMAG4&quot; &quot;IMAG5&quot; &quot;PERQ1&quot; &quot;PERQ2&quot; &quot;PERQ3&quot; ## [19] &quot;PERQ4&quot; &quot;PERQ5&quot; &quot;PERQ6&quot; &quot;PERQ7&quot; &quot;PERV1&quot; &quot;PERV2&quot; #The dataset summary(mobi) ## CUEX1 CUEX2 CUEX3 CUSA1 ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 4.00 ## 1st Qu.: 7.00 1st Qu.: 7.00 1st Qu.: 6.00 1st Qu.: 7.00 ## Median : 8.00 Median : 8.00 Median : 8.00 Median : 8.00 ## Mean : 7.58 Mean : 7.53 Mean : 7.42 Mean : 7.99 ## 3rd Qu.: 8.00 3rd Qu.: 9.00 3rd Qu.: 9.00 3rd Qu.: 9.00 ## Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 ## CUSA2 CUSA3 CUSCO CUSL1 ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 1.00 ## 1st Qu.: 6.00 1st Qu.: 7.00 1st Qu.: 6.00 1st Qu.: 6.00 ## Median : 7.00 Median : 7.00 Median : 7.00 Median : 8.00 ## Mean : 7.13 Mean : 7.32 Mean : 7.07 Mean : 7.45 ## 3rd Qu.: 8.00 3rd Qu.: 8.00 3rd Qu.: 9.00 3rd Qu.:10.00 ## Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 ## CUSL2 CUSL3 IMAG1 IMAG2 ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 1.00 ## 1st Qu.: 3.00 1st Qu.: 7.00 1st Qu.: 7.00 1st Qu.: 7.00 ## Median : 4.00 Median : 8.00 Median : 8.00 Median : 8.00 ## Mean : 4.99 Mean : 7.67 Mean : 7.64 Mean : 7.78 ## 3rd Qu.: 6.75 3rd Qu.:10.00 3rd Qu.: 9.00 3rd Qu.: 9.00 ## Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 ## IMAG3 IMAG4 IMAG5 PERQ1 ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 2.00 ## 1st Qu.: 5.00 1st Qu.: 7.00 1st Qu.: 7.00 1st Qu.: 7.00 ## Median : 7.00 Median : 8.00 Median : 8.00 Median : 8.00 ## Mean : 6.74 Mean : 7.59 Mean : 7.93 Mean : 7.94 ## 3rd Qu.: 8.00 3rd Qu.: 9.00 3rd Qu.: 9.00 3rd Qu.: 9.00 ## Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 ## PERQ2 PERQ3 PERQ4 PERQ5 PERQ6 ## Min. : 1.00 Min. : 1.0 Min. : 1.00 Min. : 3.00 Min. : 1.00 ## 1st Qu.: 6.00 1st Qu.: 7.0 1st Qu.: 7.00 1st Qu.: 7.00 1st Qu.: 7.00 ## Median : 7.00 Median : 8.0 Median : 8.00 Median : 8.00 Median : 8.00 ## Mean : 7.19 Mean : 7.7 Mean : 7.92 Mean : 7.87 Mean : 7.78 ## 3rd Qu.: 8.00 3rd Qu.: 9.0 3rd Qu.: 9.00 3rd Qu.: 9.00 3rd Qu.: 9.00 ## Max. :10.00 Max. :10.0 Max. :10.00 Max. :10.00 Max. :10.00 ## PERQ7 PERV1 PERV2 ## Min. : 1.00 Min. : 1.00 Min. : 1.00 ## 1st Qu.: 7.00 1st Qu.: 5.00 1st Qu.: 6.00 ## Median : 8.00 Median : 6.00 Median : 7.00 ## Mean : 7.59 Mean : 6.16 Mean : 6.92 ## 3rd Qu.: 9.00 3rd Qu.: 8.00 3rd Qu.: 8.00 ## Max. :10.00 Max. :10.00 Max. :10.00 describe(mobi) #from library(psych) vars n mean sd median trimmed mad min max range skew kurtosis se CUEX1 1 250 7.58 1.62 8 7.68 1.48 1 10 9 -0.663 0.829 0.103 CUEX2 2 250 7.53 1.79 8 7.67 1.48 1 10 9 -0.800 1.447 0.113 CUEX3 3 250 7.42 2.10 8 7.63 1.48 1 10 9 -0.849 0.615 0.133 CUSA1 4 250 7.99 1.23 8 8.03 1.48 4 10 6 -0.220 0.153 0.078 CUSA2 5 250 7.13 1.76 7 7.18 1.48 1 10 9 -0.547 0.558 0.112 CUSA3 6 250 7.32 1.75 7 7.41 1.48 1 10 9 -0.667 0.962 0.110 CUSCO 7 250 7.07 2.27 7 7.29 2.22 1 10 9 -0.710 0.238 0.144 CUSL1 8 250 7.45 2.66 8 7.86 2.96 1 10 9 -0.995 0.077 0.168 CUSL2 9 250 4.99 2.84 4 4.80 2.96 1 10 9 0.545 -0.906 0.180 CUSL3 10 250 7.67 2.22 8 7.96 2.96 1 10 9 -1.059 0.864 0.140 IMAG1 11 250 7.64 1.70 8 7.76 1.48 1 10 9 -0.901 1.689 0.108 IMAG2 12 250 7.78 1.69 8 7.93 1.48 1 10 9 -0.793 0.962 0.107 IMAG3 13 250 6.74 2.13 7 6.91 1.48 1 10 9 -0.793 0.657 0.135 IMAG4 14 250 7.59 1.84 8 7.75 1.48 1 10 9 -0.923 1.321 0.116 IMAG5 15 250 7.93 1.56 8 8.07 1.48 1 10 9 -1.069 2.682 0.098 PERQ1 16 250 7.94 1.42 8 8.04 1.48 2 10 8 -0.720 1.179 0.090 PERQ2 17 250 7.19 1.89 7 7.33 1.48 1 10 9 -0.819 0.729 0.120 PERQ3 18 250 7.70 1.82 8 7.88 1.48 1 10 9 -0.906 0.831 0.115 PERQ4 19 250 7.92 1.65 8 8.11 1.48 1 10 9 -1.070 1.790 0.104 PERQ5 20 250 7.87 1.45 8 7.97 1.48 3 10 7 -0.677 0.558 0.092 PERQ6 21 250 7.78 1.63 8 7.92 1.48 1 10 9 -0.974 1.787 0.103 PERQ7 22 250 7.59 1.84 8 7.78 1.48 1 10 9 -0.959 1.053 0.117 PERV1 23 250 6.16 2.18 6 6.26 1.48 1 10 9 -0.397 -0.214 0.138 PERV2 24 250 6.92 1.84 7 6.98 1.48 1 10 9 -0.645 0.971 0.117 hist(mobi$CUSL1) hist(mobi$CUSL2) #The structural model EPSIsm &lt;- matrix(c(&quot;IMAGE&quot;, &quot;VALUE&quot;,&quot;EXP&quot;,&quot;VALUE&quot;,&quot;QUAL&quot;,&quot;VALUE&quot;,&quot;VALUE&quot;,&quot;SATISF&quot;,&quot;SATISF&quot;,&quot;LOYAL&quot;,&quot;IMAGE&quot;,&quot;LOYAL&quot;),byrow=TRUE,ncol=2) colnames(EPSIsm)=c(&quot;source&quot;,&quot;target&quot;) head(EPSIsm) ## source target ## [1,] &quot;IMAGE&quot; &quot;VALUE&quot; ## [2,] &quot;EXP&quot; &quot;VALUE&quot; ## [3,] &quot;QUAL&quot; &quot;VALUE&quot; ## [4,] &quot;VALUE&quot; &quot;SATISF&quot; ## [5,] &quot;SATISF&quot; &quot;LOYAL&quot; ## [6,] &quot;IMAGE&quot; &quot;LOYAL&quot; #The measurement model #IMAG5 has been added compared to the original setup (forgotten in original setup) EPSImm1 &lt;-matrix(c(&quot;IMAGE&quot;, &quot;IMAG1&quot;,&quot;IMAGE&quot;,&quot;IMAG2&quot;,&quot;IMAGE&quot;,&quot;IMAG3&quot;,&quot;IMAGE&quot;,&quot;IMAG4&quot;, &quot;IMAGE&quot;,&quot;IMAG5&quot;),byrow=TRUE,ncol=2) colnames(EPSImm1)=c(&quot;source&quot;,&quot;target&quot;) head(EPSImm1) ## source target ## [1,] &quot;IMAGE&quot; &quot;IMAG1&quot; ## [2,] &quot;IMAGE&quot; &quot;IMAG2&quot; ## [3,] &quot;IMAGE&quot; &quot;IMAG3&quot; ## [4,] &quot;IMAGE&quot; &quot;IMAG4&quot; ## [5,] &quot;IMAGE&quot; &quot;IMAG5&quot; EPSImm2 &lt;-matrix(c(&quot;EXP&quot;, &quot;CUEX1&quot;,&quot;EXP&quot;,&quot;CUEX2&quot;,&quot;EXP&quot;,&quot;CUEX3&quot;),byrow=TRUE,ncol=2) colnames(EPSImm2)=c(&quot;source&quot;,&quot;target&quot;) # &quot;Pure&quot; service quality EPSImm3 &lt;-matrix(c(&quot;QUAL&quot;,&quot;PERQ3&quot;,&quot;QUAL&quot;,&quot;PERQ4&quot;),byrow=TRUE,ncol=2) colnames(EPSImm3)=c(&quot;source&quot;,&quot;target&quot;) head(EPSImm3,7) ## source target ## [1,] &quot;QUAL&quot; &quot;PERQ3&quot; ## [2,] &quot;QUAL&quot; &quot;PERQ4&quot; EPSImm4 &lt;-matrix(c(&quot;VALUE&quot;, &quot;PERV1&quot;,&quot;VALUE&quot;,&quot;PERV2&quot;),byrow=TRUE,ncol=2) colnames(EPSImm4)=c(&quot;source&quot;,&quot;target&quot;) EPSImm5 &lt;-matrix(c(&quot;SATISF&quot;, &quot;CUSA1&quot;,&quot;SATISF&quot;,&quot;CUSA2&quot;,&quot;SATISF&quot;,&quot;CUSA3&quot;),byrow=TRUE,ncol=2) colnames(EPSImm5)=c(&quot;source&quot;,&quot;target&quot;) #Exclusion of CUSL2 EPSImm6 &lt;-matrix(c(&quot;LOYAL&quot;, &quot;CUSL1&quot;,&quot;LOYAL&quot;,&quot;CUSL3&quot;),byrow=TRUE,ncol=2) colnames(EPSImm6)=c(&quot;source&quot;,&quot;target&quot;) EPSImm &lt;-rbind(EPSImm1,EPSImm2,EPSImm3,EPSImm4,EPSImm5,EPSImm6) EPSImm ## source target ## [1,] &quot;IMAGE&quot; &quot;IMAG1&quot; ## [2,] &quot;IMAGE&quot; &quot;IMAG2&quot; ## [3,] &quot;IMAGE&quot; &quot;IMAG3&quot; ## [4,] &quot;IMAGE&quot; &quot;IMAG4&quot; ## [5,] &quot;IMAGE&quot; &quot;IMAG5&quot; ## [6,] &quot;EXP&quot; &quot;CUEX1&quot; ## [7,] &quot;EXP&quot; &quot;CUEX2&quot; ## [8,] &quot;EXP&quot; &quot;CUEX3&quot; ## [9,] &quot;QUAL&quot; &quot;PERQ3&quot; ## [10,] &quot;QUAL&quot; &quot;PERQ4&quot; ## [11,] &quot;VALUE&quot; &quot;PERV1&quot; ## [12,] &quot;VALUE&quot; &quot;PERV2&quot; ## [13,] &quot;SATISF&quot; &quot;CUSA1&quot; ## [14,] &quot;SATISF&quot; &quot;CUSA2&quot; ## [15,] &quot;SATISF&quot; &quot;CUSA3&quot; ## [16,] &quot;LOYAL&quot; &quot;CUSL1&quot; ## [17,] &quot;LOYAL&quot; &quot;CUSL3&quot; #The whole model EPSI &lt;- plsm(data=mobi,strucmod=EPSIsm,measuremod=EPSImm) EPSI[[&quot;D&quot;]] #The structural model (inner relations) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## EXP 0 0 0 1 0 0 ## IMAGE 0 0 0 1 0 1 ## QUAL 0 0 0 1 0 0 ## VALUE 0 0 0 0 1 0 ## SATISF 0 0 0 0 0 1 ## LOYAL 0 0 0 0 0 0 EPSI[[&quot;M&quot;]] #The measurement model ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 1 0 0 0 0 0 ## CUEX2 1 0 0 0 0 0 ## CUEX3 1 0 0 0 0 0 ## IMAG1 0 1 0 0 0 0 ## IMAG2 0 1 0 0 0 0 ## IMAG3 0 1 0 0 0 0 ## IMAG4 0 1 0 0 0 0 ## IMAG5 0 1 0 0 0 0 ## PERQ3 0 0 1 0 0 0 ## PERQ4 0 0 1 0 0 0 ## PERV1 0 0 0 1 0 0 ## PERV2 0 0 0 1 0 0 ## CUSA1 0 0 0 0 1 0 ## CUSA2 0 0 0 0 1 0 ## CUSA3 0 0 0 0 1 0 ## CUSL1 0 0 0 0 0 1 ## CUSL3 0 0 0 0 0 1 #Estimation of model epsi &lt;- sempls(model=EPSI,data=mobi) ## All 250 observations are valid. ## Converged after 5 iterations. ## Tolerance: 1e-07 ## Scheme: centroid epsi ## Path Estimate ## lam_1_1 EXP -&gt; CUEX1 0.79 ## lam_1_2 EXP -&gt; CUEX2 0.58 ## lam_1_3 EXP -&gt; CUEX3 0.68 ## lam_2_1 IMAGE -&gt; IMAG1 0.74 ## lam_2_2 IMAGE -&gt; IMAG2 0.57 ## lam_2_3 IMAGE -&gt; IMAG3 0.61 ## lam_2_4 IMAGE -&gt; IMAG4 0.79 ## lam_2_5 IMAGE -&gt; IMAG5 0.73 ## lam_3_1 QUAL -&gt; PERQ3 0.90 ## lam_3_2 QUAL -&gt; PERQ4 0.85 ## lam_4_1 VALUE -&gt; PERV1 0.90 ## lam_4_2 VALUE -&gt; PERV2 0.94 ## lam_5_1 SATISF -&gt; CUSA1 0.79 ## lam_5_2 SATISF -&gt; CUSA2 0.84 ## lam_5_3 SATISF -&gt; CUSA3 0.86 ## lam_6_1 LOYAL -&gt; CUSL1 0.84 ## lam_6_2 LOYAL -&gt; CUSL3 0.91 ## beta_1_4 EXP -&gt; VALUE 0.11 ## beta_2_4 IMAGE -&gt; VALUE 0.30 ## beta_3_4 QUAL -&gt; VALUE 0.26 ## beta_4_5 VALUE -&gt; SATISF 0.61 ## beta_2_6 IMAGE -&gt; LOYAL 0.21 ## beta_5_6 SATISF -&gt; LOYAL 0.51 # Evaluation of estimated model plsLoadings(epsi) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 0.79 . . . . . ## CUEX2 0.58 . . . . . ## CUEX3 0.68 . . . . . ## IMAG1 . 0.74 . . . . ## IMAG2 . 0.57 . . . . ## IMAG3 . 0.61 . . . . ## IMAG4 . 0.79 . . . . ## IMAG5 . 0.73 . . . . ## PERQ3 . . 0.90 . . . ## PERQ4 . . 0.85 . . . ## PERV1 . . . 0.90 . . ## PERV2 . . . 0.94 . . ## CUSA1 . . . . 0.79 . ## CUSA2 . . . . 0.84 . ## CUSA3 . . . . 0.86 . ## CUSL1 . . . . . 0.84 ## CUSL3 . . . . . 0.91 #Composite reliability dgrho(epsi) ## Dillon-Goldstein&#39;s rho reflective MVs ## EXP 0.73 3 ## IMAGE 0.82 5 ## QUAL 0.86 2 ## VALUE 0.92 2 ## SATISF 0.87 3 ## LOYAL 0.87 2 # AVE is average of communality for a construct communality(epsi) ## communality reflective MVs ## EXP 0.47 3 ## IMAGE 0.48 5 ## QUAL 0.76 2 ## VALUE 0.85 2 ## SATISF 0.69 3 ## LOYAL 0.77 2 ## ## Average communality: 0.63 ### check for discriminant validity using loadings. If the relative difference between a loading and a cross-loading # is more than 0.2 the the cross-loading will be printed l &lt;-plsLoadings(epsi) print(l, type=&quot;discriminant&quot;, cutoff=0.5, reldiff=0.2) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 0.79 . . . . . ## CUEX2 0.58 . . . . . ## CUEX3 0.68 . . . . . ## IMAG1 . 0.74 . . . . ## IMAG2 . 0.57 . . . . ## IMAG3 . 0.61 . . . . ## IMAG4 . 0.79 . . . . ## IMAG5 . 0.73 . . . . ## PERQ3 . . 0.90 . . . ## PERQ4 . . 0.85 . . . ## PERV1 . . . 0.90 . . ## PERV2 . . . 0.94 . . ## CUSA1 . . . . 0.79 . ## CUSA2 . . . . 0.84 . ## CUSA3 . . . . 0.86 . ## CUSL1 . . . . . 0.84 ## CUSL3 . . . . . 0.91 pC &lt;- pathCoeff(epsi) print(pC,abbreviate=TRUE,minlength=3) ## EXP IMA QUA VAL SAT LOY ## EXP . . . 0.11 . . ## IMA . . . 0.30 . 0.21 ## QUA . . . 0.26 . . ## VAL . . . . 0.61 . ## SAT . . . . . 0.51 ## LOY . . . . . . tE&lt;- totalEffects(epsi) print(tE,abbreviate=TRUE,minlength=3) ## EXP IMA QUA VAL SAT LOY ## EXP . . . 0.106 0.065 0.033 ## IMA . . . 0.296 0.180 0.298 ## QUA . . . 0.260 0.158 0.081 ## VAL . . . . 0.610 0.312 ## SAT . . . . . 0.512 ## LOY . . . . . . #R2 rSquared(epsi) ## R-squared ## EXP . ## IMAGE . ## QUAL . ## VALUE 0.32 ## SATISF 0.37 ## LOYAL 0.45 we still see that there are low \\(R^2\\) #Bootstrapping - estimation of standard errors set.seed(123) epsiBoot &lt;- bootsempls(epsi,nboot=500,start=&quot;ones&quot;,verbose=FALSE) epsiBoot ## Call: bootsempls(object = epsi, nboot = 500, start = &quot;ones&quot;, verbose = FALSE) ## ## Estimate Bias Std.Error ## EXP -&gt; CUEX1 0.787 -1.55e-02 0.07163 ## EXP -&gt; CUEX2 0.583 -1.91e-02 0.14742 ## EXP -&gt; CUEX3 0.682 -3.18e-03 0.10264 ## IMAGE -&gt; IMAG1 0.737 -4.02e-03 0.04051 ## IMAGE -&gt; IMAG2 0.569 -4.89e-03 0.06784 ## IMAGE -&gt; IMAG3 0.607 -2.51e-03 0.06196 ## IMAGE -&gt; IMAG4 0.790 -1.31e-03 0.03987 ## IMAGE -&gt; IMAG5 0.730 -2.54e-03 0.04754 ## QUAL -&gt; PERQ3 0.896 1.46e-05 0.02078 ## QUAL -&gt; PERQ4 0.846 -2.84e-03 0.03756 ## VALUE -&gt; PERV1 0.904 -2.08e-03 0.02118 ## VALUE -&gt; PERV2 0.938 5.94e-04 0.00715 ## SATISF -&gt; CUSA1 0.785 -3.18e-03 0.03452 ## SATISF -&gt; CUSA2 0.845 3.41e-04 0.02307 ## SATISF -&gt; CUSA3 0.864 1.32e-03 0.01645 ## LOYAL -&gt; CUSL1 0.837 -3.22e-03 0.03913 ## LOYAL -&gt; CUSL3 0.914 -6.55e-05 0.01073 ## EXP -&gt; VALUE 0.106 1.60e-02 0.08455 ## IMAGE -&gt; VALUE 0.296 7.13e-04 0.10320 ## QUAL -&gt; VALUE 0.260 -8.88e-03 0.09773 ## VALUE -&gt; SATISF 0.610 -1.23e-03 0.05526 ## IMAGE -&gt; LOYAL 0.206 1.08e-02 0.07371 ## SATISF -&gt; LOYAL 0.512 -6.88e-03 0.08686 We see that again the relationship between EXP and VALUE is still very low, perhaps one should drop the expectations. # Structural model coefficients parallelplot(epsiBoot,reflinesAt=0, alpha=0.8, varnames=attr(epsiBoot$t,&quot;path&quot;)[18:23]) #Constructing observations plsWeights(epsi) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## CUEX1 0.56 . . . . . ## CUEX2 0.34 . . . . . ## CUEX3 0.52 . . . . . ## IMAG1 . 0.29 . . . . ## IMAG2 . 0.23 . . . . ## IMAG3 . 0.25 . . . . ## IMAG4 . 0.36 . . . . ## IMAG5 . 0.30 . . . . ## PERQ3 . . 0.62 . . . ## PERQ4 . . 0.52 . . . ## PERV1 . . . 0.49 . . ## PERV2 . . . 0.60 . . ## CUSA1 . . . . 0.35 . ## CUSA2 . . . . 0.38 . ## CUSA3 . . . . 0.47 . ## CUSL1 . . . . . 0.48 ## CUSL3 . . . . . 0.65 #Standardised observations for (constructs) epsi$factor_scores ## EXP IMAGE QUAL VALUE SATISF LOYAL ## [1,] -0.65830 -1.94936 -1.5568 -2.19532 -1.3218 -0.7541 ## [2,] 1.70499 1.63083 1.4452 1.85594 1.3724 1.1485 ## [3,] -0.40879 -1.01038 -1.2141 0.21503 -0.1086 -0.4604 ## [4,] -0.33371 0.39251 0.8148 -0.87890 1.9078 1.1485 ## [5,] 0.68662 0.78744 0.7598 -0.33194 0.9439 0.5611 ## [6,] 1.01461 0.89370 1.1025 1.85594 -0.1086 1.1485 ## [7,] -1.15391 -1.01026 0.1568 -0.22993 0.3733 0.0153 ## [8,] -2.79098 0.44639 -0.2134 -0.22993 -0.6081 -0.3486 ## [9,] -0.40879 -0.33157 -0.8714 -0.22993 -0.3939 -0.4604 ## [10,] -1.05993 -0.26486 0.1293 -0.77690 -1.3577 -0.0263 ## [11,] -0.35065 -0.07608 0.1293 0.21503 0.2478 0.5611 ## [12,] 1.70499 0.48005 0.1293 -0.22993 0.1056 -1.2297 ## [13,] 0.24574 0.27541 -0.2134 -0.55442 0.1056 0.0153 ## [14,] 1.95450 1.95705 1.4452 0.53952 0.9439 1.1485 ## [15,] -0.73679 0.20196 0.4445 -1.54635 -0.3228 0.2388 ## [16,] -0.60356 0.75991 0.7873 -0.89738 -0.0375 0.3792 ## [17,] -0.12004 0.97323 1.1300 0.74352 0.3909 1.1485 ## [18,] -1.10257 -2.10952 -0.8989 0.21503 -1.0724 -2.1811 ## [19,] 0.87798 0.94936 0.7873 0.43752 0.9263 0.9666 ## [20,] -1.35208 -0.90952 -0.2134 0.21503 -0.1621 -0.3902 ## [21,] -1.44947 0.34990 0.1293 0.21503 0.5875 0.4909 ## [22,] 0.12606 -0.10896 0.4995 -0.87890 -1.5719 -1.0063 ## [23,] -0.79153 -0.47928 -0.2134 -0.22993 -0.9293 -0.2784 ## [24,] -0.15929 -0.03574 0.1293 1.30897 0.3733 0.4909 ## [25,] -0.06190 -0.61496 -1.8446 -1.32387 -2.1609 -2.7685 ## [26,] 0.76170 1.95705 1.4452 0.76200 1.4794 1.1485 ## [27,] 0.37897 0.83027 0.1293 0.21503 -0.5370 -0.2784 ## [28,] 0.87798 -2.39977 -1.8446 1.85594 -0.5370 0.5611 ## [29,] -0.75568 -0.22756 -0.1859 0.53952 -0.1086 0.3792 ## [30,] 0.91382 1.72240 1.1025 1.41097 1.6401 1.1485 ## [31,] -0.56432 0.09265 0.4445 1.08649 0.6586 0.7846 ## [32,] -0.65830 -0.47928 -0.2134 -0.87890 -1.0724 -1.5235 ## [33,] -1.02749 1.02888 -0.1309 0.74352 -0.2869 1.1485 ## [34,] 0.33973 0.60667 1.4452 0.43752 0.5523 0.5611 ## [35,] 0.99767 -0.88995 -1.8446 -3.06678 -2.8570 -3.1324 ## [36,] 1.57176 1.20187 -0.4462 0.43752 -0.3939 0.0153 ## [37,] -0.10115 -0.71521 -0.2409 -1.32387 -1.4647 -2.4748 ## [38,] -0.56092 -0.50565 -0.2134 -0.65642 -0.5370 -0.2082 ## [39,] -0.06190 0.20822 1.1025 1.30897 0.9974 0.8548 ## [40,] -0.15929 -0.34312 -0.2134 0.31704 -0.1086 0.8548 ## [41,] 1.95450 0.36449 1.4452 0.76200 1.4259 0.7846 ## [42,] 0.47635 0.42992 0.4720 0.43752 0.4268 0.3792 ## [43,] -1.60159 1.20788 1.4452 1.41097 1.3724 1.1485 ## [44,] -0.06190 -0.97849 0.1293 -1.10139 -0.1262 0.1270 ## [45,] 1.06935 1.72240 1.4452 1.41097 1.9078 0.7846 ## [46,] 0.87798 1.06746 1.1300 0.43752 0.6586 -2.0406 ## [47,] 0.68662 0.13499 0.4720 -1.54635 -0.3228 -0.6423 ## [48,] -0.71644 -0.28370 -0.1859 0.53952 0.1591 0.1972 ## [49,] -0.68060 -2.58337 -2.5850 -1.64836 -1.9108 -1.7054 ## [50,] -0.67380 -1.13352 1.1025 -2.74229 -1.3042 -0.8243 ## [51,] 0.87798 0.87547 0.4995 -0.45242 0.8018 0.5611 ## [52,] -0.69754 -0.11072 -0.5287 -0.55442 -0.6081 -0.4604 ## [53,] 0.05438 -0.18898 0.1018 0.76200 0.3909 0.6027 ## [54,] 1.07275 -0.80805 0.7598 0.43752 -0.1086 0.1972 ## [55,] -0.90781 -0.34488 -0.2134 0.43752 -0.3228 -0.2784 ## [56,] 0.22685 -0.55627 0.1568 0.43752 -0.1086 -0.2784 ## [57,] -0.54202 -0.24049 -1.5018 -0.87890 -2.3033 -1.2297 ## [58,] 0.09022 1.02385 1.4452 0.76200 0.6586 0.1972 ## [59,] 1.07275 1.04845 0.4445 1.08649 1.4794 1.1485 ## [60,] -1.06333 -0.28370 -0.1859 -0.33194 -0.1086 -0.8658 ## [61,] -0.82932 -0.88753 -1.1866 -1.42587 -1.5896 -1.1595 ## [62,] -0.79153 -0.32830 0.1568 0.21503 0.1767 0.1972 ## [63,] 0.78400 0.21726 0.1293 0.98448 0.1591 0.3090 ## [64,] -0.50618 -1.07520 -2.8453 -0.55442 -0.8223 -0.0263 ## [65,] 0.53449 -0.60743 0.1293 -0.00745 -0.6616 -2.3630 ## [66,] 0.12946 0.42816 0.4445 -1.22187 -0.0910 -0.7541 ## [67,] 0.76170 -0.47109 0.7598 0.21503 0.1056 -0.5721 ## [68,] 0.37897 -0.47928 -0.1859 -0.87890 -2.7676 -2.2513 ## [69,] 0.09362 0.15559 -0.2134 -0.00745 0.1591 0.3792 ## [70,] 0.18760 -0.15355 0.4445 -0.33194 -0.6081 0.0153 ## [71,] -0.15929 0.36874 0.4445 0.21503 0.4444 0.3090 ## [72,] 1.95450 0.59562 1.1025 0.21503 0.7832 -0.3486 ## [73,] 0.51219 0.84534 1.4452 -3.06678 1.9078 -3.1324 ## [74,] -1.00179 -1.21641 -1.2141 0.21503 -1.1435 -0.8658 ## [75,] 1.26072 -0.30630 -0.1859 0.21503 -1.6792 -0.3200 ## [76,] -0.51972 -1.87578 -0.8714 -1.42587 -0.8758 -0.3902 ## [77,] 1.41624 -0.28747 0.1293 0.76200 -0.0727 0.4909 ## [78,] -1.63743 -0.66934 -1.5568 -0.87890 -0.3763 0.5611 ## [79,] -0.25327 -0.03197 -0.2134 -0.33194 -0.3228 0.1972 ## [80,] -0.21743 -0.50189 -0.2134 0.21503 -0.1797 0.1972 ## [81,] 0.97537 0.65905 1.1025 0.21503 0.5875 0.7846 ## [82,] -1.00179 0.27668 0.1293 0.53952 -0.4835 0.3792 ## [83,] 0.37897 0.54549 1.1300 0.53952 1.4083 1.1485 ## [84,] -0.65830 -0.53919 0.1293 -0.00745 -0.5905 -0.5721 ## [85,] -0.00376 -0.44386 -0.1859 -0.00745 -0.3763 -0.6839 ## [86,] -1.44266 0.25584 0.7873 -0.12793 0.0345 -0.5019 ## [87,] 1.22487 0.59738 0.1293 1.08649 1.1230 1.1485 ## [88,] 1.10859 0.77588 0.7873 1.30897 0.4268 0.5611 ## [89,] 0.18760 0.12078 -0.1859 0.53952 0.6586 1.1485 ## [90,] -0.40879 0.43369 0.8148 0.53952 0.3022 0.4909 ## [91,] 0.37897 1.00927 1.1025 0.76200 0.6762 0.5611 ## [92,] -0.06190 -0.28923 -0.5562 -0.00745 0.1056 -0.1667 ## [93,] 0.32083 0.72023 0.4720 0.53952 0.8552 0.8548 ## [94,] -0.81383 -0.13423 0.7598 0.11303 -0.1797 -0.6839 ## [95,] -0.79153 -2.70575 -3.5032 -1.42587 -3.1064 -3.1324 ## [96,] 0.53109 1.45785 0.4995 1.41097 0.9087 0.8548 ## [97,] -0.19513 0.00108 0.1018 0.98448 0.6410 0.6729 ## [98,] -0.35065 -0.59661 -0.5287 0.21503 -0.3228 0.0153 ## [99,] -1.09917 -0.91001 -0.5287 -0.22993 -0.1086 -0.8658 ## [100,] 0.09022 1.02962 1.4452 0.21503 0.8552 0.6027 ## [101,] -0.19513 -0.01915 0.1293 0.53952 -0.5370 -0.6839 ## [102,] 0.12946 0.21550 -0.1859 0.21503 0.1056 -0.2784 ## [103,] 0.18760 -0.35139 -0.5837 0.53952 0.3733 0.9666 ## [104,] -0.65830 -0.80551 -0.5012 -0.00745 0.5155 -0.3486 ## [105,] 0.28499 1.13941 0.4445 0.76200 0.7297 1.1485 ## [106,] -1.11612 -1.41559 0.1568 -0.87890 -1.3577 -0.6423 ## [107,] 1.95450 -0.66314 -0.2684 1.20697 0.9439 0.1972 ## [108,] 0.03208 0.27165 0.1293 -0.67490 0.6410 0.4909 ## [109,] -0.51632 1.08613 -1.3918 0.76200 0.1591 1.1485 ## [110,] 0.57034 0.40732 0.1293 0.53952 1.4794 1.1485 ## [111,] -0.36955 -0.80690 -0.2134 -0.22993 0.3909 0.3792 ## [112,] 0.97537 1.06668 0.1018 0.86400 -0.0551 0.8548 ## [113,] 1.35810 0.70139 0.7873 0.98448 1.1406 1.1485 ## [114,] 0.87798 0.99280 1.4452 0.76200 1.4794 1.1485 ## [115,] 0.41481 1.14445 1.4452 0.76200 0.9087 0.7846 ## [116,] 0.22004 0.77813 -2.1873 -1.54635 -1.0724 0.4909 ## [117,] -1.75176 1.01303 0.7873 1.85594 1.3372 1.1485 ## [118,] 1.95450 1.95705 0.4170 0.76200 0.7297 1.1485 ## [119,] 1.57176 0.85463 1.1300 1.30897 0.6410 0.2674 ## [120,] 0.82324 1.61109 0.1293 0.76200 1.3372 0.5611 ## [121,] 1.22487 0.65729 0.4445 0.53952 0.2839 -0.4604 ## [122,] 0.22685 -0.82635 -1.1591 -1.87084 -1.4999 -1.4948 ## [123,] 1.45548 1.95705 1.4452 1.85594 1.9078 1.1485 ## [124,] -0.42769 -1.72339 -0.8714 -0.55442 -1.3042 0.1972 ## [125,] -0.63940 -0.55627 -2.4750 -0.00745 -0.5370 -0.3902 ## [126,] -0.25327 -2.84653 0.1018 -0.87890 -0.6792 -2.0693 ## [127,] 1.51362 1.47492 0.7873 1.30897 1.3548 0.8548 ## [128,] -0.84967 -0.39927 0.1293 -0.55442 0.1056 -0.0965 ## [129,] -0.06190 -0.57511 0.1568 -0.87890 -0.8758 -1.0765 ## [130,] -0.15929 0.33283 0.1293 0.21503 -0.1086 0.0153 ## [131,] 0.49865 -0.84487 -0.8989 0.74352 -0.1614 0.5611 ## [132,] 0.14836 -0.92196 -0.8714 -1.42587 -1.3577 -1.4117 ## [133,] -2.02552 -1.77729 -2.1598 -0.87890 -1.6430 1.1485 ## [134,] 0.45745 1.21992 1.4452 1.20697 -0.1973 0.5611 ## [135,] -0.90781 0.09933 0.1293 -0.00745 -0.1086 -0.0965 ## [136,] -0.82737 0.34614 0.1568 0.76200 0.3733 1.1485 ## [137,] 1.95450 1.95705 1.4452 1.85594 1.9078 1.1485 ## [138,] 0.12946 0.07983 0.8148 0.31704 0.9439 0.7846 ## [139,] 0.91723 0.96819 1.1300 -0.47089 1.6401 0.3090 ## [140,] 0.57034 0.32681 0.1293 1.18849 0.3733 0.5611 ## [141,] 0.28499 1.47067 1.1300 0.33551 0.9263 0.7846 ## [142,] 0.12946 0.34990 0.4445 0.21503 0.1056 1.1485 ## [143,] -1.75711 -1.25548 -0.8714 -1.10139 -1.9284 -0.9360 ## [144,] 0.41481 1.06996 -0.2134 -0.57290 0.3733 0.5611 ## [145,] 1.95450 1.95705 1.4452 1.85594 1.9078 1.1485 ## [146,] 0.87798 -0.92550 0.7873 1.30897 0.9439 -0.1252 ## [147,] -0.66170 -1.67831 -2.5025 -1.20339 -1.9108 -1.5235 ## [148,] -1.69557 -0.40655 0.1293 -0.00745 0.1056 -0.2784 ## [149,] -2.12095 -1.45508 -0.8989 -0.87890 -0.0192 -1.2297 ## [150,] 0.91382 0.31132 -0.5287 -0.22993 -0.1621 1.1485 ## [151,] 0.87798 1.02585 1.1300 0.76200 0.4444 1.1485 ## [152,] 0.37897 0.33283 0.1293 -0.22993 0.3733 0.1972 ## [153,] -0.33371 0.85749 -0.8989 -1.44435 -0.4835 0.9666 ## [154,] 1.95450 -0.80941 1.4452 0.74352 0.8194 1.1485 ## [155,] 0.53109 -0.06010 0.4720 -0.00745 0.1056 -0.2784 ## [156,] 0.97537 0.21550 0.1293 -0.00745 -0.1797 -0.0965 ## [157,] 0.37897 -0.55427 -2.4475 0.21503 -0.1086 -1.0765 ## [158,] -0.50618 0.97196 0.1293 0.76200 0.6586 0.5611 ## [159,] -1.81330 -1.87754 -0.5562 -0.77690 -0.6616 -0.7126 ## [160,] 0.53449 0.17692 -0.5287 0.33551 0.1591 0.3792 ## [161,] 0.28159 0.99280 0.1293 0.53952 -0.1086 0.5611 ## [162,] 0.18760 -0.24512 0.1293 -0.55442 -0.1262 0.2674 ## [163,] 0.12946 0.46347 0.7873 0.31704 0.9263 0.9666 ## [164,] -0.60016 -1.55644 0.1018 -0.21146 -1.6613 -1.0478 ## [165,] 0.91382 0.14679 -1.8721 -2.84430 -2.0363 1.1485 ## [166,] -0.47374 0.64373 0.0743 -0.22993 0.9087 -1.2297 ## [167,] 1.26072 1.45359 1.4452 0.21503 0.3733 1.1485 ## [168,] 0.22344 -0.95838 0.1293 0.31704 -0.8758 0.0855 ## [169,] -1.54685 0.56383 0.1293 0.09455 -0.3228 0.1972 ## [170,] -0.02266 -1.35744 -1.1866 -0.22993 -0.3228 0.0153 ## [171,] -0.00716 -0.84785 -0.5287 -0.33194 -0.8758 -0.3902 ## [172,] 0.87798 -0.94171 0.8148 1.85594 0.9087 1.1485 ## [173,] -0.40879 0.05911 0.4445 0.53952 0.1591 0.4909 ## [174,] 0.18760 -0.88315 -0.5287 -0.87890 -1.1970 -0.6423 ## [175,] 0.62848 1.35383 0.7873 0.09455 1.1582 -0.4891 ## [176,] 0.91382 0.48431 0.1293 0.43752 0.6762 -1.2297 ## [177,] -0.06190 0.55897 0.7873 1.85594 1.1230 0.0568 ## [178,] 1.03351 0.19666 0.1568 0.21503 0.3198 0.3792 ## [179,] 0.33973 -0.47928 -0.5287 0.21503 0.5340 1.1485 ## [180,] 0.12946 0.37566 -2.2423 -0.00745 -0.1086 -0.6423 ## [181,] 0.14836 0.21174 -0.1859 -0.33194 -0.1086 -0.0965 ## [182,] -1.02749 -0.51610 -2.1598 -1.85236 -1.0900 -0.3902 ## [183,] 1.38040 1.57341 0.4445 0.09455 0.1591 1.1485 ## [184,] 0.37897 0.33283 0.1293 0.76200 0.1056 -1.0063 ## [185,] -1.25470 -0.57953 -1.1591 -0.33194 -1.6607 -1.1180 ## [186,] 1.45548 1.70532 1.4452 1.85594 1.9078 1.1485 ## [187,] -0.60356 -0.07481 0.7598 0.76200 -0.0551 0.3792 ## [188,] -3.41308 -0.98699 -2.5300 -3.06678 -2.6428 -0.4604 ## [189,] -1.25470 -0.69886 -1.1866 -0.55442 -0.8758 -0.8658 ## [190,] 0.28159 0.64021 0.1293 0.21503 -0.1086 0.5611 ## [191,] 1.70499 0.11099 -1.1041 -1.76884 1.0871 0.5611 ## [192,] 0.51219 1.95705 1.4452 1.18849 0.8367 -0.4891 ## [193,] 0.91723 1.07045 1.1300 -0.00745 0.8018 -0.4891 ## [194,] 1.95450 -0.15517 1.1025 1.53145 0.8728 1.1485 ## [195,] -0.60016 -0.17566 -0.2134 -0.77690 -0.8934 -1.2297 ## [196,] 0.57374 -1.14745 -0.2134 -0.87890 -1.1076 -0.5721 ## [197,] 1.95450 0.90112 1.4452 0.74352 1.9078 1.1485 ## [198,] 1.95450 0.87547 0.7598 1.85594 1.3724 1.1485 ## [199,] 0.12946 -0.18898 -0.1859 -0.55442 -0.3763 0.8548 ## [200,] 1.16673 0.44766 0.1018 -0.00745 0.9439 0.4909 ## [201,] 0.37897 -0.05081 0.4720 -0.22993 0.4268 -0.7828 ## [202,] -0.21743 0.15936 0.7873 0.43752 1.0519 0.5611 ## [203,] -0.31141 -1.50704 -0.8164 -1.42587 0.3733 -0.1667 ## [204,] -0.88891 -2.06693 -0.8714 -1.32387 -1.3577 0.1972 ## [205,] -0.06190 0.34881 -0.5287 -0.87890 -0.0551 0.5611 ## [206,] -1.23580 -1.71234 0.1568 0.21503 -1.6965 -3.1324 ## [207,] 0.57034 0.58583 0.7873 1.30897 0.3557 1.1485 ## [208,] -0.12004 0.15985 -0.5287 0.86400 1.1047 0.7846 ## [209,] -0.67720 0.61937 -0.2134 0.21503 -0.1797 0.3792 ## [210,] -0.73679 -0.38305 0.8148 1.20697 0.9087 0.2388 ## [211,] -0.15929 -0.75076 -1.2141 0.21503 -0.6440 -0.6839 ## [212,] -0.40879 0.37141 -0.2134 0.21503 -0.3939 -1.3702 ## [213,] -0.98629 -0.53810 -0.8989 -0.99938 -0.8047 -0.0965 ## [214,] -0.84967 -0.22805 -0.8714 1.08649 0.1591 0.3792 ## [215,] -1.69557 -1.02132 -0.5012 -0.10945 -1.1611 -1.0478 ## [216,] -2.48334 -0.88290 -1.2416 -1.22187 -0.2156 -1.9576 ## [217,] -1.40682 0.71646 -0.1859 0.76200 -0.1262 1.1485 ## [218,] 0.97537 0.78014 0.4445 -0.87890 0.8728 0.6729 ## [219,] 0.32083 0.02544 0.1293 -0.33194 0.5875 0.6729 ## [220,] -3.21296 -3.39975 -3.5032 -1.97284 -2.9098 -2.5450 ## [221,] -0.40879 -1.01330 0.7598 -0.87890 -0.7151 -0.8658 ## [222,] 0.01318 0.66408 0.7598 0.31704 0.4444 1.1485 ## [223,] -0.06190 0.83755 1.1025 0.21503 0.3733 0.6729 ## [224,] -0.75228 -0.05281 -0.2134 -1.32387 -0.8047 -0.2784 ## [225,] 0.32423 -0.65713 -0.5287 -1.10139 0.6234 -0.1252 ## [226,] 0.14836 0.83883 0.1293 -0.10945 -0.1086 0.2674 ## [227,] -1.00519 -0.68691 -0.5287 -2.09332 -1.2146 -1.4117 ## [228,] 0.26269 1.37534 0.4445 0.43752 0.8552 0.3792 ## [229,] -0.84967 -0.40655 -0.2134 -1.87084 -0.3763 -0.6839 ## [230,] 1.95450 1.95705 1.4452 1.85594 1.9078 1.1485 ## [231,] -2.25077 -1.00375 -1.5568 -1.10139 -1.6430 -2.3630 ## [232,] -0.36955 -0.01915 0.1293 0.09455 0.0160 -1.2297 ## [233,] -0.17819 0.66299 1.1025 1.30897 0.8728 0.5611 ## [234,] -0.56092 -0.48305 -0.8714 0.76200 -0.3939 -0.0965 ## [235,] -0.60356 -2.54571 -2.2148 -0.99938 -1.1435 -1.2297 ## [236,] -0.90781 -0.63343 -0.5287 0.21503 -0.9645 -0.2784 ## [237,] -0.88891 -1.18385 -0.8714 -0.22993 -0.3228 0.3792 ## [238,] -0.12004 -0.53494 -0.8714 -0.77690 -0.3763 -0.7126 ## [239,] -0.02266 -0.67001 -2.1873 0.11303 -1.5543 -2.9504 ## [240,] -0.75568 -0.40054 -0.1859 -0.22993 0.3733 -0.2369 ## [241,] -0.87537 1.38086 1.4452 0.43752 1.6225 1.1485 ## [242,] 1.95450 -2.41597 1.1025 -3.06678 0.1408 1.1485 ## [243,] -1.36758 0.71798 0.7598 -0.87890 1.4794 1.1485 ## [244,] 1.45548 1.47067 0.8148 0.09455 0.6945 1.1485 ## [245,] -1.60159 -0.98176 0.4995 -0.43394 -0.6081 0.0153 ## [246,] -0.40879 -0.98274 -0.8439 0.09455 -1.7141 -1.9576 ## [247,] 1.16673 0.03760 1.1025 -0.87890 0.6586 -0.0549 ## [248,] 0.12946 -0.19213 -0.5562 0.76200 0.1591 0.0153 ## [249,] -0.60016 -0.09364 -0.5562 -0.87890 -0.5370 -0.2784 ## [250,] 0.32423 0.98903 0.8148 0.41904 0.0880 1.1485 ## attr(,&quot;scaled:center&quot;) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## -7.44e-17 -3.84e-17 -1.18e-16 -4.98e-17 -2.04e-16 -5.89e-17 ## attr(,&quot;scaled:scale&quot;) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## 0.523 1.469 0.758 1.014 1.493 1.002 densityplot(epsi) densityplot(epsi,use=&quot;prediction&quot;) #Unstandardised solution # Function to get unstandardized LV unstandardized_LV &lt;- function(data, sempls_model){ # Save object to hold outer weights which will be scaled in loop outer_weights_resc &lt;- sempls_model$outer_weights # Find standard deviation of indicators std_indicators &lt;- unlist(lapply(data, FUN = sd)) for (lv in colnames(sempls_model$model$M)) { # Check estimation mode of LV mode &lt;- attr(sempls_model$model$blocks[[lv]], &quot;mode&quot;) if (mode == &quot;A&quot;) { tmp_indicator_names &lt;- sempls_model$model$measuremod[sempls_model$model$measuremod[,1]==lv,2] } else { tmp_indicator_names &lt;- sempls_model$model$measuremod[sempls_model$model$measuremod[,2]==lv,1] } tmp_weights &lt;- outer_weights_resc[tmp_indicator_names,lv] tmp_weights &lt;- tmp_weights/std_indicators[tmp_indicator_names] outer_weights_resc[tmp_indicator_names,lv] &lt;- tmp_weights/sum(tmp_weights) } # Data as matrix in order to perform matrix multiplication data_mat &lt;- as.matrix(data[,rownames(outer_weights_resc)]) unstandardized_LV &lt;- data_mat %*% outer_weights_resc return(unstandardized_LV) } # Unstandardized LV LV_unstand &lt;- unstandardized_LV(data = mobi, sempls_model = epsi) head(LV_unstand) ## EXP IMAGE QUAL VALUE SATISF LOYAL ## [1,] 6.68 5.19 5.44 2.59 5.79 6.00 ## [2,] 9.68 9.60 10.00 10.00 9.30 10.00 ## [3,] 7.00 6.35 5.96 7.00 7.37 6.62 ## [4,] 7.10 8.07 9.04 5.00 10.00 10.00 ## [5,] 8.39 8.56 8.96 6.00 8.74 8.77 ## [6,] 8.81 8.69 9.48 10.00 7.37 10.00 plot(LV_unstand[,5:6]) pairs(LV_unstand,pch=19,cex=0.7,cex.axis=0.8,col.axis=&quot;gray70&quot;,gap=0.5) summary(LV_unstand) ## EXP IMAGE QUAL VALUE ## Min. : 3.19 Min. : 3.40 Min. : 2.48 Min. : 1.00 ## 1st Qu.: 6.68 1st Qu.: 6.84 1st Qu.: 7.00 1st Qu.: 5.57 ## Median : 7.52 Median : 7.65 Median : 8.00 Median : 7.00 ## Mean : 7.52 Mean : 7.59 Mean : 7.80 Mean : 6.61 ## 3rd Qu.: 8.30 3rd Qu.: 8.47 3rd Qu.: 8.99 3rd Qu.: 7.97 ## Max. :10.00 Max. :10.00 Max. :10.00 Max. :10.00 ## SATISF LOYAL ## Min. : 3.46 Min. : 1.00 ## 1st Qu.: 6.81 1st Qu.: 6.42 ## Median : 7.64 Median : 8.00 ## Mean : 7.51 Mean : 7.59 ## 3rd Qu.: 8.39 3rd Qu.: 9.23 ## Max. :10.00 Max. :10.00 2.5 Segmentation (Clustering) 2.5.1 What is clustering Basically what we aim to do, is separate e.g., customers into a given amount of groups. We have the following measures: Between-cluster variation: this is from one center to other cluster centers, we want to maximize this. Within-cluster variation: this is the distance from the center of the clusters to the center. 2.5.2 Clustering techniques There are many techniques and no one size fits all. We are gogin to focus on: Hierarchical algorithms, and Partitioning algorithsm, and main focus on K-means, as this is the most widely used. K-means advantages and disadvantages Advantages: Relatively efficient Better than most hierarchical methods at handling noisy data and outliers Disadvantges Need to specify k, the number of clusters in advance In small samples, results depend strongly on initial cluster centers In principle k-means works only with continuous variables it will always create the amount of groups, hence it is a good idea to see if you can visualize the data 2.5.3 An example 2.5.3.1 1. Background The dataset for this case study is from Hair et al.2010, Multivariate Data Analysis, Pearson Education. HBAT is a manufacturer of paper products who sells products to two market segments: the newsprint industry and the magazine industry.The current market is very competitive, so the manufacturer wants to understand how its customers perceive the company and make purchasing decisions, in order to enforce customers’ loyalty. The manufacturer commissioned a study asking its customers to complete a questionnaire on a secure website. In total,100 customers (purchasing managers from different firms) buying from HBAT completed the questionnaire. The data consist of three main pieces of information: A 1st type of information is available from HBAT’s data warehouse and includes information, such as: customer type in terms of length of purchase relationship(x1), industry type(x2), size of the customer(x3), region of the customer(x4) and distribution system(X5). The 2nd type of information is collected based on the online questionnaire and includes customers’ perceptions of HBAT ́s performance on 13 attributes using a continuous 0-10 line scale with 10 being “Excellent” and 0 being “Poor”. The 13 attributes are: X6 Product quality X7 E-commerce X8 Technical support X9 Complaint resolution X10 Advertising X11 Product line X12 Salesforce image X13 Competitive pricing X14 Warranty and claims X15 Packaging X16 Order and billing X17 Price flexibility X18 Delivery speed The 3rd type of information relates to purchase outcomes and business relationships: satisfaction with HBAT, future purchase intention, etc (X19-X22) whether the firm would consider a strategic alliance/partnership with HBAT (X23). 2.5.3.1.1 1.1. The primary objective is: to develop a taxonomy that segments the customers into groups with similar perceptions. Once identified, separate strategies with different appeals can be formulated for each segment. In addition to forming a taxonomy of customers that can be used for market segmentation, cluster analysis also facilitates data simplification (each segment is used to define the basic character of segment members) and identification of relationships (e.g. for estimating the impact of customers perceptions on sales in each segment, enabling the analyst to understant what uniquely impacts each segment rather than the sample as a whole). 2.5.3.2 2. The data (Data Understanding Phase) 2.5.3.2.1 2.1 Load data correctly library(foreign) HBAT &lt;- read.spss(file = &quot;Data/Clustering/HBAT.sav&quot;, to.data.frame=TRUE) 2.5.3.2.2 2.2 Check if the data is numerical or categorical str(HBAT) ## &#39;data.frame&#39;: 100 obs. of 24 variables: ## $ id : num 1 2 3 4 5 6 7 8 9 10 ... ## $ x1 : Factor w/ 3 levels &quot;Less than 1 year&quot;,..: 2 3 3 1 2 1 1 2 2 1 ... ## $ x2 : Factor w/ 2 levels &quot;Magazine industry&quot;,..: 1 2 1 2 1 2 2 1 2 1 ... ## $ x3 : Factor w/ 2 levels &quot;Small (0 to 499)&quot;,..: 2 1 2 2 2 1 2 2 2 2 ... ## $ x4 : Factor w/ 2 levels &quot;USA/North America&quot;,..: 2 1 2 2 1 2 2 2 2 2 ... ## $ x5 : Factor w/ 2 levels &quot;Indirect through broker&quot;,..: 2 1 2 1 2 1 1 1 1 1 ... ## $ x6 : num 8.5 8.2 9.2 6.4 9 6.5 6.9 6.2 5.8 6.4 ... ## $ x7 : num 3.9 2.7 3.4 3.3 3.4 2.8 3.7 3.3 3.6 4.5 ... ## $ x8 : num 2.5 5.1 5.6 7 5.2 3.1 5 3.9 5.1 5.1 ... ## $ x9 : num 5.9 7.2 5.6 3.7 4.6 4.1 2.6 4.8 6.7 6.1 ... ## $ x10: num 4.8 3.4 5.4 4.7 2.2 4 2.1 4.6 3.7 4.7 ... ## $ x11: num 4.9 7.9 7.4 4.7 6 4.3 2.3 3.6 5.9 5.7 ... ## $ x12: num 6 3.1 5.8 4.5 4.5 3.7 5.4 5.1 5.8 5.7 ... ## $ x13: num 6.8 5.3 4.5 8.8 6.8 8.5 8.9 6.9 9.3 8.4 ... ## $ x14: num 4.7 5.5 6.2 7 6.1 5.1 4.8 5.4 5.9 5.4 ... ## $ x15: num 4.3 4 4.6 3.6 4.5 9.5 2.5 4.8 4.4 5.3 ... ## $ x16: num 5 3.9 5.4 4.3 4.5 3.6 2.1 4.3 4.4 4.1 ... ## $ x17: num 5.1 4.3 4 4.1 3.5 4.7 4.2 6.3 6.1 5.8 ... ## $ x18: num 3.7 4.9 4.5 3 3.5 3.3 2 3.7 4.6 4.4 ... ## $ x19: num 8.2 5.7 8.9 4.8 7.1 4.7 5.7 6.3 7 5.5 ... ## $ x20: num 8 6.5 8.4 6 6.6 6.3 7.8 5.8 7.5 5.9 ... ## $ x21: num 8.4 7.5 9 7.2 9 6.1 7.2 7.7 8.2 6.7 ... ## $ x22: num 65.1 67.1 72.1 40.1 57.1 50.1 41.1 56.1 56.1 59.1 ... ## $ x23: Factor w/ 2 levels &quot;No, would not consider&quot;,..: 2 1 2 1 1 1 1 1 2 1 ... ## - attr(*, &quot;variable.labels&quot;)= Named chr [1:24] &quot;ID&quot; &quot;X1 - Customer Type&quot; &quot;X2 - Industry Type&quot; &quot;X3 - Firm Size&quot; ... ## ..- attr(*, &quot;names&quot;)= chr [1:24] &quot;id&quot; &quot;x1&quot; &quot;x2&quot; &quot;x3&quot; ... ## - attr(*, &quot;codepage&quot;)= int 65001 2.5.3.2.3 2.3 Missing treatment library(DataExplorer) plot_missing(HBAT) any(is.na.data.frame(HBAT)) ## [1] FALSE We will only work with the perceptions and hence not the factors. 2.5.3.2.4 2.4 Select relevant variables # Select variables aimed for clustering HBAT_CLU = HBAT[,c(7:19)] 2.5.3.2.5 2.5.Identify extreme outliers (the most dissimilar customers) There is no best single way to identify the outliers. Many methods are available. One option when the variables are all metric is: Calculate the difference between the observed value for a variable and the variable mean Square the difference Sum the squared differences to get a total deviations for that observation across all variables Take the square root Repeat this process for all observations Observations with the highest dissimilarity have the potential to be outliers Do not focus on the absolute value; rather, look for any value that is relatively large compare to others. diss = sqrt((HBAT_CLU$x6 - mean(HBAT_CLU$x6))^2 + (HBAT_CLU$x7 - mean(HBAT_CLU$x7))^2 + (HBAT_CLU$x8 - mean(HBAT_CLU$x8))^2 + (HBAT_CLU$x9-mean(HBAT_CLU$x9))^2 + (HBAT_CLU$x10 - mean(HBAT_CLU$x10))^2 + (HBAT_CLU$x11- mean(HBAT_CLU$x11))^2 + (HBAT_CLU$x12 - mean(HBAT_CLU$x12))^2 + (HBAT_CLU$x13 - mean(HBAT_CLU$x13))^2 + (HBAT_CLU$x14 - mean(HBAT_CLU$x14))^2 + (HBAT_CLU$x15 - mean(HBAT_CLU$x15))^2 + (HBAT_CLU$x16 - mean(HBAT_CLU$x16))^2 + (HBAT_CLU$x17- mean(HBAT_CLU$x17))^2 + (HBAT_CLU$x18 - mean(HBAT_CLU$x18))^2) # Add diss to the dataset HBAT$diss = diss # order by diss (decreasing order) newHBAT&lt;- HBAT[order(-diss),] # display the first 10% rows (the first 10 observations in our dataset) head(newHBAT[,c(1,25)], 10) id diss 87 87 6.93 7 7 6.77 92 92 6.33 90 90 6.15 6 6 6.05 84 84 5.98 22 22 5.98 57 57 5.77 48 48 5.50 72 72 5.43 # observe in the output that id 87 displays the largest diss value, meaning the highest average distance overall. However,it does not stand out over the others too much. In this dataset, we do not find any extreme outlier. 2.5.3.2.6 2.6 Standardize the variables (mean=0, std.dev=1) sd.data=scale(HBAT_CLU) 2.5.3.3 3. Hierarchical clustering (using variables) # Using complete, avg and single linkage + Euclidian distance hc.complete = hclust(dist(sd.data), method=&quot;complete&quot;) hc.average = hclust(dist(sd.data), method=&quot;average&quot;) hc.single = hclust(dist(sd.data), method=&quot;single&quot;) # Plot the dendograms par(mfrow=c(1,1)) plot(hc.complete, main=&quot;Complete linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;, ylab=&quot;&quot;) plot(hc.average, main=&quot;Average linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;, ylab=&quot;&quot;) plot(hc.single, main=&quot;Single linkage&quot;, xlab=&quot;&quot;, sub=&quot;&quot;, ylab=&quot;&quot;) Based on the dendograms we observe: the choice of linkage type affects the results none of the linkage methods yield a balanced, attractive cluster solution one reason could be that many of the variables in our dataset are highly correlated multicolinearity (very highly correlated variables) can be an issue in cluster analysis in other words, the cluster analysis works best with uncorrelated features Therefore, we have two alternatives: Select the variables so that the remaining ones are not strongly correlated to one another Run a PCA to extract the most important principal components, and afterwards fit the cluster model using PCs vector scores, instead of the original variables. PCs are expected to retain most of the information in the dataset and by definition they are orthogonal (uncorrelated) with one another. We adopt this alternative below. we see that the complete linkage appear to yield the best result. 2.5.3.4 4. Hierarchical clustering (using PCs score vectors) #Run PCA HBAT.pca = prcomp(HBAT_CLU, scale = TRUE) HBAT.pca summary(HBAT.pca) screeplot(HBAT.pca, type=&quot;line&quot;) abline(h=1, col=&quot;red&quot;, lty= 3) ## Standard deviations (1, .., p=13): ## [1] 1.8887 1.7314 1.3184 1.1346 1.0026 0.7865 0.7426 0.6686 0.5298 0.4480 ## [11] 0.4077 0.3620 0.0949 ## ## Rotation (n x k) = (13 x 13): ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 ## x6 -0.00845 -0.3621 -0.0468 0.4703 0.0501 0.05322 -0.6659 0.3323 -0.2246 ## x7 -0.24427 0.2589 0.3590 0.3422 -0.0977 0.37672 0.2079 0.1810 0.0433 ## x8 -0.06909 -0.2741 0.5250 -0.3649 0.0142 -0.05246 -0.1286 0.0214 -0.3623 ## x9 -0.45714 -0.1231 -0.2005 -0.1024 -0.0860 0.05697 -0.0189 -0.0449 -0.3091 ## x10 -0.25522 0.2097 0.1830 0.3264 0.1255 -0.78069 -0.1380 -0.2470 0.1262 ## x11 -0.25245 -0.4220 -0.1037 0.2175 -0.0321 0.16339 0.1441 -0.5145 0.0778 ## x12 -0.29099 0.2728 0.3587 0.2956 -0.0131 0.19591 0.0905 0.0613 -0.1245 ## x13 0.00825 0.4322 -0.0141 -0.1538 0.0233 0.35802 -0.6275 -0.4831 0.1301 ## x14 -0.12846 -0.2680 0.5228 -0.3228 0.1301 0.00157 -0.1071 -0.0736 0.2955 ## x15 -0.06569 0.0120 -0.0950 0.0153 0.9701 0.14594 0.1047 0.0452 -0.0289 ## x16 -0.43050 -0.1017 -0.1684 -0.1489 -0.0566 0.02876 -0.1377 0.4002 0.6684 ## x17 -0.27855 0.3751 -0.1499 -0.3457 0.0136 -0.15476 -0.0941 0.2816 -0.3011 ## x18 -0.47871 -0.0728 -0.2165 -0.0839 -0.0377 0.02089 0.0669 -0.2112 -0.2023 ## PC10 PC11 PC12 PC13 ## x6 0.00787 -0.1892 0.00495 -0.00326 ## x7 -0.56804 -0.1257 0.24275 0.02490 ## x8 -0.31786 0.3782 -0.34065 -0.00771 ## x9 0.14360 0.4215 0.64667 0.01415 ## x10 -0.13266 0.0879 0.06437 0.01273 ## x11 -0.09583 -0.0794 -0.18383 -0.57585 ## x12 0.66320 0.1459 -0.30866 -0.04986 ## x13 -0.07431 0.1026 -0.00955 0.01785 ## x14 0.26442 -0.4612 0.36090 0.01302 ## x15 -0.05604 0.0820 -0.00397 0.01143 ## x16 -0.05576 0.2641 -0.22587 -0.00806 ## x17 -0.05326 -0.3794 -0.06753 -0.53420 ## x18 -0.06681 -0.3857 -0.29611 0.61547 ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## Standard deviation 1.889 1.731 1.318 1.135 1.0026 0.7865 0.7426 0.6686 ## Proportion of Variance 0.274 0.231 0.134 0.099 0.0773 0.0476 0.0424 0.0344 ## Cumulative Proportion 0.274 0.505 0.639 0.738 0.8150 0.8626 0.9050 0.9394 ## PC9 PC10 PC11 PC12 PC13 ## Standard deviation 0.5298 0.4480 0.4077 0.3620 0.09490 ## Proportion of Variance 0.0216 0.0154 0.0128 0.0101 0.00069 ## Cumulative Proportion 0.9610 0.9764 0.9892 0.9993 1.00000 Based on the elbow, eigen values and % of variance explained criteria, we may decide to select the first 4 PCs as the most important (see Case Study PCA from last week for full details). Hierarchical clustering based on the first 4 PCs score vectors using Euclidian distance (dist) using linkage=complete, average, or single hc.complete= hclust(dist(HBAT.pca$x[,1:4]), method=&quot;complete&quot;) hc.average= hclust(dist(HBAT.pca$x[,1:4]), method=&quot;average&quot;) hc.single= hclust(dist(HBAT.pca$x[,1:4]), method=&quot;single&quot;) plot(hc.complete, main=&quot;Hierarchical cluster with complete linkage&quot;) plot(hc.average, main=&quot;Hierarchical cluster with average linkage&quot;) plot(hc.single, main=&quot;Hierarchical cluster with single linkage&quot;) Looking at the dendograms: Single linkage does not perform well again. Likewise, average linkage creates innapropriate clusters. Complete linkage creates more appropriate clusters. It comes out that 3-cluster is the most plausible solution. To visualize the dendogram with red borders around the 3 clusters uncomment the next lines: hc.complete &lt;- hclust(dist(HBAT.pca$x[,1:4]), method=“complete”) plot(hc.complete, main=“Hierarchical cluster with complete linkage”) rect.hclust(hc.complete , k = 3, border = “red”) Before proceeding with K-means, we can profile the original variables on the three clusters to confirm that the differences between the clusters are distinctive. # Save the clusters by cutting the tree into 3 clusters hc.clusters = cutree(hc.complete,3) # Add the cluster variable to my data HBAT_CLU$clusters = hc.clusters # Check size of the clusters table(HBAT_CLU$clusters) ## ## 1 2 3 ## 26 53 21 Run Anova to see if differences in means between clusters are significant and to define the characteristics of the clusters. The independent variable: cluster membership. The dependent variables: the 13 original variables. Does not appear to run The F-statistic from one-way ANOVA examine if there are sig. differences between the three clusters on the respective independent variable. The results indicate sig. differences with respect to all the variables, except x6, x13 and x15. We may examine the means for the significant variables and characterize the clusters. Examples: For some reason, this chunk cannot be knitted hence eval=FALSE # Run the same for: x14, x16, x17, x18 # Interpreting the clusters by looking at the extremes (highest and lowest means): # - Cluster 1 has a relatively lower mean on x9 and x11 variables than the other two clusters. This means that customers in this cluster are unhappy about complaint resolution (x9) and product line (x11) # - Cluster 2 has a relatively higher mean on x8 (Technical support) and x11(Product line), than the other two clusters. # - Cluster 3 has a relatively higher mean on x7(E-commerce perceptions), x9(complain resolution), x10(advertising), x12 (salesforce image), than the other two clusters. # - Overall, as expected, these results indicate that each of the three clusters exibit somewhat distinctive characteristics. # NOTE: the label assigned to clusters &quot;1&quot;, &quot;2&quot;, &quot;3&quot; can changes from one computer to another. 2.5.3.5 5. K-means In k-means clustering the analyst must specify the number of clusters to extract. But how can one know how many segments exist in the population? One possibility is to rely on theory. The analyst might expect a certain number of segments in the population investigated based on previous knowledge. Another possibility is to use the results from Hierarchical Clustering.There we identified 3 clusters.Thus, we run K-means clutering with K=3. set.seed(1) km.out= kmeans(HBAT.pca$x[,1:4], 3, nstart=20) km.out # evaluate this output km.clusters=km.out$cluster # saving the clusters ## K-means clustering with 3 clusters of sizes 39, 30, 31 ## ## Cluster means: ## PC1 PC2 PC3 PC4 ## 1 -0.3330352 -1.7369154 -0.08640935 0.26069725 ## 2 -1.6905084 1.3919404 0.05935694 -0.31601740 ## 3 2.0549557 0.8381126 0.05126634 -0.02215066 ## ## Clustering vector: ## [1] 2 1 1 3 1 3 3 3 2 2 1 3 2 1 2 1 2 2 2 2 3 1 1 1 2 2 1 3 1 3 1 3 3 2 3 3 1 ## [38] 1 2 2 3 1 2 2 1 2 1 2 1 1 3 1 1 3 2 1 2 1 1 1 1 3 3 3 3 2 2 2 3 2 2 3 2 1 ## [75] 2 1 2 1 1 3 1 1 3 3 1 3 3 1 1 2 1 3 1 1 3 1 3 3 2 3 ## ## Within cluster sum of squares by cluster: ## [1] 181.6540 162.0146 181.0767 ## (between_SS / total_SS = 44.7 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; We can compare k-means with hierarchical solution (if we want) using a cross-classification table: table(km.clusters, hc.clusters) ## hc.clusters ## km.clusters 1 2 3 ## 1 1 36 2 ## 2 4 7 19 ## 3 21 10 0 Perfect fit would appear if only one cell in each row or column of the table contained a value. The two methods produce quite different clusters (this is typical), but most of the observations in k-means are grouped with the same observations they cluster with in the hc.cluster solution. In practice the researchers rely on the K-means solution. We now interpret the meaning of the clusters by analysing the pattern of their cluster means (as we did before with hierarchical clustering) HBAT_CLU$clustersK = km.out$cluster summary(aov(HBAT_CLU$x6 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x7 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x8 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x9 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x10 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x11 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x12 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x13 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x14 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x15 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x16 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x17 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) summary(aov(HBAT_CLU$x18 ~ HBAT_CLU$clustersK , data=HBAT_CLU[ , 1:13])) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 55.28 55.28 39.33 9.68e-09 *** ## Residuals 98 137.73 1.41 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 0.05 0.0507 0.102 0.75 ## Residuals 98 48.53 0.4952 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 19.61 19.610 9.053 0.00333 ** ## Residuals 98 212.28 2.166 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 39.84 39.84 37.28 2.06e-08 *** ## Residuals 98 104.73 1.07 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 0.1 0.1036 0.081 0.777 ## Residuals 98 125.6 1.2819 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 92.64 92.64 115.5 &lt;2e-16 *** ## Residuals 98 78.63 0.80 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 0.0 0.000 0 0.999 ## Residuals 98 113.8 1.162 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 55.76 55.76 30.27 3.01e-07 *** ## Residuals 98 180.57 1.84 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 8.62 8.623 14.6 0.000234 *** ## Residuals 98 57.90 0.591 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 0.54 0.5365 0.239 0.626 ## Residuals 98 220.15 2.2465 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 22.81 22.810 35.71 3.71e-08 *** ## Residuals 98 62.60 0.639 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 4.92 4.924 3.47 0.0655 . ## Residuals 98 139.07 1.419 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 16.48 16.483 43.76 1.98e-09 *** ## Residuals 98 36.92 0.377 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 According to Anova F-test, the K-means clusters differ significantly based on x6,x7,x8,x10,x11,x12,x13,x17. As we did before for non-hierarchical clustering, interpretation of differences among clusters begins by looking for extreme mean values associated with each cluster. In other words, variable means that are the highest or lowest compared to other clusters are useful in the process of interpreting the clusters. We can take a look at the plots of the means: Example for x6: library(&quot;ggpubr&quot;) ggboxplot(HBAT_CLU, x = &quot;clustersK&quot;, y = &quot;x13&quot;, color = &quot;clustersK&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), ylab = &quot;x13&quot;, xlab = &quot;ClustersK&quot;) # To complement the descriptive analysis, we ask for a statistical differences between means based on TukeyHSD(Honestly Significant Difference) TukeyHSD(aov(HBAT_CLU$x6 ~ as.factor(HBAT_CLU$clustersK), data=HBAT_CLU[ ,1:13])) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = HBAT_CLU$x6 ~ as.factor(HBAT_CLU$clustersK), data = HBAT_CLU[, 1:13]) ## ## $`as.factor(HBAT_CLU$clustersK)` ## diff lwr upr p adj ## 2-1 -2.0446154 -2.6544070 -1.4348237 0.0000000 ## 3-1 -1.7028950 -2.3071019 -1.0986880 0.0000000 ## 3-2 0.3417204 -0.3013718 0.9848127 0.4184721 check the output Idem evaluate the means for the rest of the variables. Interpretation of the clusters: Cluster 1 has 39 observations and is most distiguished by a relatively higher mean on x6, and x?, x?…, than the other two clusters. Cluster 2 has 31 observations and is mostly distiguished by a relatively higher /lower mean on ….,than the other two clusters. Cluster 3 has 30 observations and is most distiguished by a relatively higher /lower mean on …., than the other two clusters. Using the pca3d library we represent the cluster solution into the first 2 PCs dimensions. This is another descriptive illustration of the clusters and their profile. For some reason this cannot be run when knitting library(pca3d) pca2d(HBAT.pca, biplot=TRUE, group=km.clusters, legend = &quot;topright&quot;, show.ellipses=TRUE, ellipse.ci=0.75, show.plane=TRUE, show.labels = TRUE) # See Case Study PCA for biplot interpretation. 2.5.3.6 6. Validation and profiling the clusters The analyst should perform tests to confirm the validity of the cluster solution while also ensuring the cluster solution has practical significance. 2.5.3.6.1 6.1. Assessing cluster stability The stability of the cluster solution can be evaluated by splitting the data (if sufficiently big), running two cluster analysis and comparing the results. 2.5.3.6.2 6.2. Assessing criterion (predictive) validity To assess predictive validity of the clusters, we can focus on other variables that have a theoretically based relationship to the clustering variables, but were not included (!) in the cluster analysis. Given this relationship, we should see significant differences in these variables across the clusters. If significant differences do exist, we can draw the conclusion that the clusters depict groups that have predictive validity. For this purpose, we consider four outcome measures from the HBAT, x19 Satisfaction x20 Likelihood to recommend x21 Likelihood to purchase x22 Purchase level # MANOVA allows us to compare simultaneusly the means of these continuous variables among the clusters manova.model &lt;- manova(cbind(x19,x20,x21,x22) ~ HBAT_CLU$clustersK, data = HBAT) summary(manova.model) ## Df Pillai approx F num Df den Df Pr(&gt;F) ## HBAT_CLU$clustersK 1 0.49108 22.918 4 95 2.831e-13 *** ## Residuals 98 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # The overall F-test is significant at 10% level of significance =&gt; initial evidence that some these variables can be predicted by simply knowing to which cluster an HBAT customer belongs. # To check individual F-tests: summary.aov(manova.model) ## Response x19 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 48.930 48.930 52.292 1.064e-10 *** ## Residuals 98 91.698 0.936 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Response x20 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 30.296 30.2956 38.327 1.399e-08 *** ## Residuals 98 77.464 0.7905 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Response x21 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 16.370 16.3700 22.793 6.314e-06 *** ## Residuals 98 70.383 0.7182 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Response x22 : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## HBAT_CLU$clustersK 1 3413.8 3413.8 76.745 5.898e-14 *** ## Residuals 98 4359.2 44.5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Note the output reveals significant F-test for x20, x21 (at 10% level) and x22 # Digging more into which specific means differ between the three clusters: HBAT$clustersK = HBAT_CLU$clustersK library(&quot;ggpubr&quot;) ggboxplot(HBAT, x = &quot;clustersK&quot;, y = &quot;x20&quot;, color = &quot;clustersK&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), ylab = &quot;x20&quot;, xlab = &quot;ClustersK&quot;) # For example we see that cluster 1 is the most likely to recommend and cluster 2 is the least likely. library(&quot;ggpubr&quot;) ggboxplot(HBAT, x = &quot;clustersK&quot;, y = &quot;x21&quot;, color = &quot;clustersK&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), ylab = &quot;x21&quot;, xlab = &quot;ClustersK&quot;) # Idem we see that cluster 1 is the most likely to purchase and cluster 2 is the least likely. library(&quot;ggpubr&quot;) ggboxplot(HBAT, x = &quot;clustersK&quot;, y = &quot;x22&quot;, color = &quot;clustersK&quot;, palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;, &quot;#FC4E07&quot;), ylab = &quot;x22&quot;, xlab = &quot;ClustersK&quot;) # Idem we see that cluster 1 has the highest purchase level and cluster 2 is the lowest. 2.5.3.6.3 6.3. Profiling the final cluster solution A final task can be to profile the clusters on a set of additional qualitative variables, not included in the cluster analysis. In this example, five characteristics of HBAT customers are available: length of purchase relationship(x1) industry type(x2) size of the customer(x3) region of the customer(x4) distribution system(X5) # Cross-classification table allows us to profile the clusters based on these categorical variables # Chi-square test library(MASS) tblx1 = table(HBAT$x1, HBAT_CLU$clustersK) tblx1 ## ## 1 2 3 ## Less than 1 year 2 5 25 ## 1 to 5 years 8 22 5 ## Over 5 years 29 3 1 chisq.test(tblx1) ## ## Pearson&#39;s Chi-squared test ## ## data: tblx1 ## X-squared = 85.369, df = 4, p-value &lt; 2.2e-16 # Significant chi-square value reveals for example that x1 (customer type) tends to be associated with certain clusters. In my output observe that customers of &quot;Over 5 years&quot; tend to be concentrated in cluster 1 (recall these were customers that were also most likely to recommend and to purchase). Cluster 2 consists predominantly of customers of &quot;less than 1 year&quot; (they were the customers least likely to recommend and ourchase in the future); and cluster 3 consists predominantly of customers between 1 to 5 years. Idem, we can evaluate x2, x3, x4 and x5. From these qualitative variables, distinctive profiles can be developed for each cluster. These profiles support the distinctiveness of the clusters on variables not used in the analysis at any prior point. 2.5.3.7 7. Final remarks The clustering techniques discussed (hierarchical and K-means) can be applied to numeric (continuous) variables. This course focuses exclusively on these two methods. For clustering with categorical variables, one option is to use Expectation Maximization algorithm, which yields probabilistic classes (clusters). Finally, if interested in clustering with both categorical and continous variables, refer to a new package in R called ‘clustMixType’ (k-Prototypes Clustering for Mixed Variable-Type Data) which can handle mixed data. "],["recommender-systems-and-customer-targeting.html", "3 Recommender Systems and Customer Targeting 3.1 Association rule 3.2 Colaborative filtering 3.3 Latent Factor Models", " 3 Recommender Systems and Customer Targeting This group of lectures cover three different approaches to mining associations. Association rules Collaborative filtering Latent factor models Association rules We see that association rules looks at transactions and assess what products there appear to be on the same transaction, and thus creates association rules based on these. With these rules you can start making recommendations or generate ideas of what products should be marketed together. For instance one of the fun examples is where you see that beers and diapers often comes together. When you think about it, it is very logical, because people with small kids cant go out, but they might still enjoy a beer, thus they have to do it at home. Collaborative filtering Then we have collaborative filtering. Here we will work on two different approaches, item-based and user-based. + User-based: we want to predict one users ratings of a given product/service based on similarity with other users. + item-based: we want to predict one users ratings of a given product/service based on its similarity with other products, e.g., if one consistenly rate thai food high, then he may also like other thai food, this is in principle also what I have explored with the recommender system on the DSP. This is very much used for making recommender system. This was also very much applied in the Netflix competition. With this, we have the term cold start, that is when you have a new user, you must make some recommendations before you can use the contents, e.g., what you are doing at Netflix, so it starts knowing your preferences. Latent factor models The last approach is the latent factor model, this is just an extension of the user-based and item-based, where both scenarios are considered. This method assumes that we make some constructs, e.g., as in SEM, where the constructs are explained by different items (movies). Then it would be obvious to make constructs based on different genres. These constructs are not directly observable, hence the name latent. Within LFM cold start also applies. In general we see that this model gives the best predictions. That is also due to the possibility of regularizing user ratings, hence accounting for user specific biases. Notice that in the very end of this chapter a comparison between IBCF, UBCF, latent factor models and latent factor models accounting for user bias is shown. 3.1 Association rule Basically the method looks at what products that appear in the same transaction and the finds pairs, or items that come together. Terms: + Items: these are the different products that can be bought, can also be a product group + Itemset: this is the dataset containing a list of all of the items + Syntatic Constrains: constraints involving restrictions on items that can appear in a rule. E.g., one may say, that we only want to evaluate one item (product) or an itemrange. + Support Constraints: constraints the number of transactions in T that support a rule. Notice that in R there are default values for this. + Minsupport: This is the threshold assigned to item pairs, used in procedure 1. + Large itemssets: Item combinations that meets the minsupport. This is denoted with I1,I2,…Ik. + Small itemssets: Item combinations that does not meet the minsupport Procedure: 1. Generate all combinations of items that have fractional transactinos support. Perhaps above a certain threshold. a. We group all combinations in two groups: i. Large itemsets. Examples: 1) Diaper and beer, 2) Milk, Bread –&gt; Eggs, Coke. ii. Small itemsets. For a large itemset generate all rules, that use items from the large itemsets (I1,I2,…Ik). We call all of these rules subsets (X) of Y. # https://www.datacamp.com/community/tutorials/market-basket-analysis-r #install and load package arules library(arules) library(arulesViz) library(tidyverse) library(readxl) library(knitr) library(ggplot2) library(lubridate) library(plyr) library(dplyr) library(DataExplorer) 3.1.1 Loading and data exploration First we load it #read excel into R dataframe retail &lt;- read_excel(&#39;Data/Recommender Systems/Online_Retail.xlsx&#39;) attach(retail) The data frame has rows with granularity of items, hence one item pr. line. Where one invoice may occur several times. We also have some other information. notice that stock code is an ID for the product. We see that some have some odd quantities, these are just plotted to see what is going on. plot(Quantity[StockCode == 23843]) plot(Quantity[StockCode == 23166]) 3.1.2 Data preprocessing complete.cases(data) will return a logical vector indicating which rows have no missing values. Then use the vector to get only rows that are complete using retail[,]. That is because we have NA’s. plot_missing(retail) We see that the customer ID is only there 75% of the time, also the description appear to be missing in some examples. Now we are going to remove these. retail &lt;- retail[complete.cases(retail), ] plot_missing(retail) Now we see that we remove approx. 140.000 observations, where we had 541.909 observations before and 406.829 observations after removing NA’s, corresponding with approx. 25% of the observations. 3.1.2.1 Encoding to correct data types Now we want to make the description a factor instead of character. #The following prints whole tables, although I just show the first couple of rows in a following chunk retail %&gt;% mutate(Description = as.factor(Description)) retail %&gt;% mutate(Country = as.factor(Country)) retail$Date &lt;- as.Date(retail$InvoiceDate) head(retail) InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country Date 536365 85123A WHITE HANGING HEART T-LIGHT HOLDER 6 2010-12-01 08:26:00 2.55 17850 United Kingdom 2010-12-01 536365 71053 WHITE METAL LANTERN 6 2010-12-01 08:26:00 3.39 17850 United Kingdom 2010-12-01 536365 84406B CREAM CUPID HEARTS COAT HANGER 8 2010-12-01 08:26:00 2.75 17850 United Kingdom 2010-12-01 536365 84029G KNITTED UNION FLAG HOT WATER BOTTLE 6 2010-12-01 08:26:00 3.39 17850 United Kingdom 2010-12-01 536365 84029E RED WOOLLY HOTTIE WHITE HEART. 6 2010-12-01 08:26:00 3.39 17850 United Kingdom 2010-12-01 536365 22752 SET 7 BABUSHKA NESTING BOXES 2 2010-12-01 08:26:00 7.65 17850 United Kingdom 2010-12-01 We also want a column with the date and one with the transaction time. #Extract time from InvoiceDate and store in another variable TransTime &lt;- format(retail$InvoiceDate,&quot;%H:%M:%S&quot;) #Convert and edit InvoiceNo into numeric InvoiceNo &lt;- as.numeric(as.character(retail$InvoiceNo)) #Bind new columns TransTime and InvoiceNo into dataframe retail retail &lt;- cbind(retail,TransTime) Now we can glimpse glimpse(retail) ## Rows: 406,829 ## Columns: 10 ## $ InvoiceNo &lt;chr&gt; &quot;536365&quot;, &quot;536365&quot;, &quot;536365&quot;, &quot;536365&quot;, &quot;536365&quot;, &quot;536365&quot;… ## $ StockCode &lt;chr&gt; &quot;85123A&quot;, &quot;71053&quot;, &quot;84406B&quot;, &quot;84029G&quot;, &quot;84029E&quot;, &quot;22752&quot;, … ## $ Description &lt;chr&gt; &quot;WHITE HANGING HEART T-LIGHT HOLDER&quot;, &quot;WHITE METAL LANTERN… ## $ Quantity &lt;dbl&gt; 6, 6, 8, 6, 6, 2, 6, 6, 6, 32, 6, 6, 8, 6, 6, 3, 2, 3, 3, … ## $ InvoiceDate &lt;dttm&gt; 2010-12-01 08:26:00, 2010-12-01 08:26:00, 2010-12-01 08:2… ## $ UnitPrice &lt;dbl&gt; 2.55, 3.39, 2.75, 3.39, 3.39, 7.65, 4.25, 1.85, 1.85, 1.69… ## $ CustomerID &lt;dbl&gt; 17850, 17850, 17850, 17850, 17850, 17850, 17850, 17850, 17… ## $ Country &lt;chr&gt; &quot;United Kingdom&quot;, &quot;United Kingdom&quot;, &quot;United Kingdom&quot;, &quot;Uni… ## $ Date &lt;date&gt; 2010-12-01, 2010-12-01, 2010-12-01, 2010-12-01, 2010-12-0… ## $ TransTime &lt;chr&gt; &quot;08:26:00&quot;, &quot;08:26:00&quot;, &quot;08:26:00&quot;, &quot;08:26:00&quot;, &quot;08:26:00&quot;… 3.1.2.2 Create a data frame with one row pr. invoice What you need to do is group data in the retail dataframe either by CustomerID, CustomerID, and Date or you can also group data using InvoiceNo and Date. We need this grouping and apply a function on it and store the output in another dataframe. This can be done by ddply. The following lines of code will combine all products from one InvoiceNo and date and combine all products from that InvoiceNo and date as one row, with each item, separated by “,”. library(plyr) transactionData &lt;- ddply(.data = retail ,.variables = c(&quot;InvoiceNo&quot;,&quot;Date&quot;) #Group by ,.fun = function(df1)paste(df1$Description #This concatenates the items ,collapse = &quot;,&quot;)) head(transactionData) InvoiceNo Date V1 536365 2010-12-01 WHITE HANGING HEART T-LIGHT HOLDER,WHITE METAL LANTERN,CREAM CUPID HEARTS COAT HANGER,KNITTED UNION FLAG HOT WATER BOTTLE,RED WOOLLY HOTTIE WHITE HEART.,SET 7 BABUSHKA NESTING BOXES,GLASS STAR FROSTED T-LIGHT HOLDER 536366 2010-12-01 HAND WARMER UNION JACK,HAND WARMER RED POLKA DOT 536367 2010-12-01 ASSORTED COLOUR BIRD ORNAMENT,POPPY’S PLAYHOUSE BEDROOM,POPPY’S PLAYHOUSE KITCHEN,FELTCRAFT PRINCESS CHARLOTTE DOLL,IVORY KNITTED MUG COSY,BOX OF 6 ASSORTED COLOUR TEASPOONS,BOX OF VINTAGE JIGSAW BLOCKS,BOX OF VINTAGE ALPHABET BLOCKS,HOME BUILDING BLOCK WORD,LOVE BUILDING BLOCK WORD,RECIPE BOX WITH METAL HEART,DOORMAT NEW ENGLAND 536368 2010-12-01 JAM MAKING SET WITH JARS,RED COAT RACK PARIS FASHION,YELLOW COAT RACK PARIS FASHION,BLUE COAT RACK PARIS FASHION 536369 2010-12-01 BATH BUILDING BLOCK WORD 536370 2010-12-01 ALARM CLOCK BAKELIKE PINK,ALARM CLOCK BAKELIKE RED,ALARM CLOCK BAKELIKE GREEN,PANDA AND BUNNIES STICKER SHEET,STARS GIFT TAPE,INFLATABLE POLITICAL GLOBE,VINTAGE HEADS AND TAILS CARD GAME,SET/2 RED RETROSPOT TEA TOWELS,ROUND SNACK BOXES SET OF4 WOODLAND,SPACEBOY LUNCH BOX,LUNCH BOX I LOVE LONDON,CIRCUS PARADE LUNCH BOX,CHARLOTTE BAG DOLLY GIRL DESIGN,RED TOADSTOOL LED NIGHT LIGHT,SET 2 TEA TOWELS I LOVE LONDON,VINTAGE SEASIDE JIGSAW PUZZLES,MINI JIGSAW CIRCUS PARADE,MINI JIGSAW SPACEBOY,MINI PAINT SET VINTAGE,POSTAGE So we see that now we have granularity on invoiceNo, and the the date and lastly the items are listed, where they are separated with a comma. Now we only want the items pr. invoice (basket), hence we remove the invoice InvoiceNo and Date. transactionData &lt;- as.data.frame(transactionData$V1) colnames(transactionData) &lt;- c(&quot;items&quot;) #Rename column to items transactionData[1,] ## [1] &quot;WHITE HANGING HEART T-LIGHT HOLDER,WHITE METAL LANTERN,CREAM CUPID HEARTS COAT HANGER,KNITTED UNION FLAG HOT WATER BOTTLE,RED WOOLLY HOTTIE WHITE HEART.,SET 7 BABUSHKA NESTING BOXES,GLASS STAR FROSTED T-LIGHT HOLDER&quot; We see that the dataframe now consists of only on column with all items separated with a comma. Now we are going to save it, and load it back into the environment. Saving the data write.csv(transactionData,&quot;Data/Recommender Systems/market_basket_transactions.csv&quot;, quote = FALSE, row.names = FALSE) 3.1.3 Loading transactions tr &lt;- read.transactions(&#39;market_basket_transactions.csv&#39;, format = &#39;basket&#39;, sep=&#39;,&#39;) summary(tr) ## transactions as itemMatrix in sparse format with ## 22191 rows (elements/itemsets/transactions) and ## 7876 columns (items) and a density of 0.00193 ## ## most frequent items: ## WHITE HANGING HEART T-LIGHT HOLDER REGENCY CAKESTAND 3 TIER ## 1803 1709 ## JUMBO BAG RED RETROSPOT PARTY BUNTING ## 1460 1285 ## ASSORTED COLOUR BIRD ORNAMENT (Other) ## 1250 329938 ## ## element (itemset/transaction) length distribution: ## sizes ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 3598 1594 1141 908 861 758 696 676 663 593 624 537 516 531 551 522 ## 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ## 464 441 483 419 395 315 306 272 238 253 229 213 222 215 170 159 ## 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 ## 138 142 134 109 111 90 113 94 93 87 88 65 63 67 63 60 ## 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 ## 59 49 64 40 41 49 43 36 29 39 30 27 28 17 25 25 ## 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 ## 20 27 24 22 15 20 19 13 16 16 11 15 12 7 9 14 ## 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 ## 15 12 8 9 11 11 14 8 6 5 6 11 6 4 4 3 ## 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 ## 6 5 2 4 2 4 4 3 2 2 6 3 4 3 2 1 ## 113 114 116 117 118 120 121 122 123 125 126 127 131 132 133 134 ## 3 1 3 3 3 1 2 2 1 3 2 2 1 1 2 1 ## 140 141 142 143 145 146 147 150 154 157 168 171 177 178 180 202 ## 1 2 2 1 1 2 1 1 3 2 2 2 1 1 1 1 ## 204 228 236 249 250 285 320 400 419 ## 1 1 1 1 1 1 1 1 1 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 3 10 15 21 419 ## ## includes extended item information - examples: ## labels ## 1 1 HANGER ## 2 10 COLOUR SPACEBOY PEN ## 3 12 COLOURED PARTY BALLOONS We see that when we load it back into the environment using read.transactions() we create something similar to a data term matrix, this is merely a sparse matrix, where we see that we have 7.876 items and 22.191 baskets/invoices. Hence each row is just ‘ticked-off’ if the basket contains the product. We see the summary specify that we have 22.191 transactions and 7.876 items, the most frequent is ‘white hanging heeart t-light holder’. We see that the largest basket has 419 units. Where basket sizes of 1 unit is the most typical. Then the mean = 15 products and the median 10 products. 3.1.4 Plotting Now we want to plot the most frequent products. # Create an item frequency plot for the top 20 items library(RColorBrewer) itemFrequencyPlot(tr ,topN = 20 ,type = &quot;absolute&quot; ,col = brewer.pal(8,&#39;Pastel2&#39;) ,main = &quot;Absolute Item Frequency Plot&quot;) We see that top combinations. E.g., more than 1.500 baskets where the top combination occurs. Now we can look at it in a relative way instead. itemFrequencyPlot(tr ,topN = 20 ,type = &quot;relative&quot; ,col = brewer.pal(8,&#39;Pastel2&#39;) ,main = &quot;Relative Item Frequency Plot&quot;) This is basically the same, just written in percentage. 3.1.5 Apriori calculation We will now start looking at association rules. To do so, we apply the apriori principle. We set the following: supp = minimum support, where we see that the combinations must occur in at least 0.1% of the baskets conf = the describes the conditional relationship between the left-hand and right-hand side. Hence we want a relationship that is greater than 80%, as we want to avoid random pairs. maxlen = the maximum length of items in a basket. # Min Support as 0.001, confidence as 0.8. association.rules &lt;- apriori(data = tr ,parameter = list(supp = 0.001 #we only see rules where the support is at least 0.001 ,conf = 0.8 #The confidence should be at least 0.8 ,maxlen = 10 #We only want ot look at a max length of 10 items pr. basket ) ) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.8 0.1 1 none FALSE TRUE 5 0.001 1 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 22 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[7876 item(s), 22191 transaction(s)] done [0.11s]. ## sorting and recoding items ... [2324 item(s)] done [0.01s]. ## creating transaction tree ... done [0.01s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 done [0.30s]. ## writing ... [49122 rule(s)] done [0.04s]. ## creating S4 object ... done [0.05s]. The parameters can be seen as constraints, to lower the amount of rules that we are going to make Extra note on support. We only want to see combinations that occurs 1 out of 1000 times, hence supp = 0.001 Now we have calculated association rules lets call the summary. summary(association.rules) ## set of 49122 rules ## ## rule length distribution (lhs + rhs):sizes ## 2 3 4 5 6 7 8 9 10 ## 105 2111 6854 16424 14855 6102 1937 613 121 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.0 5.0 5.0 5.5 6.0 10.0 ## ## summary of quality measures: ## support confidence coverage lift ## Min. :0.00104 Min. :0.800 Min. :0.00104 Min. : 10 ## 1st Qu.:0.00108 1st Qu.:0.833 1st Qu.:0.00126 1st Qu.: 22 ## Median :0.00126 Median :0.879 Median :0.00144 Median : 29 ## Mean :0.00142 Mean :0.885 Mean :0.00161 Mean : 65 ## 3rd Qu.:0.00153 3rd Qu.:0.926 3rd Qu.:0.00171 3rd Qu.: 69 ## Max. :0.01600 Max. :1.000 Max. :0.01911 Max. :716 ## count ## Min. : 23 ## 1st Qu.: 24 ## Median : 28 ## Mean : 31 ## 3rd Qu.: 34 ## Max. :355 ## ## mining info: ## data ntransactions support confidence ## tr 22191 0.001 0.8 We see some basic descriptions, e.g., we have the most combinations around 5 and 6. We see that the lower end and upper end has very few. My guess would be that for a basket combination of 10 to not be random, there are only few things that are naturally there. In the other end (e.g., with length of 2), then there are many products that comes in pairs, but it is also influenced by a lot of randomness. Hence we are ruling out a lot of combinations with the confidence = 80%, if we were to decrease the confidence, then we could have found more association rules. Now we can inspect the first 10 combinations inspect(association.rules[1:10]) ## lhs rhs support confidence coverage lift count ## [1] {WOBBLY CHICKEN} =&gt; {DECORATION} 0.00126 1.000 0.00126 444 28 ## [2] {WOBBLY CHICKEN} =&gt; {METAL} 0.00126 1.000 0.00126 444 28 ## [3] {DECOUPAGE} =&gt; {GREETING CARD} 0.00104 1.000 0.00104 389 23 ## [4] {BILLBOARD FONTS DESIGN} =&gt; {WRAP} 0.00131 1.000 0.00131 716 29 ## [5] {WRAP} =&gt; {BILLBOARD FONTS DESIGN} 0.00131 0.935 0.00140 716 29 ## [6] {ENAMEL PINK TEA CONTAINER} =&gt; {ENAMEL PINK COFFEE CONTAINER} 0.00140 0.816 0.00171 385 31 ## [7] {WOBBLY RABBIT} =&gt; {DECORATION} 0.00153 1.000 0.00153 444 34 ## [8] {WOBBLY RABBIT} =&gt; {METAL} 0.00153 1.000 0.00153 444 34 ## [9] {ART LIGHTS} =&gt; {FUNK MONKEY} 0.00171 1.000 0.00171 584 38 ## [10] {FUNK MONKEY} =&gt; {ART LIGHTS} 0.00171 1.000 0.00171 584 38 We see that the confidence is one in many examples, that is for instance because when you have bought wobbly chicken, the you also buy decoration and for this pair it also goes the other way around. We see an example where the coverage is not one, that is because when you buy one, then you do not always buy the other. That is calculated by: \\[\\frac{support}{coverage} = confidence\\] We see that: Support = \\(\\frac{TotalInvoices}{AbsoluteNo.OfTransactions} = Support\\) Coverage = \\(\\frac{Support}{Confidence} = Coverage\\) Lift = \\(\\frac{P(A|B)}{P(A)} = Lift\\), we see with lift, that we actually need to find out how the pairs appear. See an example in the following. Lift example Manual calculation of support, coverage and confidence We are going to do three things: Count no. of transactions with the left hand side combination Count the no. of transactions where the right hand side combination is within the transactions found in step 1. Divide support by coverage to find confidence. If 1 = the right hand side combination is always in the basket when the left hand side is, when e.g., 0.5 instead, then the right hand side is only half of the time in the basket when the left hand side is. #We are going to look at the pair woobly chicken and decoration left &lt;- &quot;SUNSET CHECK HAMMOCK&quot; right &lt;- &quot;UNION STRIPE WITH FRINGE HAMMOCK&quot; #Find row number of a given item idx &lt;- which(t(t(as.vector(tr@itemInfo))) == left) table(tr@data[idx,]) ## ## FALSE TRUE ## 22152 39 coverage = as.vector(table(tr@data[idx,])[2]/ncol(tr@data)) idx &lt;- which(t(t(as.vector(tr@itemInfo))) == left) idx_right &lt;- which(t(t(as.vector(tr@itemInfo))) == right) #which(tr@data[idx,] == TRUE) #which(tr@data[idx_right,] == TRUE) right_side &lt;- table(which(tr@data[idx,] == TRUE) %in% which(tr@data[idx_right,] == TRUE)) right_side #Inteprete to see if there are only T, F or both. ## ## FALSE TRUE ## 14 25 #We see that decoration is always in the basket when wobbly chicken is. support = as.vector(right_side[2]/ncol(tr@data)) support ## [1] 0.00113 confidence = support / coverage confidence #Hence a very stron relationship from woobly chicken to decoration ## [1] 0.641 Now we can inspect the rules after they are sorted sortedRules &lt;- sort(association.rules,by=&quot;lift&quot;,decreasing=TRUE) inspect(sortedRules[c(1:10)]) Sorted Rules Output The conclusions are the same, it is just sorted now. The LIFT reflects how often the LHS is bought given the RHS is in the basket. 3.1.5.1 Another example where maxlen = 3 Here we only want to look at combinations with up to three items. shorter.association.rules &lt;- apriori(tr, parameter = list(supp=0.001 #must occur in 1 in 1000 ,conf=0.8 #We need to pairs to be at least 80% of the time together ,maxlen=3)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.8 0.1 1 none FALSE TRUE 5 0.001 1 ## maxlen target ext ## 3 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 22 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[7876 item(s), 22191 transaction(s)] done [0.12s]. ## sorting and recoding items ... [2324 item(s)] done [0.01s]. ## creating transaction tree ... done [0.01s]. ## checking subsets of size 1 2 3 done [0.12s]. ## writing ... [2216 rule(s)] done [0.02s]. ## creating S4 object ... done [0.01s]. Notice that the RHS and LHS summarize up to 3, where before it could go all the way up to 10. 3.1.5.2 Filtering on right hand side. An example with the item {METAL} We want to see what is purchased before buying metal. #For example, to find what customers buy before buying &#39;METAL&#39; run the following line of code metal.association.rules &lt;- apriori(data = tr ,parameter = list(supp=0.001, conf=0.8) ,appearance = list(default=&quot;lhs&quot;,rhs=&quot;METAL&quot;)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.8 0.1 1 none FALSE TRUE 5 0.001 1 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 22 ## ## set item appearances ...[1 item(s)] done [0.00s]. ## set transactions ...[7876 item(s), 22191 transaction(s)] done [0.12s]. ## sorting and recoding items ... [2324 item(s)] done [0.01s]. ## creating transaction tree ... done [0.01s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 done [0.24s]. ## writing ... [5 rule(s)] done [0.03s]. ## creating S4 object ... done [0.01s]. inspect(metal.association.rules) ## lhs rhs support confidence coverage ## [1] {WOBBLY CHICKEN} =&gt; {METAL} 0.001261773 1 0.001261773 ## [2] {WOBBLY RABBIT} =&gt; {METAL} 0.001532153 1 0.001532153 ## [3] {DECORATION} =&gt; {METAL} 0.002253166 1 0.002253166 ## [4] {DECORATION,WOBBLY CHICKEN} =&gt; {METAL} 0.001261773 1 0.001261773 ## [5] {DECORATION,WOBBLY RABBIT} =&gt; {METAL} 0.001532153 1 0.001532153 ## lift count ## [1] 443.82 28 ## [2] 443.82 34 ## [3] 443.82 50 ## [4] 443.82 28 ## [5] 443.82 34 Now we see the combinations where the metal is the right hand side. Where we which products that leads to purchase of metal. 3.1.5.3 Filtering on left hand side. An example with the item {METAL} We can do the same for the left hand side. metal.association.rules &lt;- apriori(data = tr ,parameter = list(supp=0.001, conf=0.8) ,appearance = list(lhs=&quot;METAL&quot;,default=&quot;rhs&quot;)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.8 0.1 1 none FALSE TRUE 5 0.001 1 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 22 ## ## set item appearances ...[1 item(s)] done [0.00s]. ## set transactions ...[7876 item(s), 22191 transaction(s)] done [0.19s]. ## sorting and recoding items ... [2324 item(s)] done [0.01s]. ## creating transaction tree ... done [0.02s]. ## checking subsets of size 1 2 done [0.01s]. ## writing ... [1 rule(s)] done [0.00s]. ## creating S4 object ... done [0.01s]. inspect(head(metal.association.rules)) ## lhs rhs support confidence coverage lift count ## [1] {METAL} =&gt; {DECORATION} 0.002253166 1 0.002253166 443.82 50 Where this is showing what products metal leads to buying. We see only one item that is followed by buying metal. 3.1.6 Plotting association rules We want to get all where the confidence is greater than 40%. #This is just showed as an example, so we can do the filtering association.rules &lt;- apriori(data = tr ,parameter = list(supp=0.001, conf=0.3,maxlen=10)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.3 0.1 1 none FALSE TRUE 5 0.001 1 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 22 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[7876 item(s), 22191 transaction(s)] done [0.10s]. ## sorting and recoding items ... [2324 item(s)] done [0.01s]. ## creating transaction tree ... done [0.01s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 done [0.24s]. ## writing ... [180603 rule(s)] done [0.05s]. ## creating S4 object ... done [0.06s]. #Subsetting subRules &lt;- association.rules[quality(association.rules)$confidence&gt;0.4] We see that the higher the confidence the higher the lift, so there appears to be a strong relationship between these. We can also show it in the following. #Plot SubRules #plot(subRules) We see that usually the lift is the greatest in the top left corner, where the confidence is high and support is low. This makes sense, as you cannot lift (boost) the sales of another product, if their relationship is already rather high. Now we can plot the length of the rule with respect to confidence #The order is the number of items in the rule #plot(subRules,method=&quot;two-key plot&quot;) The following is supposed to be an interactive plot. But does not appear to work, missing a library. #plotly_arules(subRules) Now we can plot some interactive plot, where one can explore the rules. #top10subRules &lt;- head(subRules, n = 10, by = &quot;confidence&quot;) #plot(top10subRules, method = &quot;graph&quot;, engine = &quot;htmlwidget&quot;) rm(list = ls()) 3.2 Colaborative filtering As mentioned in the beginning of the chapter, we see that one can make recommendations based on similarity with other users and or similarity between products. 3.2.1 Loading and formatting data library(recommenderlab) library(tidyverse) data(&quot;MovieLense&quot;) # Data is given in realRatingMatrix format ; Optimized to store sparse matrices class(MovieLense) str(MovieLense,vec.len=2) #not as we normally reference list elements by \\\\$ but \\\\@ ## [1] &quot;realRatingMatrix&quot; ## attr(,&quot;package&quot;) ## [1] &quot;recommenderlab&quot; ## Formal class &#39;realRatingMatrix&#39; [package &quot;recommenderlab&quot;] with 2 slots ## ..@ data :Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots ## .. .. ..@ i : int [1:99392] 0 1 4 5 9 ... ## .. .. ..@ p : int [1:1665] 0 452 583 673 882 ... ## .. .. ..@ Dim : int [1:2] 943 1664 ## .. .. ..@ Dimnames:List of 2 ## .. .. .. ..$ : chr [1:943] &quot;1&quot; &quot;2&quot; ... ## .. .. .. ..$ : chr [1:1664] &quot;Toy Story (1995)&quot; &quot;GoldenEye (1995)&quot; ... ## .. .. ..@ x : num [1:99392] 5 4 4 4 4 ... ## .. .. ..@ factors : list() ## ..@ normalize: NULL the following are the different methods that we can use. methods(class = class(MovieLense)) # methods applicable to this class ## [1] [ [&lt;- binarize ## [4] calcPredictionAccuracy coerce colCounts ## [7] colMeans colSds colSums ## [10] denormalize dim dimnames ## [13] dimnames&lt;- dissimilarity evaluationScheme ## [16] getData.frame getList getNormalize ## [19] getRatingMatrix getRatings getTopNLists ## [22] hasRating image nratings ## [25] Recommender removeKnownRatings rowCounts ## [28] rowMeans rowSds rowSums ## [31] sample show similarity ## see &#39;?methods&#39; for accessing help and source code We see the dimensions of our matrix with users and the movies. dim(MovieLense) ## [1] 943 1664 We see that there 1.664 movies and 943 users. 3.2.1.1 Loading metadata that gets loaded with main dataset This is not applied in the exercise, but just shown moviemeta &lt;- MovieLenseMeta class(moviemeta) ## [1] &quot;data.frame&quot; We see that it is a data.frame. It has the following column names: colnames(moviemeta) ## [1] &quot;title&quot; &quot;year&quot; &quot;url&quot; &quot;unknown&quot; &quot;Action&quot; ## [6] &quot;Adventure&quot; &quot;Animation&quot; &quot;Children&#39;s&quot; &quot;Comedy&quot; &quot;Crime&quot; ## [11] &quot;Documentary&quot; &quot;Drama&quot; &quot;Fantasy&quot; &quot;Film-Noir&quot; &quot;Horror&quot; ## [16] &quot;Musical&quot; &quot;Mystery&quot; &quot;Romance&quot; &quot;Sci-Fi&quot; &quot;Thriller&quot; ## [21] &quot;War&quot; &quot;Western&quot; We see that the genre for each movie is presented. 3.2.2 Data Exploration We can see the topics for each movie. Only used for exploration purposes. #What do we know about the films? library(pander) pander(head(moviemeta,2),caption = &quot;First few Rows within Movie Meta Data &quot;) First few Rows within Movie Meta Data (continued below) title year Toy Story (1995) 1995 GoldenEye (1995) 1995 Table continues below url unknown Action http://us.imdb.com/M/title-exact?Toy%20Story%20(1995) 0 0 http://us.imdb.com/M/title-exact?GoldenEye%20(1995) 0 1 Table continues below Adventure Animation Children’s Comedy Crime Documentary Drama 0 1 1 1 0 0 0 1 0 0 0 0 0 0 Table continues below Fantasy Film-Noir Horror Musical Mystery Romance Sci-Fi Thriller 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 War Western 0 0 0 0 We can also interpret the ratings for a given customer ## look at the first few ratings of the first user head(as(MovieLense[1,], &quot;list&quot;)[[1]]) ## Toy Story (1995) ## 5 ## GoldenEye (1995) ## 3 ## Four Rooms (1995) ## 4 ## Get Shorty (1995) ## 3 ## Copycat (1995) ## 3 ## Shanghai Triad (Yao a yao yao dao waipo qiao) (1995) ## 5 3.2.2.1 Plotting ## number of ratings per user hist(rowCounts(MovieLense)) We see that most has reviewed less than 100 movies. Therefore we also see in the following plot, that most movies have less than 100 reviews, where we have a long tail with some movies that have very few ratings. ## number of ratings per movie hist(colCounts(MovieLense)) Now we can look at which movies that are top 10, based on number of times watched. #Top 10 movies movie_watched &lt;- data.frame( movie_name = names(colCounts(MovieLense)), watched_times = colCounts(MovieLense) ) top_ten_movies &lt;- movie_watched[order(movie_watched$watched_times, decreasing = TRUE), ][1:10, ] ggplot(top_ten_movies) + aes(x=movie_name, y=watched_times) + geom_bar(stat = &quot;identity&quot;,fill = &quot;firebrick4&quot;, color = &quot;dodgerblue2&quot;) + xlab(&quot;Movie Tile&quot;) + ylab(&quot;Count&quot;) + theme(axis.text = element_text(angle = 40, hjust = 1)) Looking at the overall data, we see that the mean rating is 3.5 where the median is actually 4, which seem pretty high. summary(getRatings(MovieLense)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 3.00 4.00 3.53 4.00 5.00 This we can also plot. data.frame(ratings = getRatings(MovieLense)) %&gt;% ggplot(aes(ratings)) + geom_bar(width = 0.75)+ labs(title = &#39;MovieLense Ratings Distribution&#39;) 3.2.3 What models can we use? Here we can see some of the recommender models that are available. #Different recommender models recommender_models &lt;- recommenderRegistry$get_entries(dataType=&quot;realRatingMatrix&quot;) names(recommender_models) ## [1] &quot;HYBRID_realRatingMatrix&quot; &quot;ALS_realRatingMatrix&quot; ## [3] &quot;ALS_implicit_realRatingMatrix&quot; &quot;IBCF_realRatingMatrix&quot; ## [5] &quot;LIBMF_realRatingMatrix&quot; &quot;POPULAR_realRatingMatrix&quot; ## [7] &quot;RANDOM_realRatingMatrix&quot; &quot;RERECOMMEND_realRatingMatrix&quot; ## [9] &quot;SVD_realRatingMatrix&quot; &quot;SVDF_realRatingMatrix&quot; ## [11] &quot;UBCF_realRatingMatrix&quot; #recommenderRegistry$get_entries(dataType=&quot;realRatingMatrix&quot;) #If run, you will see the parameters We will focus on: UBCF IBCF SVD ALS SVDF 3.2.4 Training and test set We want movies with at least 30 ratings and at least 100 users for each items. We see that the following approach is actually pretty clever, where we see that we filter on rows and columns by making the logical directly in the subsetting. #Training and test set At least 30 items evaluated or at least 100 users for each item rates &lt;- MovieLense[rowCounts(MovieLense) &gt; 30 ,colCounts(MovieLense) &gt; 100] rates1 &lt;- rates[rowCounts(rates) &gt; 30,] #This is included again, to make sure that we have accounted for both Now we can split the data into train and test data. We randomly define the which_train vector that is TRUE for users in the training set and FALSE for the others. Will set the probability in the training set as 80% which_train &lt;- sample(x = c(TRUE, FALSE), size = nrow(rates1), replace = TRUE, prob = c(0.8, 0.2)) #Random sampling # Define the training and the test sets recc_data_train &lt;- rates1[which_train, ] recc_data_test &lt;- rates1[!which_train, ] My guess is that recc is for recommender. 3.2.5 A small example with IBCF (item based) Fitting the model # Let&#39;s build the recommender IBCF - cosine: recc_model &lt;- Recommender(data = recc_data_train #On the train data ,method = &quot;IBCF&quot; #The col. filt. model ,parameter = list(k = 30) #No. of neighbors ) We have now created a IBCF Recommender Model Predicting recommendations on test data We want 5 recommendations for each user, based on the test data set. n_recommended &lt;- 5 recc_predicted &lt;- predict(object = recc_model ,newdata = recc_data_test ,n = n_recommended) # This is the recommendation for the first user recc_predicted@items[[1]] ## [1] 5 27 39 33 49 These are the recommendations for the given user. Now we can look for the movei name, which we are recommending. Now we want to convert the numbers into the actual movie names. # Now let&#39;s define a list with the recommendations for each user recc_matrix &lt;- lapply(recc_predicted@items, function(x){ colnames(rates)[x] }) # Let&#39;s take a look the recommendations for the first four users: recc_matrix[1:4] ## $`5` ## [1] &quot;Babe (1995)&quot; ## [2] &quot;Pulp Fiction (1994)&quot; ## [3] &quot;Maverick (1994)&quot; ## [4] &quot;Ace Ventura: Pet Detective (1994)&quot; ## [5] &quot;Nightmare Before Christmas, The (1993)&quot; ## ## $`7` ## [1] &quot;Fifth Element, The (1997)&quot; ## [2] &quot;Like Water For Chocolate (Como agua para chocolate) (1992)&quot; ## [3] &quot;Happy Gilmore (1996)&quot; ## [4] &quot;Face/Off (1997)&quot; ## [5] &quot;Wag the Dog (1997)&quot; ## ## $`26` ## [1] &quot;Welcome to the Dollhouse (1995)&quot; &quot;True Lies (1994)&quot; ## [3] &quot;Ghost (1990)&quot; &quot;Star Trek: Generations (1994)&quot; ## [5] &quot;Abyss, The (1989)&quot; ## ## $`41` ## [1] &quot;Twelve Monkeys (1995)&quot; &quot;Taxi Driver (1976)&quot; ## [3] &quot;While You Were Sleeping (1995)&quot; &quot;Blade Runner (1982)&quot; ## [5] &quot;Lone Star (1996)&quot; Then we see recommendations for four persons. 3.2.6 UBCF - User based collaborative based The method computes the similarity between users with cosine 3.2.6.1 Fitting the model # Let&#39;s build a recommender model leaving the parameters to their defaults. recc_model &lt;- Recommender(data = recc_data_train ,method = &quot;UBCF&quot;) A UBCF recommender has now been created 3.2.6.2 Predicting (making) recommendations n_recommended &lt;- 5 recc_predicted &lt;- predict(object = recc_model ,newdata = recc_data_test ,n = n_recommended) # Let&#39;s define a list with the recommendations to the test set users. recc_matrix &lt;- sapply(recc_predicted@items, function(x) { colnames(rates)[x] }) # Again, let&#39;s look at the first four users recc_matrix[1:4] ## [1] &quot;Gone with the Wind (1939)&quot; &quot;Lawrence of Arabia (1962)&quot; ## [3] &quot;Annie Hall (1977)&quot; &quot;Beautiful Girls (1996)&quot; Now we get recommendations based on user based filtering. 3.2.7 Cross validation The purpose is to make a more stable model through cross validation. The cross validation, that we set up here, is going to be used in the following code, for estimating models. We can split the data into some chunks, take a chunk out as the test set, and evaluate the accuracy. Then we can do the same with each other chunk and compute the average accuracy. Here we construct the evaluation model n_fold &lt;- 4 rating_threshold &lt;- 4 # threshold at which we consider the item to be good items_to_keep &lt;- 20 # given=20 means that while testing the model use only 20 randomly picked ratings from every user to predict the unknown ratings # in the test set the known data set has the ratings specified by given and the unknown data set the remaining ratings used for validation eval_sets &lt;- evaluationScheme(data = rates1 ,method = &quot;cross-validation&quot; ,k = n_fold, given = items_to_keep ,goodRating = rating_threshold) size_sets &lt;-sapply(eval_sets@runsTrain, length) size_sets ## [1] 459 459 459 459 3.2.8 IBCF - Item based collaborative based 3.2.8.1 Fitting the model (using CV) model_to_evaluate &lt;- &quot;IBCF&quot; model_parameters &lt;- NULL # we use the standard settings eval_recommender &lt;-Recommender(data = getData(eval_sets, &quot;train&quot;) ,method = model_to_evaluate ,parameter = model_parameters) 3.2.8.2 Making predictions The IBCF can recommend new items and predict their ratings. In order to build the model, we need to specify how many items we want to recommend, for example, 5. items_to_recommend &lt;- 5 # We can build the matrix with the predicted ratings using the predict function: eval_prediction &lt;- predict(object = eval_recommender ,newdata = getData(eval_sets, &quot;known&quot;) ,n = items_to_recommend ,type = &quot;ratings&quot;) 3.2.8.3 Performance metrics Per user By using the calcPredictionAccuracy, we can calculate: the Root mean square error (RMSE), Mean squared error (MSE), and the Mean absolute error (MAE). We want to compare the results on each user, hence we take the head. In the following we are also plotting a histogram. eval_accuracy &lt;- calcPredictionAccuracy( x = eval_prediction ,data = getData(eval_sets, &quot;unknown&quot;) ,byUser = TRUE #Because, how good does the model work for each user ) # This is a small sample of the results for the Prediction and Accuracy head(eval_accuracy) ## RMSE MSE MAE ## 5 1.618733 2.620298 1.3540077 ## 6 1.335989 1.784867 0.9377790 ## 7 1.267880 1.607520 0.9272107 ## 11 1.097127 1.203688 0.8608482 ## 15 1.760519 3.099426 1.4578952 ## 16 1.155948 1.336215 0.6713985 # Now, let&#39;s take a look at the RMSE by each user qplot(eval_accuracy[,&quot;RMSE&quot;]) + geom_histogram(binwidth = 0.1) + ggtitle(&quot;Distribution of the RMSE by user&quot;) We see that the typical RMSE is between 1 and 1.5. Overall The following is the average for all users # However, we need to evaluate the model as a whole, so we will set the byUser to False eval_accuracy &lt;- calcPredictionAccuracy( x = eval_prediction ,data = getData(eval_sets, &quot;unknown&quot;) ,byUser = FALSE #We want it for the whole model ) eval_accuracy #for IBCF ## RMSE MSE MAE ## 1.366885 1.868374 1.046393 The mean absolute error is just above 1, hence one rating off. Lets us take a look at the confusionmatrix to see how it looks there. Notice that we specificy n, that is because we want predictions for different number of predictions. # Confusion matrix good threshold =4 results &lt;- evaluate(x = eval_sets #Using the Cross validation ,method = model_to_evaluate ,n = seq(10, 100, 10)) #We make from 10 to 100 recommendations ## IBCF run fold/sample [model time/prediction time] ## 1 [0.158sec/0.04sec] ## 2 [0.216sec/0.057sec] ## 3 [0.237sec/0.067sec] ## 4 [0.175sec/0.074sec] results object is an evaluationResults object containing the results of the evaluation. Each element of the list corresponds to a different split of the k-fold. Let’s look at the first element #Results for each CV getConfusionMatrix(results)[[1]] #the first cross validation, 2 would be the second CV etc. ## TP FP FN TN N precision recall TPR ## [1,] 1.743590 8.25641 43.11538 258.8846 312 0.1743590 0.04006280 0.04006280 ## [2,] 3.314103 16.68590 41.54487 250.4551 312 0.1657051 0.07731537 0.07731537 ## [3,] 4.955128 25.03205 39.90385 242.1090 312 0.1652778 0.11629358 0.11629358 ## [4,] 6.326923 33.59615 38.53205 233.5449 312 0.1586538 0.14898650 0.14898650 ## [5,] 7.634615 42.22436 37.22436 224.9167 312 0.1533974 0.18148772 0.18148772 ## [6,] 9.000000 50.79487 35.85897 216.3462 312 0.1508547 0.21260538 0.21260538 ## [7,] 10.442308 59.28846 34.41667 207.8526 312 0.1501374 0.25034654 0.25034654 ## [8,] 11.628205 68.03846 33.23077 199.1026 312 0.1463942 0.27633480 0.27633480 ## [9,] 12.724359 76.87821 32.13462 190.2628 312 0.1424858 0.29713798 0.29713798 ## [10,] 14.038462 85.50000 30.82051 181.6410 312 0.1415385 0.32788012 0.32788012 ## FPR n ## [1,] 0.03081865 10 ## [2,] 0.06240341 20 ## [3,] 0.09363603 30 ## [4,] 0.12571313 40 ## [5,] 0.15821967 50 ## [6,] 0.19033215 60 ## [7,] 0.22226847 70 ## [8,] 0.25506440 80 ## [9,] 0.28819483 90 ## [10,] 0.32053667 100 #getConfusionMatrix(results)[[2]] #the first cross validation, 2 would be the second CV etc. #getConfusionMatrix(results)[[3]] #the first cross validation, 2 would be the second CV etc. #getConfusionMatrix(results)[[4]] #the first cross validation, 2 would be the second CV etc. We see that the more recommendations we make, the ‘better’ does the results get, although making 100 recommendations also means that the FP is also increasing, hence we start recommending something, where it did not turn out to be purchased. Hence it is a trade-off. In this case, look at the first four columns True Positives (TP): These are recommended items that have been purchased. False Positives (FP): These are recommended items that haven’t been purchased False Negatives (FN): These are not recommended items that have been purchased. True Negatives (TN): These are not recommended items that haven’t been purchased. Manually making the ROC curve # If we want to take account of all the splits at the same time, we can just sum up the indices: columns_to_sum &lt;- c(&quot;TP&quot;, &quot;FP&quot;, &quot;FN&quot;, &quot;TN&quot;) indices_summed &lt;- Reduce(&quot;+&quot;, getConfusionMatrix(results))[, columns_to_sum] TPR &lt;- indices_summed[,&quot;TP&quot;]/(indices_summed[,&quot;TP&quot;] + indices_summed[,&quot;FN&quot;]) FPR &lt;- indices_summed[,&quot;FP&quot;]/(indices_summed[,&quot;FP&quot;] + indices_summed[,&quot;TN&quot;]) indices_summed &lt;- cbind(indices_summed,TPR,FPR) indices_summed ## TP FP FN TN TPR FPR ## [1,] 7.173077 32.82692 173.9679 1034.0321 0.03959941 0.03076969 ## [2,] 13.660256 66.33974 167.4808 1000.5192 0.07541227 0.06218230 ## [3,] 19.794872 100.19231 161.3462 966.6667 0.10927879 0.09391336 ## [4,] 25.653846 134.26923 155.4872 932.5897 0.14162361 0.12585471 ## [5,] 31.057692 168.80128 150.0833 898.0577 0.17145587 0.15822268 ## [6,] 36.679487 203.11538 144.4615 863.7436 0.20249133 0.19038635 ## [7,] 42.057692 237.67308 139.0833 829.1859 0.23218204 0.22277835 ## [8,] 47.288462 272.37821 133.8526 794.4808 0.26105882 0.25530854 ## [9,] 52.294872 307.29487 128.8462 759.5641 0.28869701 0.28803701 ## [10,] 57.794872 341.66667 123.3462 725.1923 0.31906009 0.32025476 plot(x = TPR &lt;- indices_summed[,&quot;FPR&quot;],y = TPR &lt;- indices_summed[,&quot;TPR&quot;],type = &quot;b&quot; ,cex = 0 #To remove dots. ,xlab = &quot;FPR&quot; ,ylab = &quot;TPR&quot; ) text(x = TPR &lt;- indices_summed[,&quot;FPR&quot;],y = TPR &lt;- indices_summed[,&quot;TPR&quot;],labels = seq(10,100,10)) title(&quot;ROC&quot;) 3.2.9 Building ROC curve and precision / recall rate Building an ROC curve. Will need these factors True Positive Rate (TPR): Percentage of purchased items that have been recommended. TP/(TP + FN) False Positive Rate (FPR): Percentage of not purchased items that have been recommended. FP/(FP + TN) plot(results, annotate = TRUE, main = &quot;ROC curve&quot;) Now we can also make a precision / recall rate We can also look at the accuracy metrics as well: Precision: Percentage of recommended items that have been purchased. FP/(TP + FP) Recall: Percentage of purchased items that have been recommended. TP/(TP + FN) = True Positive Rate plot(results, &quot;prec/rec&quot;, annotate = TRUE, main = &quot;Precision-Recall&quot;) The plots above gives an indication of how many recommendations to make. Notice that it is based upon the results object that we are making, there we specify the intervals for which we are going to recommend. 3.2.10 Comparing UBCF and IBCF let us see how the models compare #Comparing models models_to_evaluate &lt;- list(IBCF_cos = list(name = &quot;IBCF&quot;, param = list(method = &quot;cosine&quot;)), #Cosine correl. coef. IBCF_cor = list(name = &quot;IBCF&quot;, param = list(method = &quot;pearson&quot;)), #Pearson correl. coef. UBCF_cos = list(name = &quot;UBCF&quot;, param = list(method = &quot;cosine&quot;)), UBCF_cor = list(name = &quot;UBCF&quot;, param = list(method = &quot;pearson&quot;)), random = list(name = &quot;RANDOM&quot;, param = NULL)) # In order to evaluate the models, we need to test them, varying the number of items. n_recommendations &lt;- c(1,5,seq(10,100,10)) # Now let&#39;s run and evaluate the models list_results &lt;- evaluate(x = eval_sets, method = models_to_evaluate, n = n_recommendations) ## IBCF run fold/sample [model time/prediction time] ## 1 [0.213sec/0.098sec] ## 2 [0.191sec/0.065sec] ## 3 [0.165sec/0.053sec] ## 4 [0.156sec/0.056sec] ## IBCF run fold/sample [model time/prediction time] ## 1 [0.24sec/0.054sec] ## 2 [0.254sec/0.063sec] ## 3 [0.239sec/0.071sec] ## 4 [0.277sec/0.063sec] ## UBCF run fold/sample [model time/prediction time] ## 1 [0.004sec/0.323sec] ## 2 [0.004sec/0.375sec] ## 3 [0.005sec/0.253sec] ## 4 [0.004sec/0.344sec] ## UBCF run fold/sample [model time/prediction time] ## 1 [0.003sec/0.36sec] ## 2 [0.003sec/0.348sec] ## 3 [0.023sec/0.307sec] ## 4 [0.005sec/0.392sec] ## RANDOM run fold/sample [model time/prediction time] ## 1 [0.001sec/0.096sec] ## 2 [0sec/0.04sec] ## 3 [0.001sec/0.06sec] ## 4 [0.001sec/0.083sec] Now we can plot the roc curves and the and the precision - recall rate. plot(list_results, annotate = 1, legend = &quot;topleft&quot;) title(&quot;ROC curve&quot;) plot(list_results, &quot;prec/rec&quot;, annotate = 1, legend = &quot;bottomright&quot;, ylim = c(0,0.4)) title(&quot;Precision-recall&quot;) We see that the IBCF Pearson correlation coefficient gets the best result. We also see that the random recommendations is almost as good as some of the other methods. Other evaluation criteria Coverage - what we have seen earlier Diversity and novelty - To avoid monotone lists, discover new families of items Serendipity - Unexpected and surprisng items might be valuable Familiarity - GIve the user the impression of understanding his/her needs Biases - Do we just end up recommended the best movies? hence there is a great bias. rm(list = ls()) 3.3 Latent Factor Models This is a snippet from the introduction in the chapter: The last approach is the latent factor model, this is just an extension of the user-based and item-based, where both scenarios are considered. This method assumes that we make some constructs, e.g., as in SEM, where the constructs are explained by different items (movies). Then it would be obvious to make constructs based on different genres. These constructs are not directly observable, hence the name latent. Within LFM cold start also applies How is correlation measured? We see that the dot product (inner product) between items and users, here we are able to interpret the projection of these from the center to see how similar/dissimilar they are. From the image we can see that Gus and the guy in the top right is more or less perpendicular, so they do not share much interest on the y axis, although on the x axis they appear to be equally far out, hence the dot product appears to be able to reflect similarity between perople. . Hence to make these projections we need to find to matrices - namely U and V. To estimate the values for these to matrices, we have used one approach, the Signular Value Decomposition (SVD). Notice that this approach demands that we have a full matrix, as we often have sparse matrices, is that it replace missing values with the column means. This introduce a lot of bias, but makes the model work. For this method we cannot really avoid it, because each movie is never reviewed by all users. Notice that for this method we are able to account for bias for ratings, e.g., one person may often give relatively higher ratings than other, hence we can regularize this to get rid of some bias. library(recommenderlab) library(tidyverse) data(MovieLense) class(MovieLense) ## [1] &quot;realRatingMatrix&quot; ## attr(,&quot;package&quot;) ## [1] &quot;recommenderlab&quot; help(MovieLense) dim(MovieLense) ## [1] 943 1664 Select only the users who have rated at least 50 movies or movies that had been rated more than 100 times. As if they did not rate more than this it starts getting difficult recommeding movies for them. (ratings_movies &lt;- MovieLense[rowCounts(MovieLense) &gt; 50, colCounts(MovieLense) &gt; 100]) ## 560 x 332 rating matrix of class &#39;realRatingMatrix&#39; with 55298 ratings. # use the minimum number of items purchased by any user to decide item number to keep min(rowCounts(ratings_movies)) n_fold &lt;- 4 items_to_keep &lt;- 15 #randomly chosen items from a given responded rating_threshold &lt;- 3 #A good rating is three or higher ## [1] 18 In the following we include SVD-Funk, there is a bunch of tuning parameters, these are: k = number of topics gamma = Learning parameter / learning rate lambda = The regularization Notice that there are default values, although we choose to set k to 50. # Use k-fold to validate models eval_sets &lt;- evaluationScheme(data = ratings_movies ,method = &quot;cross-validation&quot; ,k = n_fold ,given = items_to_keep ,goodRating = rating_threshold) #We want to comapre different approaches models &lt;- list( IBCF = list(name = &quot;IBCF&quot;,param = list(method = &quot;cosine&quot;)), UBCF = list(name = &quot;UBCF&quot;, param = list(method = &quot;pearson&quot;)), SVD = list(name = &quot;SVD&quot;, param = list(k = 50)), #50 concepts included SVDF = list(name = &quot;SVDF&quot;, param = list(k = 50)) #50 concepts included ) notice that the SVDF takes some time to compute. That is beacuse it has to go through all the known ratings and construct the U and V matrix. # varying the number of items we want to recommend to users n_rec &lt;- c(1, 5, seq(10, 100, 10)) # evaluating the recommendations results &lt;- evaluate(x = eval_sets, method = models, n= n_rec) ## IBCF run fold/sample [model time/prediction time] ## 1 [0.168sec/0.064sec] ## 2 [0.154sec/0.032sec] ## 3 [0.187sec/0.031sec] ## 4 [0.168sec/0.441sec] ## UBCF run fold/sample [model time/prediction time] ## 1 [0.005sec/0.288sec] ## 2 [0.004sec/0.223sec] ## 3 [0.003sec/0.26sec] ## 4 [0.003sec/0.245sec] ## SVD run fold/sample [model time/prediction time] ## 1 [0.125sec/0.036sec] ## 2 [0.18sec/0.063sec] ## 3 [0.195sec/0.047sec] ## 4 [0.123sec/0.079sec] ## SVDF run fold/sample [model time/prediction time] ## 1 [52.423sec/12.738sec] ## 2 [53.388sec/12.344sec] ## 3 [50.845sec/12.432sec] ## 4 [48.381sec/12.476sec] # extract the related average confusion matrices (avg_matrices &lt;- lapply(results, avg)) plot(results, annotate=TRUE) plot(results, &quot;prec/rec&quot;, annotate = TRUE, main = &quot;Precision-Recall&quot;) ## $IBCF ## TP FP FN TN N precision recall ## [1,] 0.3482143 0.6517857 72.71429 243.2857 317 0.3482143 0.00491590 ## [2,] 1.4589286 3.5410714 71.60357 240.3964 317 0.2917857 0.01967898 ## [3,] 2.7857143 7.2142857 70.27679 236.7232 317 0.2785714 0.03835957 ## [4,] 5.3392857 14.6607143 67.72321 229.2768 317 0.2669643 0.07391105 ## [5,] 7.7035714 22.2964286 65.35893 221.6411 317 0.2567857 0.10613546 ## [6,] 10.0750000 29.9250000 62.98750 214.0125 317 0.2518750 0.13979789 ## [7,] 12.2553571 37.7428571 60.80714 206.1946 317 0.2451152 0.16961429 ## [8,] 14.4553571 45.5250000 58.60714 198.4125 317 0.2409961 0.20003095 ## [9,] 16.6071429 53.3553571 56.45536 190.5821 317 0.2373652 0.22866781 ## [10,] 18.7482143 61.1732143 54.31429 182.7643 317 0.2345848 0.25801586 ## [11,] 20.9392857 68.8928571 52.12321 175.0446 317 0.2330842 0.28861480 ## [12,] 23.0375000 76.6875000 50.02500 167.2500 317 0.2310267 0.31799055 ## TPR FPR n ## [1,] 0.00491590 0.002640127 1 ## [2,] 0.01967898 0.014389413 5 ## [3,] 0.03835957 0.029322262 10 ## [4,] 0.07391105 0.059713549 20 ## [5,] 0.10613546 0.091065483 30 ## [6,] 0.13979789 0.122509370 40 ## [7,] 0.16961429 0.154798067 50 ## [8,] 0.20003095 0.186763693 60 ## [9,] 0.22866781 0.218875612 70 ## [10,] 0.25801586 0.251033210 80 ## [11,] 0.28861480 0.282693419 90 ## [12,] 0.31799055 0.314755874 100 ## ## $UBCF ## TP FP FN TN N precision recall ## [1,] 0.2053571 0.7946429 72.85714 243.1429 317 0.2053571 0.00301467 ## [2,] 1.0625000 3.9375000 72.00000 240.0000 317 0.2125000 0.01514849 ## [3,] 2.2696429 7.7303571 70.79286 236.2071 317 0.2269643 0.03186838 ## [4,] 4.9678571 15.0321429 68.09464 228.9054 317 0.2483929 0.06896208 ## [5,] 7.7321429 22.2678571 65.33036 221.6696 317 0.2577381 0.10845983 ## [6,] 10.5892857 29.4107143 62.47321 214.5268 317 0.2647321 0.14770760 ## [7,] 13.4589286 36.5410714 59.60357 207.3964 317 0.2691786 0.18683732 ## [8,] 16.2428571 43.7571429 56.81964 200.1804 317 0.2707143 0.22412311 ## [9,] 19.0303571 50.9696429 54.03214 192.9679 317 0.2718622 0.26086940 ## [10,] 21.7964286 58.2035714 51.26607 185.7339 317 0.2724554 0.29756155 ## [11,] 24.4553571 65.5446429 48.60714 178.3929 317 0.2717262 0.33315249 ## [12,] 27.1000000 72.9000000 45.96250 171.0375 317 0.2710000 0.37003899 ## TPR FPR n ## [1,] 0.00301467 0.003266251 1 ## [2,] 0.01514849 0.016205094 5 ## [3,] 0.03186838 0.031743009 10 ## [4,] 0.06896208 0.061402265 20 ## [5,] 0.10845983 0.090847621 30 ## [6,] 0.14770760 0.119839234 40 ## [7,] 0.18683732 0.148698202 50 ## [8,] 0.22412311 0.177921239 60 ## [9,] 0.26086940 0.207113400 70 ## [10,] 0.29756155 0.236446368 80 ## [11,] 0.33315249 0.266269698 90 ## [12,] 0.37003899 0.296255963 100 ## ## $SVD ## TP FP FN TN N precision recall ## [1,] 0.4714286 0.5285714 72.59107 243.4089 317 0.4714286 0.00749966 ## [2,] 2.0964286 2.9035714 70.96607 241.0339 317 0.4192857 0.03213059 ## [3,] 3.8482143 6.1517857 69.21429 237.7857 317 0.3848214 0.05815476 ## [4,] 7.1160714 12.8839286 65.94643 231.0536 317 0.3558036 0.10586948 ## [5,] 10.1357143 19.8642857 62.92679 224.0732 317 0.3378571 0.14745252 ## [6,] 12.8875000 27.1125000 60.17500 216.8250 317 0.3221875 0.18598032 ## [7,] 15.4196429 34.5803571 57.64286 209.3571 317 0.3083929 0.22077686 ## [8,] 17.8428571 42.1571429 55.21964 201.7804 317 0.2973810 0.25306744 ## [9,] 20.2589286 49.7410714 52.80357 194.1964 317 0.2894133 0.28552879 ## [10,] 22.6625000 57.3375000 50.40000 186.6000 317 0.2832812 0.31851925 ## [11,] 24.9589286 65.0410714 48.10357 178.8964 317 0.2773214 0.34946212 ## [12,] 27.2017857 72.7982143 45.86071 171.1393 317 0.2720179 0.38052596 ## TPR FPR n ## [1,] 0.00749966 0.00208126 1 ## [2,] 0.03213059 0.01156953 5 ## [3,] 0.05815476 0.02460278 10 ## [4,] 0.10586948 0.05164570 20 ## [5,] 0.14745252 0.07978709 30 ## [6,] 0.18598032 0.10932347 40 ## [7,] 0.22077686 0.13985131 50 ## [8,] 0.25306744 0.17067767 60 ## [9,] 0.28552879 0.20165965 70 ## [10,] 0.31851925 0.23273640 80 ## [11,] 0.34946212 0.26425240 90 ## [12,] 0.38052596 0.29612846 100 ## ## $SVDF ## TP FP FN TN N precision recall ## [1,] 0.4857143 0.5142857 72.57679 243.4232 317 0.4857143 0.007201657 ## [2,] 2.1357143 2.8642857 70.92679 241.0732 317 0.4271429 0.031113969 ## [3,] 4.0178571 5.9821429 69.04464 237.9554 317 0.4017857 0.058556427 ## [4,] 7.3375000 12.6625000 65.72500 231.2750 317 0.3668750 0.103916994 ## [5,] 10.2642857 19.7357143 62.79821 224.2018 317 0.3421429 0.144022419 ## [6,] 13.2089286 26.7910714 59.85357 217.1464 317 0.3302232 0.184240962 ## [7,] 16.0589286 33.9410714 57.00357 209.9964 317 0.3211786 0.223408405 ## [8,] 18.7839286 41.2160714 54.27857 202.7214 317 0.3130655 0.259535975 ## [9,] 21.4767857 48.5232143 51.58571 195.4143 317 0.3068112 0.295299340 ## [10,] 24.2982143 55.7017857 48.76429 188.2357 317 0.3037277 0.333443455 ## [11,] 27.1339286 62.8660714 45.92857 181.0714 317 0.3014881 0.371935689 ## [12,] 29.8232143 70.1767857 43.23929 173.7607 317 0.2982321 0.407457253 ## TPR FPR n ## [1,] 0.007201657 0.002025358 1 ## [2,] 0.031113969 0.011331845 5 ## [3,] 0.058556427 0.023791598 10 ## [4,] 0.103916994 0.050589340 20 ## [5,] 0.144022419 0.079231513 30 ## [6,] 0.184240962 0.107775329 40 ## [7,] 0.223408405 0.136781000 50 ## [8,] 0.259535975 0.166279278 60 ## [9,] 0.295299340 0.196006663 70 ## [10,] 0.333443455 0.224961977 80 ## [11,] 0.371935689 0.253965717 90 ## [12,] 0.407457253 0.283543368 100 We will see from the plot how the different models perform. We see that the SVDF appear to have the best performance. 3.3.1 Fitting the model recommender_ibcf &lt;- Recommender(data = getData(eval_sets, &quot;train&quot;), method = &quot;IBCF&quot;,parameter = list(method = &quot;cosine&quot;)) recommender_ubcf &lt;- Recommender(data = getData(eval_sets, &quot;train&quot;), method = &quot;UBCF&quot;,parameter = list(method = &quot;pearson&quot;)) recommender_svd &lt;- Recommender(data = getData(eval_sets, &quot;train&quot;), method = &quot;SVD&quot;,parameter = list(k=50)) recommender_svdf &lt;- Recommender(data = getData(eval_sets, &quot;train&quot;), method = &quot;SVDF&quot;,parameter = list(k=50)) items_to_recommend &lt;- 10 3.3.2 Making predictions #Produce predictions based upon the known dataset eval_prediction_ibcf &lt;- predict(object = recommender_ibcf ,newdata = getData(eval_sets, &quot;known&quot;) ,n = items_to_recommend ,type = &quot;ratings&quot;) eval_prediction_ubcf &lt;- predict(object = recommender_ubcf ,newdata = getData(eval_sets, &quot;known&quot;) ,n = items_to_recommend ,type = &quot;ratings&quot;) eval_prediction_svd &lt;- predict(object = recommender_svd ,newdata = getData(eval_sets, &quot;known&quot;) ,n = items_to_recommend ,type = &quot;ratings&quot;) eval_prediction_svdf &lt;- predict(object = recommender_svdf ,newdata = getData(eval_sets, &quot;known&quot;) ,n = items_to_recommend ,type = &quot;ratings&quot;) Now we want to compare the different models on the unknown data. ######################RANDOM###################### #UBCF eval_accuracy_ubcf &lt;- calcPredictionAccuracy( x = eval_prediction_ubcf, data = getData(eval_sets, &quot;unknown&quot;), byUser = F) eval_accuracy_ubcf_user &lt;- calcPredictionAccuracy( x = eval_prediction_ubcf, data = getData(eval_sets, &quot;unknown&quot;), byUser = TRUE) print(&quot;UBCF&quot;) head(eval_accuracy_ubcf_user) #IBCF eval_accuracy_ibcf &lt;- calcPredictionAccuracy( x = eval_prediction_ibcf, data = getData(eval_sets, &quot;unknown&quot;), byUser = F) eval_accuracy_ibcf_user &lt;- calcPredictionAccuracy( x = eval_prediction_ibcf, data = getData(eval_sets, &quot;unknown&quot;), byUser = TRUE) print(&quot;IBCF&quot;) head(eval_accuracy_ibcf_user) #SVD eval_accuracy_svd &lt;- calcPredictionAccuracy( x = eval_prediction_svd, data = getData(eval_sets, &quot;unknown&quot;), byUser = F) eval_accuracy_svd_user &lt;- calcPredictionAccuracy( x = eval_prediction_svd, data = getData(eval_sets, &quot;unknown&quot;), byUser = TRUE) print(&quot;SVD&quot;) head(eval_accuracy_svd_user) #SVDF eval_accuracy_svdf &lt;- calcPredictionAccuracy( x = eval_prediction_svdf, data = getData(eval_sets, &quot;unknown&quot;), byUser = F) eval_accuracy_svdf_user &lt;- calcPredictionAccuracy( x = eval_prediction_svdf, data = getData(eval_sets, &quot;unknown&quot;), byUser = TRUE) print(&quot;SVDF&quot;) head(eval_accuracy_svdf_user) ## [1] &quot;UBCF&quot; ## RMSE MSE MAE ## 6 1.0903114 1.1887789 0.9020429 ## 10 0.6478319 0.4196862 0.4956284 ## 14 1.6074909 2.5840271 1.2409350 ## 18 0.9514405 0.9052391 0.7995348 ## 28 0.8863502 0.7856166 0.7355511 ## 37 0.8162697 0.6662963 0.6304372 ## [1] &quot;IBCF&quot; ## RMSE MSE MAE ## 6 1.4052207 1.9746452 1.0919266 ## 10 0.7318435 0.5355949 0.5587116 ## 14 1.4211250 2.0195963 0.9920766 ## 18 1.1829320 1.3993280 0.9567777 ## 28 1.2659009 1.6025052 1.0011266 ## 37 1.4958532 2.2375768 1.1161399 ## [1] &quot;SVD&quot; ## RMSE MSE MAE ## 6 1.0528300 1.1084509 0.8102372 ## 10 0.5864089 0.3438754 0.4807015 ## 14 1.3151982 1.7297463 0.9978417 ## 18 0.8516606 0.7253258 0.7415427 ## 28 0.9625632 0.9265279 0.7596821 ## 37 0.7845724 0.6155538 0.5981831 ## [1] &quot;SVDF&quot; ## RMSE MSE MAE ## 6 0.9054762 0.8198871 0.7159928 ## 10 0.5111024 0.2612257 0.4266370 ## 14 1.4040173 1.9712646 1.0372160 ## 18 0.7968216 0.6349246 0.6966087 ## 28 0.8546096 0.7303576 0.6960481 ## 37 0.8371373 0.7007988 0.6414081 We can then assess the RMSE for each model. eval_accuracy_ubcf eval_accuracy_ibcf eval_accuracy_svd eval_accuracy_svdf ## RMSE MSE MAE ## 1.0380870 1.0776245 0.8154369 ## RMSE MSE MAE ## 1.335593 1.783809 1.020383 ## RMSE MSE MAE ## 0.9845277 0.9692949 0.7907637 ## RMSE MSE MAE ## 0.9063570 0.8214830 0.7075204 We see that the SVDF appear to do the best. "],["probabilistic-models-for-customer-analytics.html", "4 Probabilistic Models for Customer Analytics 4.1 Bayesian Networks 4.2 Application 1: Explaining and Predicting Customer Retention in a Virtual Community 4.3 Application 2: Identifying populations with Positive Expected Lift in Profit (ELP) and targeting 4.4 Application 3: Product Recommendation using Probabilistic Models (BN) 4.5 Latent models for cross selling and acquisition sequence", " 4 Probabilistic Models for Customer Analytics 4.1 Bayesian Networks Why do we use bayesian network analysis? We have seen EFA, CFA and lastly SEM, which are structural models where you assume relationships among variables. Bayesian network is applied to identify causality in the data. Notice that Bayesian Networks ONLY deals with categorical data, hence if you are given a continous dataset, then you want to encode it as factors before modeling, e.g., in ordered categories, non-ordered, or as binary. 4.1.1 Why Bayesian? We apply the bayes theorem to update probabilities in light of new evidence. A quick summary: \\[P(A|B) = P(A)\\frac{P(B|A)}{P(B)}\\] Where we see A is the outcome given event B happening. This means: \\(P(A)\\) = prior, i.e., unconditional probability, hence not accounting for event B. \\(P(A|B)\\) = the posterior, i.e., conditional probaiblity, here we account for event A happening given event B is happening. \\(P(B)\\) = Plausible cause, i.e., the predictor. \\(P(B|A)\\) = The likelihood, i.e., conditional probability of B given that event A occurs. We should notice that this assumes that there is a dependency between the variables. As if there were no dependence then event A happening given event B happening is not changed at all, for instance wind gusts in Denmark and temperature in Denmark must have a dependency on each other, e.g., when it is very windy it is probable that the temperature will decrease. Although when it is windy in Australia one may not infer a relationship between these two variables. This could also be written with \\(P(A|B) = P(Y)\\), as event B has no effect at all. 4.1.1.1 Many names for Bayesian Networks Causal probabilistic networks Probabilistic graphical models Bayesian Belief networks Causal networks Directed graphs Probabilistic expert systems Influence diagrams 4.1.2 Terms and explanatory examples Whe have the following terms: DAG: Directed Acyclic Graph: meaning that we have graphs with directed arrows that cannot lead to cycles. Within the structure we have the following terms: Nodes = variable Arcs = links = relationships (the arrows) Path = A path where you are only able to directions of the arrows. Chain = This is path, but you are not constrained to following the directions. A cycle = that would be that a path is starting and able to end in the same node. in BN we are not able to have these instances. Connected graph = Where all nodes are connected Empty graph = A graph with nodes, but not with arcs. Multivariate analysis: Where the graph is the means and the variable relationships the ends. Bayesian Networks is a stochastic data-mining technique. It applies different elements to construct the model. We see in the following picture, that there is a relationship from Job down to Credit. These we call the parent and the child (root and non-root). Hence it is clear the the direction is also from Job to Credit. Simple example of a Bayesian network It can be seen in the model that the prior \\(P(A) = 0.915\\), hence a probability of approx. 91% being that you have a job. Then we see the different kinds of probabilities of B and non B (credit) given a person having a job or not having a job. We see a more extensive example here: Extensive example of a Bayesian network Notice that the decendents are effetcs of the given variables, e.g., that a given season influence the probability of how it is going to rain or sprinkle etc. In general we should be able to say, that a guy has a broken leg and then we can calculate backwards what the probabilities of the different variables being occurent. Recall that each connected variables (and adjacent) have a conditional probability between them, meaning that we can take the same example as above: Example: We know a guy has broken his leg, we can now calculate backwards and use the probabilities for the certain events to get an idea of the whole situation. Hence it is probable that it was slippery, due to rain or sprinkles and ultimately due to the season, hence the information flows. On the other hand, if we find out that it was in fact slippery, hence we impose evidence, i.e., control for slippery, we see that broken leg and (sprinkle,rain) is completely independent of each other, as we know that it was slippery, hence no more information is needed. 4.1.2.1 Types of connections We have three types of connections: Serial connection, Explaining away (discounting), Common cause (divergent) I have summarized the approaches in the following visualization and following by an example for each. Examples of connections Serial connection example:We have a scenario where wet is the cause of slippery, and slippery is the cause of breaking legs. Lets say that we know that a person has a broken leg, hence we can update the probability for it being slippery and then if it was wet. this will also increase probabilities for B (slippery) and C (Wet) Now lets say that we also know that it was slippery any new information about C (broken leg condition) is not changing A. And changes in A (the wet condition) will not affect C (broken leg condition) because these are now known as we found evidence, e.g., if we find out that it was not wet, but we still know that it was slippery (I guess that would also require some other relationship to B) Explaining away example: We have a scenario where both having covid and a cold is the cause of dry coughing, hence dry coughing being the children while the others the parents. Notice that A and B is completely independent. Lets say that a person has symptoms C and we later find out that the person has covid (A). Hence we set probability of A = 100 (or in practive it would probably be 99%) then the probability of B will decrease as it is now less likely that the person is also having a could, because we have found one cause for the symptoms. Common cause example: We have a scenario where we have children that are related to the same parents. We now know that the house is burning, this will increase the probability that of B (forgetting stove) is also higher. This leads with the probability of food being overcooked is also increasing. Now lets say that we get the information that you did in fact forget the stove. Hence any new information in A (about burning house) will not change anything to the probability of C (overcooked meal) 4.1.2.2 Variable selection based on a Bayesian Network We know that a Bayesian network can be used to indentify causality and hence dependencies among variables, given that we apply an algorithm for this (more about this in a later section), we can now start inferring important variables based on this. But how to do this? Recall that parents are the causes while the children being the outcome. Let us say that we have a child, which we want to analyze, e.g., a burning house, and we want to explore the causes. What would be natural to do? One could take the following variables: The child’s parents’ The child’s children. Other parents of the child. This is called the the Markov Blanket. Hence one is able to find the Markov Blanket of a variable, meaning identifying variables that you must find information about (hence control for, i.e., evidence) making the child conditional independent of all other variables. Meaning that any change in any given non-related variable will not affect the variable that we want to look into. Therefore, Bayesian networks in combination with Markov Blanket method is a strong combination for variable selection. Although one must be aware that, the Markov Blanket model build on the Markov property: For each variable X in a graph, X is conditionally independent of the set of all nondescendents given the set of all its parents That means that all we need to do is check for a nodes parents, if we can find evidance on these, then the given node is not dependent on all non descendents. In laymans words it can be said, that if a node is not a descendent (can be found by following the arc directions), then it is a nondescendent. The following draws an example. library(ggdag) #Generating the relationships dag &lt;- dagitty::dagitty(&quot;dag { X -&gt; Y -&gt; W X -&gt; Z -&gt; W X -&gt; Z -&gt; V }&quot; ) tidy_dag &lt;- tidy_dagitty(dag) #Adding information on the color tidy_dag$data &lt;- dplyr::mutate(tidy_dag$data, colour = ifelse(tidy_dag$data$name == &quot;Z&quot; ,&quot;Reference point&quot; ,ifelse(tidy_dag$data$name == &quot;X&quot;,&quot;Parent&quot; ,ifelse(tidy_dag$data$name %in% c(&quot;V&quot;,&quot;W&quot;),&quot;Descendent&quot;,&quot;Non descendents&quot;)))) #Making the plot tidy_dag %&gt;% ggplot(aes( x = x, y = y, xend = xend, yend = yend )) + geom_dag_point(aes(colour = colour)) + geom_dag_edges() + geom_dag_text() + theme_dag() We that for Z, V and W are its descendents. We see that X is its parent and Y being its ancestor i.e., nondescendent. We can make Z independent (hence it being conditionally independent of Y) of Y, if we just control for X as it is its parent. This mean that the graph must include all relevant variables, hence you cannot have hidden causes. In such as case we cannot say that it is a complete Bayesian Network, although the model may still hold and can be used for prediction. In general what we see here, is that this is no different that we have previously seen in machine learning, that you can search for the true model, although truth is utopia that can only very rarely if ever be achieved. I am actually a bit in doubt whether this is actually a true statement I make here, perhaps one could perhaps just say that when you have found the Markov Blanket in your model, then changes in all variables that become conditional independent cannot have an effect on the variable under evaluation if their ‘state’ changes, e.g., we impose evidence. 4.1.3 Independence We have three different scenarios: No dependence between two variables. Dependence between two variables. Conditional independence between two variables. This is the example with the broken leg, where we can make two non adjacent nodes in a serial correlation independent if we control for a given variable, that is why the name conditional independence, because variables can become independent, if check for the variable inbetween, because then you don’t need to know the ancestor (parent of the parent). See a visual example of these. Dependence examples Notice that in the last scenario we see that the A is no longer depend on B because we account for C, hence we have a conditional indepedency, where we see the chain reaction, that A is dependent on C which is dependent on B. With Bayesian networks we are able to graph this and find the dependencies and independencies and also the directions hence the causality among the variables. Therefore, we see that constrained based algorithms explores this, where score-based algorithms it primarily focusing on making good predictions. 4.1.4 Methods for identifying the network There are many different methods, although we are going to focus on two different schools: 4.1.4.1 1. Algorithms focused on causality discovery We are going to work with constrained based algorithms. Basically what this is doing is: 1. Assess data 2. Identify conditional independence tests 3. Create arcs (arrows, i.e., dependencies) based on the dependencies we find in step two. Therefore, this approach is heavily relying on the principles found the section above, regarding independence. Algorithms: There are different approaches, we are going to apply the Grow-Shrink (gs). Limitations: 1) This approach requires sufficient data to learn conditional independencies; 2) we manually have set the significance levels, hence we introduce bias. Hence we want the graph to be stable, e.g., if we introduce new data we should not see that the model changes. 4.1.4.2 2. Algorithms focused on prediction We are going to work with score-based algorithms. This is developed after the method above, hence it is also building on their algorithms. The aim for this algorithm is to make predictions, and hence not primarily identify causalities. It works in the following way: All possible DAGs (Directed Acyclic Graphs) are drawed. Evaluate and score all DAGs based on different information criteria, e.g., AIC and BIC. Choose the DAG with the highest score. We are going to work with the algorithm Hill Climbing (hs), which is a greedy search algorithm, hence acting according to the procedure above. The reason that this is not used for discovering causality, is that for instance the HS approach is looking at the model on an overall level and not on each ARC, hence we cannot rely on actual causality in the ARC, although it we can by coincidence find actual causalities. 4.1.5 Real life application There are not many applications for Bayesian Networks within Customer Analytics, but there are some examples. Although it is becoming more used. But examples are: Medical diagnostics Mechanical diagnostics Weather prediction Stock market prediction Bank services Insurances Computer science for robotic vision And other examples, all though they are limited, see the slide We see that we can use this to find the directions between the nodes and we are able to infer the probabilities between the nodes and scenarios. Example from the slides: We can use the network to simulate situations, f.eks. if we have some information on customers and we want to know if we should send out mails/commercials to the given customer. Hence we can apply a BN to see how characteristics of a given customer affects him making a purchase or not. Then we can introduce a variable about marketing or not to see how this affects the outcome. Naturally, this requires that we have data to support this. Assume that we have the data from the example, then we can calculate the expected Lift in profit for a customer by simulating whether we market or not, e.g., finding out whehter it is worth to market or not by looking at the probabilities of making a purchase. 4.2 Application 1: Explaining and Predicting Customer Retention in a Virtual Community Getting the structure Building the structure manually. Here we manually decide the arrows and hence the directions. Learning the structure from data. Here we are going to learn the arcs based on actual data. Model evaluation Making inference 4.2.1 1. Getting the structure 4.2.1.1 A. Building the structure and parameters manually 4.2.1.1.1 Approaches to make the network Three approaches: we are going to see three different approaches to making this. Use set.arc() Construct a matrix, with two columns, from and to Manually write the conditional probabilties. First we will build the graph based on a given structure. Notice that this corresponds with the graph in the slides: 4.2.1.1.1.1 First approach library(bnlearn) # Create an empty graph dag &lt;- empty.graph(nodes = c(&quot;Fuse&quot;,&quot;Plea&quot;,&quot;Atti&quot;,&quot;Comm&quot;)) dag ## ## Random/Generated Bayesian network ## ## model: ## [Fuse][Plea][Atti][Comm] ## nodes: 4 ## arcs: 0 ## undirected arcs: 0 ## directed arcs: 0 ## average markov blanket size: 0.00 ## average neighbourhood size: 0.00 ## average branching factor: 0.00 ## ## generation algorithm: Empty We see that the arcs are empty. Now we can start building the structure, hence adding arrows. # Add the arcs that encode the direct dependencies between variables dag &lt;- set.arc(dag, from = &quot;Fuse&quot;, to = &quot;Atti&quot;) dag &lt;- set.arc(dag, from = &quot;Plea&quot;, to = &quot;Atti&quot;) dag &lt;- set.arc(dag, from = &quot;Fuse&quot;, to = &quot;Comm&quot;) dag &lt;- set.arc(dag, from = &quot;Plea&quot;, to = &quot;Comm&quot;) dag &lt;- set.arc(dag, from = &quot;Atti&quot;, to = &quot;Comm&quot;) # Print the DAG dag ## ## Random/Generated Bayesian network ## ## model: ## [Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti] ## nodes: 4 ## arcs: 5 ## undirected arcs: 0 ## directed arcs: 5 ## average markov blanket size: 3.00 ## average neighbourhood size: 2.50 ## average branching factor: 1.25 ## ## generation algorithm: Empty we see the directed graph here. Now we can get the model string, also show the nodes. # Direct dependencies are listed for each variable: modelstring(dag) ## [1] &quot;[Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti]&quot; Notice that this is written as the conditional probabilities. We see that Fuse and Plea are parent nodes. We see that attitude is based on Functional usefulness and Pleasure Now we can explore it visually. # Explore the elements of the graphical network nodes(dag) arcs(dag) plot(dag) ## [1] &quot;Fuse&quot; &quot;Plea&quot; &quot;Atti&quot; &quot;Comm&quot; ## from to ## [1,] &quot;Fuse&quot; &quot;Atti&quot; ## [2,] &quot;Plea&quot; &quot;Atti&quot; ## [3,] &quot;Fuse&quot; &quot;Comm&quot; ## [4,] &quot;Plea&quot; &quot;Comm&quot; ## [5,] &quot;Atti&quot; &quot;Comm&quot; Notice that throughout the code, the Rgraphviz package is applied to better looking plots, although I have not been able to install it. Although it is not necessary. # Optional # library(Rgraphviz) # graphviz.plot(dag) 4.2.1.1.1.2 Second approach Another way to build a large network from scratch is to define the nodes and create a matrix to set the whole arc set at once: dag2 &lt;- empty.graph(nodes = c(&quot;Fuse&quot;,&quot;Plea&quot;,&quot;Atti&quot;,&quot;Comm&quot;)) arcs(dag2) = matrix (c(&quot;Fuse&quot;, &quot;Atti&quot;, &quot;Plea&quot;, &quot;Atti&quot;, &quot;Fuse&quot;, &quot;Comm&quot;, &quot;Plea&quot;, &quot;Comm&quot;, &quot;Atti&quot;, &quot;Comm&quot;), byrow = TRUE, ncol = 2, dimnames = list (NULL, c(&quot;from&quot;, &quot;to&quot;))) plot(dag2) We see that we get the exact same plot. 4.2.1.1.1.3 Third approach An even easier way to build the DAG when we know the structure: Notice that this is the output that we have seen from the first approach. When we are having a simple network it is doable, but with larger networks it gets more difficult. dag3 &lt;- model2network(&quot;[Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti]&quot;) plot(dag3) # graphviz.plot (dag3) We see that they are the same. Although we can also use a function to check for this. # Compare dags all.equal(dag, dag2) all.equal(dag, dag3) ## [1] TRUE ## [1] TRUE We see that all methods are the same. 4.2.1.1.2 Loading/entering data We see that we are manually entering the matrices: # Introducing the parameters manually Fuse.lv &lt;- c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;) Plea.lv &lt;- c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;) Atti.lv &lt;- c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;) Comm.lv &lt;- c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;) Fuse.prob &lt;- array(c(0.02, 0.26, 0.72), dim = 3, dimnames = list(Fuse = Fuse.lv)) Fuse.prob Plea.prob &lt;- array(c(0.01, 0.55, 0.44), dim = 3, dimnames= list(Plea = Plea.lv)) Plea.prob Atti.prob &lt;- array(c(0.99, 0.01, 0.00, 0.00, 0.67, 0.33, 0.01, 0.99, 0.00, 0.34, 0.33, 0.33, 0.00, 0.79, 0.21, 0.00, 0.40, 0.60, 0.99, 0.01, 0.00, 0.00, 0.47, 0.53, 0.00, 0.09, 0.91), dim = c(3, 3, 3), dimnames = list(Atti = Atti.lv, Plea = Plea.lv, Fuse = Fuse.lv)) Atti.prob ## Fuse ## Low Med High ## 0.02 0.26 0.72 ## Plea ## Low Med High ## 0.01 0.55 0.44 ## , , Fuse = Low ## ## Plea ## Atti Low Med High ## Low 0.99 0.00 0.01 ## Med 0.01 0.67 0.99 ## High 0.00 0.33 0.00 ## ## , , Fuse = Med ## ## Plea ## Atti Low Med High ## Low 0.34 0.00 0.0 ## Med 0.33 0.79 0.4 ## High 0.33 0.21 0.6 ## ## , , Fuse = High ## ## Plea ## Atti Low Med High ## Low 0.99 0.00 0.00 ## Med 0.01 0.47 0.09 ## High 0.00 0.53 0.91 We see that all these are probabilities for the given situations, just as we have seen previously with naive bayes. Comm.prob &lt;- array(c(0.00, 1.00, 0.00, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.00, 1.00, 0.00, 1.00, 0.00, 0.00, 0.34, 0.33, 0.33, 0.00, 1.00, 0.00, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.00, 0.98, 0.02, 0.00, 0.83, 0.17, 0.34, 0.33, 0.33, 0.00, 0.33, 0.67, 0.00, 0.44, 0.56, 1.00, 0.00, 0.00, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.34, 0.33, 0.33, 0.00, 0.84, 0.16, 0.00, 0.71, 0.29, 0.34, 0.33, 0.33, 0.00, 0.40, 0.60, 0.00, 0.10, 0.90), dim = c(3, 3, 3, 3), dimnames= list(Comm = Comm.lv, Atti = Atti.lv, Plea = Plea.lv, Fuse = Fuse.lv)) Comm.prob ## , , Plea = Low, Fuse = Low ## ## Atti ## Comm Low Med High ## Low 0 0.34 0.34 ## Med 1 0.33 0.33 ## High 0 0.33 0.33 ## ## , , Plea = Med, Fuse = Low ## ## Atti ## Comm Low Med High ## Low 0.34 0 1 ## Med 0.33 1 0 ## High 0.33 0 0 ## ## , , Plea = High, Fuse = Low ## ## Atti ## Comm Low Med High ## Low 0.34 0 0.34 ## Med 0.33 1 0.33 ## High 0.33 0 0.33 ## ## , , Plea = Low, Fuse = Med ## ## Atti ## Comm Low Med High ## Low 0.34 0.34 0.34 ## Med 0.33 0.33 0.33 ## High 0.33 0.33 0.33 ## ## , , Plea = Med, Fuse = Med ## ## Atti ## Comm Low Med High ## Low 0.34 0.00 0.00 ## Med 0.33 0.98 0.83 ## High 0.33 0.02 0.17 ## ## , , Plea = High, Fuse = Med ## ## Atti ## Comm Low Med High ## Low 0.34 0.00 0.00 ## Med 0.33 0.33 0.44 ## High 0.33 0.67 0.56 ## ## , , Plea = Low, Fuse = High ## ## Atti ## Comm Low Med High ## Low 1 0.34 0.34 ## Med 0 0.33 0.33 ## High 0 0.33 0.33 ## ## , , Plea = Med, Fuse = High ## ## Atti ## Comm Low Med High ## Low 0.34 0.00 0.00 ## Med 0.33 0.84 0.71 ## High 0.33 0.16 0.29 ## ## , , Plea = High, Fuse = High ## ## Atti ## Comm Low Med High ## Low 0.34 0.0 0.0 ## Med 0.33 0.4 0.1 ## High 0.33 0.6 0.9 Now where we have defined the numbers, we want to combine the matrices and the graphical structure. 4.2.1.1.3 Conditional probability table (CPT) Now we are going to combine the probabilities into a list # Relate the CPT to the labels cpt &lt;- list(Fuse = Fuse.prob, Plea = Plea.prob, Atti = Atti.prob, Comm = Comm.prob) # Relate the DAG and CPT and define a fully-specified BN bn &lt;- custom.fit(dag, cpt) bn ## ## Bayesian network parameters ## ## Parameters of node Fuse (multinomial distribution) ## ## Conditional probability table: ## Fuse ## Low Med High ## 0.02 0.26 0.72 ## ## Parameters of node Plea (multinomial distribution) ## ## Conditional probability table: ## Plea ## Low Med High ## 0.01 0.55 0.44 ## ## Parameters of node Atti (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = Low ## ## Fuse ## Atti Low Med High ## Low 0.99 0.34 0.99 ## Med 0.01 0.33 0.01 ## High 0.00 0.33 0.00 ## ## , , Plea = Med ## ## Fuse ## Atti Low Med High ## Low 0.00 0.00 0.00 ## Med 0.67 0.79 0.47 ## High 0.33 0.21 0.53 ## ## , , Plea = High ## ## Fuse ## Atti Low Med High ## Low 0.01 0.00 0.00 ## Med 0.99 0.40 0.09 ## High 0.00 0.60 0.91 ## ## ## Parameters of node Comm (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = Low, Atti = Low ## ## Fuse ## Comm Low Med High ## Low 0.00 0.34 1.00 ## Med 1.00 0.33 0.00 ## High 0.00 0.33 0.00 ## ## , , Plea = Med, Atti = Low ## ## Fuse ## Comm Low Med High ## Low 0.34 0.34 0.34 ## Med 0.33 0.33 0.33 ## High 0.33 0.33 0.33 ## ## , , Plea = High, Atti = Low ## ## Fuse ## Comm Low Med High ## Low 0.34 0.34 0.34 ## Med 0.33 0.33 0.33 ## High 0.33 0.33 0.33 ## ## , , Plea = Low, Atti = Med ## ## Fuse ## Comm Low Med High ## Low 0.34 0.34 0.34 ## Med 0.33 0.33 0.33 ## High 0.33 0.33 0.33 ## ## , , Plea = Med, Atti = Med ## ## Fuse ## Comm Low Med High ## Low 0.00 0.00 0.00 ## Med 1.00 0.98 0.84 ## High 0.00 0.02 0.16 ## ## , , Plea = High, Atti = Med ## ## Fuse ## Comm Low Med High ## Low 0.00 0.00 0.00 ## Med 1.00 0.33 0.40 ## High 0.00 0.67 0.60 ## ## , , Plea = Low, Atti = High ## ## Fuse ## Comm Low Med High ## Low 0.34 0.34 0.34 ## Med 0.33 0.33 0.33 ## High 0.33 0.33 0.33 ## ## , , Plea = Med, Atti = High ## ## Fuse ## Comm Low Med High ## Low 1.00 0.00 0.00 ## Med 0.00 0.83 0.71 ## High 0.00 0.17 0.29 ## ## , , Plea = High, Atti = High ## ## Fuse ## Comm Low Med High ## Low 0.34 0.00 0.00 ## Med 0.33 0.44 0.10 ## High 0.33 0.56 0.90 4.2.1.2 B. Learning the structure and parameters from observational data We have seen in the previous section, that we are able to manually set the directions. We can also use a constrained based algorithms to learn the links based on historical data. retention &lt;- read.csv(&quot;Data/Probabilistic Models for Customer Analytics/retention.csv&quot; ,header = T ,colClasses = &quot;factor&quot;) retention_test &lt;- read.csv(&quot;Data/Probabilistic Models for Customer Analytics/retention_test.csv&quot;, header = T, colClasses = &quot;factor&quot; ) head(retention) str(retention) Fuse Plea Atti Comm High High High High Med High High Med High Med High High Low Med High Low High Med Med Med High Med High Med ## &#39;data.frame&#39;: 5000 obs. of 4 variables: ## $ Fuse: Factor w/ 3 levels &quot;High&quot;,&quot;Low&quot;,&quot;Med&quot;: 1 3 1 2 1 1 1 1 1 1 ... ## $ Plea: Factor w/ 3 levels &quot;High&quot;,&quot;Low&quot;,&quot;Med&quot;: 1 1 3 3 3 3 1 1 3 1 ... ## $ Atti: Factor w/ 3 levels &quot;High&quot;,&quot;Low&quot;,&quot;Med&quot;: 1 1 1 1 3 1 1 1 3 1 ... ## $ Comm: Factor w/ 3 levels &quot;High&quot;,&quot;Low&quot;,&quot;Med&quot;: 1 3 1 2 3 3 1 1 3 1 ... We see that we have four vectors with three factor levels, high, low and medium. Notice that the data must be factors. Although she does mention that one can also have all numeric data, we did not explore this. bn are particularly designed for categorical variables, continuous variable require to be discretisized if all variables are continuous, a Gausian bn can be built (not discussed here). 4.2.1.2.1 Grow-shrink (gs) - Learning a structure using a constrained-based algorithm 2.1 Learning a structure using a constrained-based algorithm “grow-shrink (gs)”, with conditional independence test chi-squared Constrained-based alg. do not work with missing data bn.gs &lt;- gs(retention #The data ,alpha = 0.05 #The signifance level, for deciding if there is a relationship ,test =&quot;x2&quot;) # alternative test =&quot;mi&quot; plot(bn.gs, main = &quot;Grow shrink_X2&quot;) We see that this is the model that the model suggests, purely on the data. We see that it could not draw the direction of the Communicating and Attitude, but the rest, it was able to identify the arrows. Hence we must set the arrow ourselves. Then how do we set the arrow? A rule of thumb, is chronological order, if it cannot be applied, derive it theoretically The following is just an example with graphviz but not mandatory. #graphviz.plot (bn.gs, main = &quot;Grow shrink_X2&quot;) # notice that in the constrained-based alg some links are undirected. # this occurs because the algorithm cannot establish the direction of &quot;causality&quot;. 4.2.1.2.1.1 Identifying undirectred links Notice that when we do not manually set the links, then there is a risk of having variables where the technique is not able to identify directions. As we cannot have bidrectional links, we much search for these. If we already have knowledge about what relationships that can and cannot be there, we can blacklist and whitelist these. Blacklisting is about prohibiting a relationship while whitelisting a relationship if it should be there. Notice that you must specify the directions correctly, if a relationship just cannot be there, then you blacklist both directions. Now we can look into which links that the algorithm could not define. We can see the undirected arcs in the following. undirected.arcs(bn.gs) ## from to ## [1,] &quot;Atti&quot; &quot;Comm&quot; ## [2,] &quot;Comm&quot; &quot;Atti&quot; We need to set the direction of the undirected arcs to be able, this is done in the following. Setting directions manually Now we can set the relationship manually. bn.gs1 &lt;- set.arc(bn.gs, from = &quot;Atti&quot;, to = &quot;Comm&quot;) # to learn the parameters from observational data plot(bn.gs1, main = &quot;Grow Shrink_&quot;) #graphviz.plot(bn.gs1, main = &quot;Grow Shrink&quot;) Now we see that all variables are directed. Blacklisting and whitelisting relationships In the following we blacklist a relationship, hence we see that there can be no relationship from commitment to attitude, but not the other way around. Hence if we know such, it could be a good idea to provide that information. Therefore we are now going to imposing restrictions to the algorithm based on previous knowledge + blacklist() and whitelist(): e.g., if Comm cannot theoretically determine Atti, but Atti can determine Comm bn.gs &lt;- gs(retention, blacklist = c(&quot;Comm&quot;, &quot;Atti&quot;)) # graphviz.plot(bn.gs, main = &quot;Grow Shrink_ with restriction 1&quot;) Now we can blacklist one more relationship if arc Atti - Comm should not be there at all blacklist &lt;- data.frame(from = c(&quot;Comm&quot;, &quot;Atti&quot;), to = c(&quot;Atti&quot;, &quot;Comm&quot;)) blacklist bn.gsb &lt;- gs(retention, blacklist = blacklist) plot(bn.gsb, main = &quot;Grow Shrink_with restriction 2&quot;) # graphviz.plot(bn.gsb, main = &quot;Grow Shrink_with restrictions&quot;) from to Comm Atti Atti Comm Now we see that the link between Comm and Atti is not there. We can also allow for a relationship, that is done by white listing. if theoretically the arc Atti -&gt; Comm should be there bn.gsw &lt;- gs(retention, whitelist = c(&quot;Atti&quot;, &quot;Comm&quot;)) plot(bn.gsw, main = &quot;Grow Shrink_with restriction 3&quot;) # graphviz.plot(bn.gsw, main = &quot;Grow Shrink_with restriction 3&quot;) We see that the arrow is not there again, going from attitude to commitment, because we whitelisted that relationship. 4.2.1.2.2 Hill-Climbing (hc) - Learning the structure using a score-based algorithm Hill-Climbing (hc) greedy search, this is made for prediction. Here we are using an information criteria to predict the relationships and choose on the IC. bn.hc &lt;- hc(retention, score = &quot;bde&quot;) plot (bn.hc, main = &quot;Hill Climbing_BDe&quot;) #graphviz.plot (bn.hc, main = &quot;Hill Climbing_BDe&quot;) bn.hc &lt;- hc(retention, score = &quot;bic&quot;) plot (bn.hc, main = &quot;Hill Climbing_BIC&quot;) #graphviz.plot (bn.hc, main = &quot;Hill Climbing_BIC&quot;) We see that this suggests two different structures. 4.2.1.2.3 Fitting the model to the data (learning the probabilities) Notice that in the first section with the manual method, we manually made the conditional probabilities. We are going to learn the model parameters using the Grow-Shrink model that was created earlier. Bayesian network model I chose bn.gs1. bn.mle &lt;- bn.fit(bn.gs1, data = retention, method = &quot;mle&quot;) bn.mle #One by one bn.mle$Fuse #Parent bn.mle$Plea #Parent bn.mle$Atti bn.mle$Comm # Other useful functions # set.arc(net, from = &quot;A&quot;, to = &quot;T&quot;) # drop.arc(net, from=&quot;A&quot;, to=&quot;&quot;T) # e.g. newnet = drop.arc(net, from = &quot;T&quot;, to = &quot;A&quot;) # Test for the conditional independence between variables # ci.test(&quot;T&quot;, &quot;E&quot;, c(&quot;O&quot;, &quot;R&quot;), test = &quot;x2&quot;, data = data) ## ## Bayesian network parameters ## ## Parameters of node Fuse (multinomial distribution) ## ## Conditional probability table: ## High Low Med ## 0.7206 0.0212 0.2582 ## ## Parameters of node Plea (multinomial distribution) ## ## Conditional probability table: ## High Low Med ## 0.4346 0.0094 0.5560 ## ## Parameters of node Atti (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = High ## ## Fuse ## Atti High Low Med ## High 0.91332059 0.00000000 0.58676208 ## Low 0.00000000 0.02222222 0.00000000 ## Med 0.08667941 0.97777778 0.41323792 ## ## , , Plea = Low ## ## Fuse ## Atti High Low Med ## High 0.00000000 0.00000000 0.36363636 ## Low 1.00000000 1.00000000 0.45454545 ## Med 0.00000000 0.00000000 0.18181818 ## ## , , Plea = Med ## ## Fuse ## Atti High Low Med ## High 0.52576288 0.28333333 0.19694868 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.47423712 0.71666667 0.80305132 ## ## ## Parameters of node Comm (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = High, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.89602233 0.56402439 ## Low 0.00000000 0.00000000 ## Med 0.10397767 0.43597561 ## ## , , Plea = Low, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.50000000 ## Low 0.25000000 ## Med 0.25000000 ## ## , , Plea = Med, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.30066603 0.00000000 0.11267606 ## Low 0.00000000 1.00000000 0.00000000 ## Med 0.69933397 0.00000000 0.88732394 ## ## , , Plea = High, Atti = Low ## ## Fuse ## Comm High Low Med ## High 1.00000000 ## Low 0.00000000 ## Med 0.00000000 ## ## , , Plea = Low, Atti = Low ## ## Fuse ## Comm High Low Med ## High 0.00000000 0.00000000 0.00000000 ## Low 1.00000000 0.00000000 0.40000000 ## Med 0.00000000 1.00000000 0.60000000 ## ## , , Plea = Med, Atti = Low ## ## Fuse ## Comm High Low Med ## High ## Low ## Med ## ## , , Plea = High, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.61029412 0.00000000 0.67532468 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.38970588 1.00000000 0.32467532 ## ## , , Plea = Low, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.00000000 ## Low 0.50000000 ## Med 0.50000000 ## ## , , Plea = Med, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.17405063 0.00000000 0.01899827 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.82594937 1.00000000 0.98100173 ## ## ## ## Parameters of node Fuse (multinomial distribution) ## ## Conditional probability table: ## High Low Med ## 0.7206 0.0212 0.2582 ## ## Parameters of node Plea (multinomial distribution) ## ## Conditional probability table: ## High Low Med ## 0.4346 0.0094 0.5560 ## ## Parameters of node Atti (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = High ## ## Fuse ## Atti High Low Med ## High 0.91332059 0.00000000 0.58676208 ## Low 0.00000000 0.02222222 0.00000000 ## Med 0.08667941 0.97777778 0.41323792 ## ## , , Plea = Low ## ## Fuse ## Atti High Low Med ## High 0.00000000 0.00000000 0.36363636 ## Low 1.00000000 1.00000000 0.45454545 ## Med 0.00000000 0.00000000 0.18181818 ## ## , , Plea = Med ## ## Fuse ## Atti High Low Med ## High 0.52576288 0.28333333 0.19694868 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.47423712 0.71666667 0.80305132 ## ## ## Parameters of node Comm (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = High, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.89602233 0.56402439 ## Low 0.00000000 0.00000000 ## Med 0.10397767 0.43597561 ## ## , , Plea = Low, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.50000000 ## Low 0.25000000 ## Med 0.25000000 ## ## , , Plea = Med, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.30066603 0.00000000 0.11267606 ## Low 0.00000000 1.00000000 0.00000000 ## Med 0.69933397 0.00000000 0.88732394 ## ## , , Plea = High, Atti = Low ## ## Fuse ## Comm High Low Med ## High 1.00000000 ## Low 0.00000000 ## Med 0.00000000 ## ## , , Plea = Low, Atti = Low ## ## Fuse ## Comm High Low Med ## High 0.00000000 0.00000000 0.00000000 ## Low 1.00000000 0.00000000 0.40000000 ## Med 0.00000000 1.00000000 0.60000000 ## ## , , Plea = Med, Atti = Low ## ## Fuse ## Comm High Low Med ## High ## Low ## Med ## ## , , Plea = High, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.61029412 0.00000000 0.67532468 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.38970588 1.00000000 0.32467532 ## ## , , Plea = Low, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.00000000 ## Low 0.50000000 ## Med 0.50000000 ## ## , , Plea = Med, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.17405063 0.00000000 0.01899827 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.82594937 1.00000000 0.98100173 4.2.1.2.4 Alternatives to Grow-Shrink and HC These are others algorithms to constructing the Bayesian Network. # other constraint-based algorithms have been developed bn2 &lt;- iamb(retention, alpha = 0.05, test =&quot;mi&quot;) #graphviz.plot (bn2, main = &quot;Iamb1_mi&quot; ) bn3 &lt;- fast.iamb(retention, alpha = 0.05, test =&quot;mi&quot;) #graphviz.plot (bn3, main = &quot;FastIamb_mi&quot;) bn4 &lt;- inter.iamb(retention, alpha = 0.05, test =&quot;mi&quot; ) #graphviz.plot (bn4, main = &quot;InterIamb_mi&quot;) # in the optimal case, all will return the same graph 4.2.2 2. Model evaluation Now we are going to test how good the model is. 4.2.2.1 I. Metrics of model complexity nodes(bn.mle) arcs(bn.mle) bn.mle plot(bn.gs1) ## [1] &quot;Fuse&quot; &quot;Plea&quot; &quot;Atti&quot; &quot;Comm&quot; ## from to ## [1,] &quot;Fuse&quot; &quot;Atti&quot; ## [2,] &quot;Fuse&quot; &quot;Comm&quot; ## [3,] &quot;Plea&quot; &quot;Atti&quot; ## [4,] &quot;Plea&quot; &quot;Comm&quot; ## [5,] &quot;Atti&quot; &quot;Comm&quot; ## ## Bayesian network parameters ## ## Parameters of node Fuse (multinomial distribution) ## ## Conditional probability table: ## High Low Med ## 0.7206 0.0212 0.2582 ## ## Parameters of node Plea (multinomial distribution) ## ## Conditional probability table: ## High Low Med ## 0.4346 0.0094 0.5560 ## ## Parameters of node Atti (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = High ## ## Fuse ## Atti High Low Med ## High 0.91332059 0.00000000 0.58676208 ## Low 0.00000000 0.02222222 0.00000000 ## Med 0.08667941 0.97777778 0.41323792 ## ## , , Plea = Low ## ## Fuse ## Atti High Low Med ## High 0.00000000 0.00000000 0.36363636 ## Low 1.00000000 1.00000000 0.45454545 ## Med 0.00000000 0.00000000 0.18181818 ## ## , , Plea = Med ## ## Fuse ## Atti High Low Med ## High 0.52576288 0.28333333 0.19694868 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.47423712 0.71666667 0.80305132 ## ## ## Parameters of node Comm (multinomial distribution) ## ## Conditional probability table: ## ## , , Plea = High, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.89602233 0.56402439 ## Low 0.00000000 0.00000000 ## Med 0.10397767 0.43597561 ## ## , , Plea = Low, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.50000000 ## Low 0.25000000 ## Med 0.25000000 ## ## , , Plea = Med, Atti = High ## ## Fuse ## Comm High Low Med ## High 0.30066603 0.00000000 0.11267606 ## Low 0.00000000 1.00000000 0.00000000 ## Med 0.69933397 0.00000000 0.88732394 ## ## , , Plea = High, Atti = Low ## ## Fuse ## Comm High Low Med ## High 1.00000000 ## Low 0.00000000 ## Med 0.00000000 ## ## , , Plea = Low, Atti = Low ## ## Fuse ## Comm High Low Med ## High 0.00000000 0.00000000 0.00000000 ## Low 1.00000000 0.00000000 0.40000000 ## Med 0.00000000 1.00000000 0.60000000 ## ## , , Plea = Med, Atti = Low ## ## Fuse ## Comm High Low Med ## High ## Low ## Med ## ## , , Plea = High, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.61029412 0.00000000 0.67532468 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.38970588 1.00000000 0.32467532 ## ## , , Plea = Low, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.00000000 ## Low 0.50000000 ## Med 0.50000000 ## ## , , Plea = Med, Atti = Med ## ## Fuse ## Comm High Low Med ## High 0.17405063 0.00000000 0.01899827 ## Low 0.00000000 0.00000000 0.00000000 ## Med 0.82594937 1.00000000 0.98100173 We see that the model is rather simple, not many variables and links. 4.2.2.2 II. Metrics of model sensitivity Test if any two nodes are d-separated, meaning that information for one variables to another flows or not. For this I refer back to the conditional independence. In the following we see that we just compare two nodes, although one could also add the scenario of imposing evidence (hence controlling for a given variable). dsep(bn.mle, x = &quot;Plea&quot;, y = &quot;Fuse&quot;) dsep(bn.mle, x = &quot;Plea&quot;, y = &quot;Comm&quot;) ## [1] TRUE ## [1] FALSE We see that Pleasure and functional usefulness is not dependent on each other, although pleasure and communication is not separated, hence dependent. 4.2.2.3 III. Evaluate the arc.strength() 4.2.2.3.1 For the growth shrink Now we want to see how strong the relationship is between the nodes. Normally we have beta values as normally, hence we can look at: p-values: with criterion = “x2” or “mi” (mutual information), the output reports the p-value for the test. The lower the p-value, the stronger the relationship. using BIC: with criterion =“bic” reports the change in the BIC score of the net caused by an arc removal.The more negative the change, means the BIC score will go worse if we delete that arc (i.e. the arc is important for the model). library(dplyr) options(cipen = 0) arc.strength(bn.gs1, retention, criterion = &quot;x2&quot;) %&gt;% as.data.frame() %&gt;% arrange(strength) from to strength Plea Atti 0 Plea Comm 0 Fuse Comm 0 Fuse Atti 0 Atti Comm 0 We see that the strength between Pleasure and Attitude is not strong, hence it makes sense that not all models find the relationship. That also means the the significance of this relationship is low. arc.strength(bn.gs1, retention, criterion = &quot;bic&quot;) %&gt;% as.data.frame() %&gt;% arrange(strength) from to strength Plea Comm -668.21187 Plea Atti -638.17237 Fuse Atti -302.40435 Fuse Comm -147.83732 Atti Comm 40.48931 The output reveals that, if we remove Plea -&gt; Comm, BIC will decrease with -668.211, which in bnlearn means the model will get worse. The output reveals that, if we remove Atti -&gt; Comm, BIC will increase with 40.48, which in bnlearn package means the model may improve based on this index. We see that the relationship between Pleasure and commitment for instance is very high. 4.2.2.3.2 For the hill-climbing structure Notice that we are just merely repeating the same here, just with the other method. arc.strength(bn.hc, retention, criterion = &quot;bic&quot;) %&gt;% as.data.frame() %&gt;% arrange(strength) from to strength Plea Comm -1221.6057 Plea Atti -638.1724 Fuse Atti -302.4043 Fuse Comm -277.3471 As expected, all strenghts are negative as the model is fitted against BIC, hence if it could improve the model by removing the given link, then it would do so. 4.2.2.3.3 Different information criteria evaluations Metrics of evaluation and selection among several dags: BIC, BDe, AIC scores are used to compare alternative structures and choose the best. In bnlearn, AIC, BIC, BDE closer to zero means better model; often the three indexesdo not agree. See the following examples: bnlearn::score(bn.gs1, retention, type = &quot;aic&quot;) bnlearn::score(bn.hc, retention, type = &quot;aic&quot;) bnlearn::score(bn.gs1, retention, type = &quot;bic&quot;) bnlearn::score(bn.hc, retention, type = &quot;bic&quot;) bnlearn::score(bn.gs1, retention, type = &quot;bde&quot;) bnlearn::score(bn.hc, retention, type = &quot;bde&quot;) ## [1] -11842.95 ## [1] -11919.77 ## [1] -12090.61 ## [1] -12050.12 ## [1] -11904.39 ## [1] -11983.02 We see that the models that we have learned through HC ans GS, they appear to all be the same. Probably because they were trained on the same IC, during the lecture we see that there is a difference. 4.2.2.4 IV. Metrics of predictive accuracy (error rate, confusion matrix, AUC) Here we are going to predict one variable and see what the error rate will be. library(gRain) library(gRbase) library (caTools) 4.2.2.4.1 Cross validation This function requires as one of its parameters only structure (not the full bayesian model). We use classification error (“pred”) for the node Comm (our target) as a loss function. netcv &lt;- bn.cv(retention, bn.gs1 ,loss =&quot;pred&quot; #Gives error rate ,k = 5 ,loss.args = list(target = &quot;Comm&quot;) ,debug = TRUE) netcv ## * splitting 5000 data in 5 subsets. ## ---------------------------------------------------------------- ## * fitting the parameters of the network from the training sample. ## * applying the loss function to the data from the test sample. ## &gt; classification error for node Comm is 0.1971972 . ## @ total loss is 0.1971972 . ## ---------------------------------------------------------------- ## * fitting the parameters of the network from the training sample. ## * applying the loss function to the data from the test sample. ## &gt; classification error for node Comm is 0.182 . ## @ total loss is 0.182 . ## ---------------------------------------------------------------- ## * fitting the parameters of the network from the training sample. ## * applying the loss function to the data from the test sample. ## &gt; classification error for node Comm is 0.204 . ## @ total loss is 0.204 . ## ---------------------------------------------------------------- ## * fitting the parameters of the network from the training sample. ## * applying the loss function to the data from the test sample. ## &gt; classification error for node Comm is 0.1771772 . ## @ total loss is 0.1771772 . ## ---------------------------------------------------------------- ## * fitting the parameters of the network from the training sample. ## * applying the loss function to the data from the test sample. ## &gt; classification error for node Comm is 0.176 . ## @ total loss is 0.176 . ## ---------------------------------------------------------------- ## * summary of the observed values for the loss function: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.1760 0.1772 0.1820 0.1873 0.1972 0.2040 ## ## k-fold cross-validation for Bayesian networks ## ## target network structure: ## [Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti] ## number of folds: 5 ## loss function: Classification Error ## training node: Comm ## expected loss: 0.1872749 The prediction accuracy of Comm based on 5-fold cross validation is quite high (1-0.18 = 0.82) one can also look at each individual variables. Using a testing sample to evaluate the model performance we need to transform the full bayesian model into a gRain object net1 &lt;- as.grain(bn.mle) net1 ## Independence network: Compiled: TRUE Propagated: FALSE ## Nodes: chr [1:4] &quot;Fuse&quot; &quot;Plea&quot; &quot;Atti&quot; &quot;Comm&quot; assuming Comm is the target node, we predict the probability of Comm being High, Medium and Large using net1 in the test sample. predComm &lt;- predict(net1 ,response = c(&quot;Comm&quot;) ,newdata = retention_test ,predictors = names(retention_test)[-4] #Removing target variable ,type = &quot;distribution&quot; #This returns likelihoods ) predComm = predComm$pred$Comm #View(predComm) #Shows the likelihood # write.csv(predComm, &quot;PredComm.csv&quot;, row.names = FALSE) Or add the predictions to the data file creating a new variable. Instead of probabilities of 0 or 1, one can save the actual CLASS (0/1). predComm_class &lt;- predict(object = net1 ,response = c(&quot;Comm&quot;) ,newdata = retention_test ,predictors = names (retention_test)[-4] #Removing target variable ,type = &quot;class&quot; #This just returns the class ) predCommclass &lt;- predComm_class$pred$Comm #View(predCommclass) # write.csv(predictionsTclass, &quot;PredictedTclass&quot;) 4.2.2.4.2 Alternative approach to fit and predict (Without gRain) A quick method WITHOUT using package gRain. #Speficy network model &lt;- model2network(&quot;[Fuse][Plea][Atti|Fuse:Plea][Comm|Fuse:Plea:Atti]&quot;) #Fit the data bn.mle1 &lt;- bn.fit(model ,retention) #Make predictions predComm1 &lt;- predict(bn.mle1 ,node = &quot;Comm&quot; ,data = retention_test) #See the predictions compared to previous predictions. table(predCommclass, predComm1) ## predComm1 ## predCommclass High Low Med ## High 1098 0 0 ## Low 0 26 2 ## Med 0 0 1374 We see that this methods almost makes the same results. We cam also make a confusion matrix. table(predComm_class$pred$Comm, retention_test$Comm) ## ## High Low Med ## High 903 1 194 ## Low 2 25 1 ## Med 250 1 1123 Plotting ROC and AUC Notice that ROC and AUC is made for a two class scenario, hence it returns nine lines, one for each line. library(caTools) colAUC(predComm, retention_test[ ,4], plotROC = TRUE) #requires the predicted probabilities, not the predicted class ## High Low Med ## High vs. Low 0.9910855 0.9998397 0.9195447 ## High vs. Med 0.8753043 0.5001077 0.8764289 ## Low vs. Med 0.9556286 0.9998876 0.9876075 we get an AUC for every column of the prediction matrix our DV has 3 categories: Low, Med and High we observe that the model has problems when d istiguisging between high and medium but performs pretty well when indentifying the Low category (customers who are not commited) 4.2.3 3. Making inference and queries based on the model How do we use the model in practice to make inference with incomplete data? Below we consider several hypothetical situations. A customer is considered retained if he has a positive attitude and high commitment to the Virtual Community (hereafter, VC). How do functional usefulness and pleasure influence on attitude and commitment? We know the strength of the relationship based on the arc strength. The following analysis helps to understand the relationship, which we will see that it is not be necessarily linear Using BN, one can evaluate the expected changes in attitude, and respectively, commitment due to changes in functional usefulness and pleasure. We will set evidence in the network for Fuse and Plea and we´ll look at the cpt (conditional probability table) for Atti and Comm before and after setting the evidence. Setting (hard) evidence means setting one of the states of the variable at probability 1 (that is, as 100%). Transform the bn into a junction tree We are going to make two types of diagnositcs: Forward inference: following the directions, hence setting evidence for the parents and then see how that affects the children Backward inference: going against the directions, hence setting evidence for children and then seeing how that affects the parents. The following is also forward integration library (gRain) junction &lt;- compile(as.grain(bn.mle)) # &quot;querygrain&quot; function extracts the marginal distribution of the nodes First we are going to look at the prior probabilities, that is done in the following. # ctp for atti and comm are: querygrain(junction, nodes = &quot;Atti&quot;) querygrain(junction, nodes = &quot;Comm&quot;) ## $Atti ## Atti ## High Low Med ## 0.595014752 0.008280883 0.396704365 ## ## $Comm ## Comm ## High Low Med ## 0.44373332 0.01099592 0.54527076 Imagine a new person joins the VC reporting a Low, Medium, or High Functional Usefulness perception. This information can be fed to the network as evidence in order to predict the conditional probability of his/her attitude and commitment to VC. # new ctp(if Fuse = Low) jLow &lt;- setEvidence (junction, nodes = &quot;Fuse&quot;, states = &quot;Low&quot;) A1 &lt;- querygrain(jLow, nodes = &quot;Atti&quot;) A1 ## $Atti ## Atti ## High Low Med ## 0.15753333 0.01905778 0.82340889 We see that we enter a piece of information and then see how the attitude changes. So we see that it went from 59% to 15%, which is a strong decrease. We can do the same with commitment C1 &lt;- querygrain(jLow, nodes = &quot;Comm&quot;) C1 ## $Comm ## Comm ## High Low Med ## 0.009657778 0.157533333 0.832808889 We see that the High segment in the commitment as weel. # new ctp (if Fuse = Med) jMed &lt;- setEvidence (junction, nodes = &quot;Fuse&quot;, states = &quot;Med&quot;) A2 &lt;- querygrain(jMed, nodes = &quot;Atti&quot;) A2 C2 &lt;- querygrain(jMed, nodes = &quot;Comm&quot;) C2 # new ctp (if Fuse = High) jHigh &lt;- setEvidence (junction, nodes = &quot;Fuse&quot;, states = &quot;High&quot;) A3 &lt;- querygrain(jHigh, nodes = &quot;Atti&quot;) A3 C3 &lt;- querygrain(jHigh, nodes = &quot;Comm&quot;) C3 ## $Atti ## Atti ## High Low Med ## 0.367928447 0.004272727 0.627798826 ## ## $Comm ## Comm ## High Low Med ## 0.287643947 0.003418182 0.708937871 ## ## $Atti ## Atti ## High Low Med ## 0.6892533 0.0094000 0.3013467 ## ## $Comm ## Comm ## High Low Med ## 0.5124326 0.0094000 0.4781674 4.2.3.1 Diagnostics (Forward inference) Here we are going to set evidence for a given customer and see how probabilities for other variables are affected. We see that we get likelihood of the different states where attitude is set to low, medium and high. We see that when attitude = low, then changes in fuse will have no effect. # Summary (only for Atti) AttiHigh &lt;-c(A1$Atti[[3]], A2$Atti[[3]], A3$Atti[[3]]) AttiLow &lt;- c(A1$Atti[[1]], A2$Atti[[1]], A3$Atti[[1]]) AttiMed &lt;- c(A1$Atti[[2]], A2$Atti[[2]], A3$Atti[[2]]) df1 &lt;- data.frame(Fuse = c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;), AttiLow, AttiMed, AttiHigh) df1 matplot(rownames(df1), df1 ,type = &#39;l&#39; ,xlab = &#39;Fuse&#39; ,ylab = &#39;Probability&#39; ,ylim = c(0,1) ,col = c(3,4,2)) legend(&#39;topright&#39; ,inset = .01 ,legend = colnames(df1[,2:4]) #&quot;AttiLow&quot; &quot;AttiMed&quot; &quot;AttiHigh&quot; ,pch = 1,horiz = T ,col = 2:4) Fuse AttiLow AttiMed AttiHigh Low 0.1575333 0.0190578 0.8234089 Med 0.3679284 0.0042727 0.6277988 High 0.6892533 0.0094000 0.3013467 Discussion as Fuse changes from low to medium to high, the high state of attitude shows an increasing trend, the medium state of attitude shows a decreasing trend, the low state of attitude shows a constant trend. it depicts that when functional usefulness is low, the probability of attitude medium is quite high (0.80) and thus functional usefulness does not radically affect the customer´s attitude. 4.2.3.1.1 let us repeat the same analysis by setting evidence in Pleasure We can do the same graphs, just where we see the relationship between pleasure and attitude. # new ctp jLow &lt;- setEvidence (junction, nodes = &quot;Plea&quot;, states = &quot;Low&quot;) A1 = querygrain(jLow, nodes = &quot;Atti&quot;) C1 = querygrain(jLow, nodes = &quot;Comm&quot;) # New ctp jMed &lt;- setEvidence (junction, nodes = &quot;Plea&quot;, states = &quot;Med&quot;) A2 = querygrain(jMed, nodes = &quot;Atti&quot;) C2 = querygrain(jMed, nodes = &quot;Comm&quot;) # New ctp jHigh &lt;- setEvidence (junction, nodes = &quot;Plea&quot;, states = &quot;High&quot;) A3 = querygrain(jHigh, nodes = &quot;Atti&quot;) C3 = querygrain(jHigh, nodes = &quot;Comm&quot;) # summary AttiHigh &lt;- c(A1$Atti[[3]], A2$Atti[[3]], A3$Atti[[3]]) AttiLow &lt;- c(A1$Atti[[1]], A2$Atti[[1]], A3$Atti[[1]]) AttiMed &lt;- c(A1$Atti[[2]], A2$Atti[[2]], A3$Atti[[2]]) df2 &lt;- data.frame(Plea = c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;), AttiLow, AttiMed, AttiHigh) options(scipen = 999) df2 matplot(rownames(df2), df2, type = &#39;l&#39;, xlab = &#39;Plea&#39; ,ylab = &#39;Probability&#39;, ylim = c(0,1) ,col = c(3,4,2)) legend(&#39;topright&#39;, inset=.01, legend=colnames(df2[,2:4]), pch=1, horiz=T, col=2:4) Plea AttiLow AttiMed AttiHigh Low 0.0938909 0.8591636 0.0469455 Med 0.4357235 0.0000000 0.5642765 High 0.8096408 0.0004711 0.1898881 We see that the probability of a high attitude increases as pleasure increases. Although we see that the low attitude probability decrease as pleasure increases, and at some point, the low attitude will Discussion as Plea changes from low to medium to high, the high state of attitude shows an increasing trend the medium state of attitude shows a mixed trend and the low state of attitude shows a decreasing trend When pleasure is low, the probability of attitude being low is high (0.8592). This means pleasure has a stronger relationship with attitude; hence, to increase the attitude of customers, it is more important to enhance their perceived pleasure than to enhance functional usefulness. Similar influences of changes of FUSE, PLEA and ATTI on commitment can be similarly explored. To conclude, the above inference is called forward inference because we set evidence in the parent and look at ctp changes in the child. Next, we look at backward inference - a great type of inference offered by BN models. 4.2.3.2 Diagnostic (Backward inference) Here we are going to go against the arrows. Let’s assume that the evidence given is that the customer’s attitude towards VC is high. This information is fed to the network by setting the probability of attitude being high (that is, as 1.00) and observing the changes in the parent variables (Fuse and Plea) # Prior ctp querygrain(junction, nodes = &quot;Fuse&quot;) querygrain(junction, nodes = &quot;Plea&quot;) # New ctp jHigh &lt;- setEvidence (junction, nodes = &quot;Atti&quot;, states = &quot;High&quot;) querygrain(jHigh, nodes = &quot;Fuse&quot;) querygrain(jHigh, nodes = &quot;Plea&quot;) ## $Fuse ## Fuse ## High Low Med ## 0.7206 0.0212 0.2582 ## ## $Plea ## Plea ## High Low Med ## 0.4346 0.0094 0.5560 ## ## $Fuse ## Fuse ## High Low Med ## 0.834728750 0.005612813 0.159658437 ## ## $Plea ## Plea ## High Low Med ## 0.591363295 0.001483282 0.407153423 Discussion The probability of the high state of Plea and Fuse is increasing, while the probability of the low and medium states of Plea and Fuse is decreasing. This implies that increased attitude towards interaction in a VC is because of person’s increased perception of pleasure and functional usefulness in the VC. An example Assume that the online vendor observes decreasing commitment towards participation among its customers. He can set evidence to the network that the probability of commitment is low and see the effect of the parent variables (attitude, functional usefulness and pleasure). #Prior ctp querygrain(junction, nodes = &quot;Atti&quot;) querygrain(junction, nodes = &quot;Fuse&quot;) querygrain(junction, nodes = &quot;Plea&quot;) ## $Atti ## Atti ## High Low Med ## 0.595014752 0.008280883 0.396704365 ## ## $Fuse ## Fuse ## High Low Med ## 0.7206 0.0212 0.2582 ## ## $Plea ## Plea ## High Low Med ## 0.4346 0.0094 0.5560 # New ctp CLow &lt;- setEvidence (junction, nodes = &quot;Comm&quot;, states = &quot;Low&quot;) #We set commitment to low querygrain(CLow, nodes = &quot;Atti&quot;) querygrain(CLow, nodes = &quot;Fuse&quot;) querygrain(CLow, nodes = &quot;Plea&quot;) ## $Atti ## Atti ## High Low Med ## 0.32378827 0.65614578 0.02006595 ## ## $Fuse ## Fuse ## High Low Med ## 0.61601387 0.30372232 0.08026381 ## ## $Plea ## Plea ## High Low Med ## 0.0000000 0.6962777 0.3037223 A vendor, therefore, needs to take corrective action to enhance customers’ pleasure aspect in the VC to improve customers’ commitment toward the VC. we found that it is mainly due to lack of enhancement of the pleasure aspect of the website; the vendors needed to take corrective action to provide the customers with more fun and enjoyment. We see that when the communication is set to low, then the probability of pleasure being high is 0. Modeling contradictory behavior Assume some customers interact in the VC to seek information from the VC, but do not participate in VC activities. Such customers can be considered persons with positive attitudes but low commitment. Can BN predict the reasons behind such contradictory behavior? # Prior ctp querygrain(junction, nodes = &quot;Fuse&quot;) querygrain(junction, nodes = &quot;Plea&quot;) # New ctp AHigh &lt;- setEvidence (junction, nodes = &quot;Att&quot;, states = &quot;High&quot;) AHighCLow &lt;- setEvidence (AHigh, nodes = &quot;Comm&quot;, states = &quot;Low&quot;) querygrain(AHighCLow, nodes = &quot;Fuse&quot;) querygrain(AHighCLow, nodes = &quot;Plea&quot;) ## $Fuse ## Fuse ## High Low Med ## 0.7206 0.0212 0.2582 ## ## $Plea ## Plea ## High Low Med ## 0.4346 0.0094 0.5560 ## ## $Fuse ## Fuse ## High Low Med ## 0.61601387 0.30372232 0.08026381 ## ## $Plea ## Plea ## High Low Med ## 0.0000000 0.6962777 0.3037223 Discussion The probability of attitude high and commitment low was set to 1.00. The results imply that FUSE is 0.3 likely to be low but a significant proportion is still likely to be high (0.62); instead, PLEA is most likely to be low (0.7) and unlikelly to be high (0%). This reveals that customers interact primarily because of fun, and they do not perceive the VC to be sufficiently useful to them to commit to. 4.3 Application 2: Identifying populations with Positive Expected Lift in Profit (ELP) and targeting The idea is to use BN to identify segments of customers that will most likely purchase when sending the ad (persuadable segments) and avoid sending the ad to the rest: 1) to the ones who will not buy the advertised product ever, 2) to the ones who will be offended by receiving an unwanted advertisement or call, 3) or to the ones who will always buy) If historical data is available, we start learning the relationships between the variables train and select the best model as a prelimimnary step. Below we use the model structure from the text (given),and we only have to learn the parameters (probabilities). Next,we focus exclusively on how model is used as a decision support for marketing managers. targeted.adv &lt;- read.csv(&quot;Data/Probabilistic Models for Customer Analytics/targeted.adv.csv&quot; ,header = T, colClasses = &quot;factor&quot;) head(targeted.adv) str(targeted.adv) IDnum Buy Income Sex Mailed 1 yes high male no 2 no low male no 3 no low female yes 4 no low female yes 5 no low female no 6 no low female no ## &#39;data.frame&#39;: 1000 obs. of 5 variables: ## $ IDnum : Factor w/ 1000 levels &quot;1&quot;,&quot;10&quot;,&quot;100&quot;,..: 1 113 224 335 446 557 668 779 890 2 ... ## $ Buy : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 1 1 1 1 1 2 2 2 1 ... ## $ Income: Factor w/ 3 levels &quot;high&quot;,&quot;low&quot;,&quot;medium&quot;: 1 2 2 2 2 2 1 3 1 3 ... ## $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 1 1 1 1 1 1 2 1 ... ## $ Mailed: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 2 2 1 1 2 2 2 1 ... We see that the table contains 5 columns, ID, Buy, Income, Sex and Mailed. All the variables are factor variables # Build the structure dagTA &lt;- model2network(&quot;[Income][Sex][Mailed][Buy|Income:Sex:Mailed]&quot;) plot(dagTA) We see that the all variables leads towards buy. # Learn the cpt (conditional probability table) bnTA.mle &lt;- bn.fit(dagTA, data = targeted.adv[, c(2:5)], method = &quot;mle&quot;) bnTA.mle ## ## Bayesian network parameters ## ## Parameters of node Buy (multinomial distribution) ## ## Conditional probability table: ## ## , , Mailed = no, Sex = female ## ## Income ## Buy high low medium ## no 0.4090909 0.5326087 0.5609756 ## yes 0.5909091 0.4673913 0.4390244 ## ## , , Mailed = yes, Sex = female ## ## Income ## Buy high low medium ## no 0.4197531 0.4204545 0.2812500 ## yes 0.5802469 0.5795455 0.7187500 ## ## , , Mailed = no, Sex = male ## ## Income ## Buy high low medium ## no 0.4285714 0.5306122 0.8043478 ## yes 0.5714286 0.4693878 0.1956522 ## ## , , Mailed = yes, Sex = male ## ## Income ## Buy high low medium ## no 0.4555556 0.3827160 0.5853659 ## yes 0.5444444 0.6172840 0.4146341 ## ## ## Parameters of node Income (multinomial distribution) ## ## Conditional probability table: ## high low medium ## 0.321 0.359 0.320 ## ## Parameters of node Mailed (multinomial distribution) ## ## Conditional probability table: ## no yes ## 0.514 0.486 ## ## Parameters of node Sex (multinomial distribution) ## ## Conditional probability table: ## female male ## 0.473 0.527 We know that the whole network is based upon the conditional probabilities. Now we want to calculate the ELP (Expected Lift in Profit) The function is basically \\[ELP = P(Buy = Yes| Mailed = yes) * r_s - P(Buy = Yes| Mailed = no) r_u - c\\] That applies for any given population Y. We see that we have the following notations: \\(c\\) = cost of mailing the ad to a give person \\(r_u\\) = the income obtained from a sale to en unsolicited customer \\(r_c\\) = the income obtained from a sale to en solicited customer \\(r_u\\) and \\(r_s\\) = are different because we may offer some discount in the ad We can take the following assumptions. Now we need to do three different scenarios: Compute the ELP for the population consisting of individuals with medium income who are male. Should we mail an ad to this population? Compute the ELP for the population consisting of individuals with medium income who are female. Should we mail the ad to this population? Compute the ELP for the population consisting of individuals with low income. Should we mail an ad this population? Scenario 1 We are going to evaluate males with high income. First we see the probability of buying when mailed and when not. c = 0.5 #Cost r_s = 8 #Income with discount r_u = 10 #Income obtained from a sale to en unsolicited customer # set evidence and get the cpt library (gRain) junctionTA &lt;- compile(as.grain(bnTA.mle)) #We have mailed Med_male_yes &lt;- setEvidence(junctionTA, nodes = c(&quot;Income&quot;, &quot;Sex&quot;, &quot;Mailed&quot;) , states = c(&quot;medium&quot;, &quot;male&quot;, &quot;yes&quot;)) querygrain(Med_male_yes, nodes = &quot;Buy&quot;) ## $Buy ## Buy ## no yes ## 0.5853659 0.4146341 Probability of buying when mailed = 41.46%. #Not mailed Med_male_no &lt;- setEvidence(junctionTA, nodes = c(&quot;Income&quot;, &quot;Sex&quot;, &quot;Mailed&quot;) , states = c(&quot;medium&quot;, &quot;male&quot;, &quot;no&quot;)) querygrain(Med_male_no, nodes = &quot;Buy&quot;) ## $Buy ## Buy ## no yes ## 0.8043478 0.1956522 Probability of buying when not mailed = 19.56% Calculate expected lift. options(digits = 2) ELP = querygrain(Med_male_yes, nodes = &quot;Buy&quot;)$Buy[[2]] * r_s - querygrain(Med_male_no, nodes = &quot;Buy&quot;)$Buy[[2]] * r_u - c ELP ## [1] 0.86 We see that the expected lift (ELP) is positive hence, we should mail the high income males. Scenario 2 In this examplle we basically do the same, just with females instead. #Assumptions c = 0.6 #Cost r_s = 7 #Income with discount r_u = 9 #Income obtained from a sale to en unsolicited customer Med_fem_yes &lt;- setEvidence(junctionTA, nodes = c(&quot;Income&quot;, &quot;Sex&quot;, &quot;Mailed&quot;) ,states = c(&quot;medium&quot;, &quot;female&quot;, &quot;yes&quot;)) querygrain(Med_fem_yes, nodes = &quot;Buy&quot;) ## $Buy ## Buy ## no yes ## 0.28 0.72 We see the probability of buying = 72% when they are mailed. Med_fem_no &lt;- setEvidence(junctionTA, nodes = c(&quot;Income&quot;, &quot;Sex&quot;, &quot;Mailed&quot;) ,states = c(&quot;medium&quot;, &quot;female&quot;, &quot;no&quot;)) querygrain(Med_fem_no, nodes = &quot;Buy&quot;) ## $Buy ## Buy ## no yes ## 0.56 0.44 We see that the probability of buying = 44% when mailed. ELP = querygrain(Med_fem_yes, nodes = &quot;Buy&quot;)$Buy[[2]] * r_s - querygrain(Med_fem_no, nodes = &quot;Buy&quot;)$Buy[[2]] * r_u - c ELP ## [1] 0.48 We see that the ELP = 0.48, hence positive. Meaning that we should mail the medium income females as well, because we see that we earn more on these. Scenario 3 We go with the same assumptions this time. We are going to look at low income customers. Low_yes &lt;- setEvidence (junctionTA, nodes = c(&quot;Income&quot;, &quot;Mailed&quot;) ,states = c(&quot;low&quot;, &quot;yes&quot;)) querygrain( Low_yes, nodes = &quot;Buy&quot;) ## $Buy ## Buy ## no yes ## 0.4 0.6 We see that the probability of buying = 60% Low_no &lt;- setEvidence (junctionTA, nodes = c(&quot;Income&quot;, &quot;Mailed&quot;) ,states = c(&quot;low&quot;, &quot;no&quot;)) querygrain(Low_no, nodes = &quot;Buy&quot;) ## $Buy ## Buy ## no yes ## 0.53 0.47 Now we see that there is 47% probability of buying when they are mailed. ELP = querygrain(Low_yes, nodes = &quot;Buy&quot;)$Buy[[2]] * r_s - querygrain(Low_no, nodes = &quot;Buy&quot;)$Buy[[2]] * r_u - c ELP ## [1] -0.62 We see that it is negative, hence we should not email the low income people. Discussion: Using BN in this application allows to identify persuadable segments of individuals who would buy only if they are sent an ad. It avoids sending ads to those who will never buy those who always buy (thus avoid wasting the ad), and those who are turned off by the advertisement when they receive it. The network can be extended with more nodes according to the characteritics of the pop. in the dataset 4.4 Application 3: Product Recommendation using Probabilistic Models (BN) 4.4.1 Method 1: Learn the probabilistic relationships using a Bayesian network learning Now we are going to use the Hill+Climbing algorithm to estimate the model. We are going to use the Bayesian Network for Collaborative Filtering. library(bnlearn) library(Rgraphviz) data = read.csv(&quot;Data/Probabilistic Models for Customer Analytics/CollFilforR.csv&quot; ,header = T,colClasses = &quot;factor&quot;,sep = &quot;;&quot;) head(data) V1 V2 V3 V4 1 4 5 4 5 1 1 2 4 1 2 1 2 5 4 5 1 5 5 5 1 5 5 NA We see that we have information on four different items. Structural expectation-maximization function learns with mising data a Hill-Climbing structure and further predict the missing values, which will be the recommendations mystructural &lt;- structural.em(data, maximize = &quot;hc&quot;, return.all = TRUE) graphviz.plot(mystructural$dag) We see the structure. mystructural$dag # the structure ## ## Bayesian network learned from Missing Data ## ## model: ## [V1][V2][V4|V1][V3|V4] ## nodes: 4 ## arcs: 2 ## undirected arcs: 0 ## directed arcs: 2 ## average markov blanket size: 1.00 ## average neighbourhood size: 1.00 ## average branching factor: 0.50 ## ## learning algorithm: Structural EM ## score-based method: Hill-Climbing ## parameter learning method: Maximum Likelihood ## imputation method: ## Posterior Expectation (Likelihood Weighting) ## penalization coefficient: 2.3 ## tests used in the learning procedure: 34 ## optimized: TRUE In the following see the conditional probabilities. mystructural$fitted # the parameters ## ## Bayesian network parameters ## ## Parameters of node V1 (multinomial distribution) ## ## Conditional probability table: ## 1 2 3 4 5 ## 0.67 0.06 0.03 0.07 0.17 ## ## Parameters of node V2 (multinomial distribution) ## ## Conditional probability table: ## 1 2 3 4 5 ## 0.58 0.17 0.03 0.03 0.19 ## ## Parameters of node V3 (multinomial distribution) ## ## Conditional probability table: ## ## V4 ## V3 1 2 4 5 ## 1 0.600 0.556 0.000 0.045 ## 2 0.400 0.444 0.000 0.061 ## 3 0.000 0.000 0.000 0.015 ## 4 0.000 0.000 0.000 0.061 ## 5 0.000 0.000 1.000 0.818 ## ## Parameters of node V4 (multinomial distribution) ## ## Conditional probability table: ## ## V1 ## V4 1 2 3 4 5 ## 1 0.000 0.000 1.000 0.571 0.176 ## 2 0.075 0.500 0.000 0.000 0.588 ## 4 0.090 0.000 0.000 0.000 0.000 ## 5 0.836 0.500 0.000 0.429 0.235 Now we see that the method imputed the NA that we had in row 6. head(data) head(mystructural$imputed) # the predictions based on the probabilistic structure V1 V2 V3 V4 1 4 5 4 5 1 1 2 4 1 2 1 2 5 4 5 1 5 5 5 1 5 5 NA V1 V2 V3 V4 1 4 5 4 5 1 1 2 4 1 2 1 2 5 4 5 1 5 5 5 1 5 5 5 Alternatively, BN model is trained with any other algorithm using non-missing registers, and used afterwards for making inference; impute() function allow us to make inference in a new data set of customers for which we want to make recommendations. data_wNA = na.omit(data) dag = tree.bayes(data_wNA, &quot;V4&quot;) # a tree bayes graphviz.plot(dag) myBN = bn.fit(dag, data_wNA) Assuming the model was trained and tested through a cross-validation procedure (as in App 1), one can further use it for making recommendations with impute() function in a new dataset dataimputed = bnlearn::impute(myBN, data, method = &quot;parents&quot;, debug = TRUE) table(is.na(dataimputed)) ## * checking node V4 . ## &gt; found 16 missing value(s). ## * checking node V1 . ## &gt; found 7 missing value(s). ## * checking node V3 . ## &gt; found 13 missing value(s). ## * checking node V2 . ## &gt; found 10 missing value(s). ## ## FALSE ## 400 compare the recommendations made by the two methods. We make confusion matrices. table(mystructural$imputed$V1, dataimputed$V1) table(mystructural$imputed$V2, dataimputed$V2) table(mystructural$imputed$V3, dataimputed$V3) table(mystructural$imputed$V4, dataimputed$V4) ## ## 1 2 3 4 5 ## 1 67 0 0 0 0 ## 2 0 6 0 0 0 ## 3 0 0 3 0 0 ## 4 0 0 0 7 0 ## 5 0 0 0 0 17 ## ## 1 2 3 4 5 ## 1 51 2 3 1 1 ## 2 0 17 0 0 0 ## 3 0 0 3 0 0 ## 4 0 0 0 3 0 ## 5 0 0 0 0 19 ## ## 1 2 3 4 5 ## 1 19 0 0 0 0 ## 2 0 16 0 0 0 ## 3 0 0 1 0 0 ## 4 0 0 0 4 0 ## 5 0 0 0 1 59 ## ## 1 2 4 5 ## 1 10 0 0 0 ## 2 0 15 0 3 ## 4 0 0 6 0 ## 5 0 0 0 66 as seen, the recommendations made by the two methods are almost the same 4.4.2 Method 2: Finding latent segments A latent class (segment) that can accurately predict the ratings for the user on all items. The advantages of this method is that is considers both the similarities between users, items and their interaction. Hence we want to use Bayesian Networks to identify segments / or clusters. These are latent, hence we cannot directly measure them. Therefore we are going to test different number of latent segments and see which has the lowest best performance according to the log likelihood. Notice that the data consists of four different variables, these could for instance be movies. library(poLCA) # Define the variables used for (the model ~1 means without covariates) f &lt;- cbind(V1, V2, V3, V4)~1 We are going to run several models with the for() function. We are going to evaluate BIC. One also have used AIC, \\(G^2\\), \\(X^2\\). min_bic &lt;- 100000 for(i in 2:10){ #We test from 2 to 10 segments. lc &lt;- poLCA(f, data, nclass=i #We iterate through no. of classes. , maxiter=3000, tol=1e-5, na.rm=FALSE, nrep=1, verbose=TRUE, calc.se=TRUE) if(lc$bic &lt; min_bic){ min_bic &lt;- lc$bic LCA_best_model&lt;-lc } } LCA_best_model ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.95 0.052 0.000 0.00 0.00 ## class 2: 0.24 0.081 0.075 0.18 0.43 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.62 0.00 0.000 0.051 0.33 ## class 2: 0.37 0.54 0.095 0.000 0.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.00 0.00 0.000 0.079 0.921 ## class 2: 0.44 0.44 0.028 0.000 0.085 ## ## $V4 ## 1 2 4 5 ## class 1: 0.00 0.00 0.12 0.88 ## class 2: 0.31 0.46 0.00 0.23 ## ## Estimated class population shares ## 0.6 0.4 ## ## Predicted class memberships (by modal posterior prob.) ## 0.61 0.39 ## ## ========================================================= ## Fit for 2 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 31 ## residual degrees of freedom: 69 ## maximum log-likelihood: -312 ## ## AIC(2): 686 ## BIC(2): 767 ## G^2(2): 160 (Likelihood ratio/deviance statistic) ## X^2(2): 381 (Chi-square goodness of fit) ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.95 0.052 0.00 0.00 0.00 ## class 2: 0.14 0.149 0.00 0.32 0.38 ## class 3: 0.36 0.000 0.17 0.00 0.48 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.62 0.00 0.00 0.051 0.33 ## class 2: 0.26 0.56 0.19 0.000 0.00 ## class 3: 0.48 0.52 0.00 0.000 0.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0 0.0 0.00 0.079 0.92 ## class 2: 0 0.8 0.05 0.000 0.15 ## class 3: 1 0.0 0.00 0.000 0.00 ## ## $V4 ## 1 2 4 5 ## class 1: 0.00 0.00 0.12 0.88 ## class 2: 0.27 0.44 0.00 0.29 ## class 3: 0.34 0.49 0.00 0.17 ## ## Estimated class population shares ## 0.6 0.22 0.18 ## ## Predicted class memberships (by modal posterior prob.) ## 0.61 0.2 0.19 ## ## ========================================================= ## Fit for 3 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 47 ## residual degrees of freedom: 53 ## maximum log-likelihood: -300 ## ## AIC(3): 694 ## BIC(3): 816 ## G^2(3): 141 (Likelihood ratio/deviance statistic) ## X^2(3): 287 (Chi-square goodness of fit) ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.36 0.000 0.17 0.00 0.48 ## class 2: 0.95 0.054 0.00 0.00 0.00 ## class 3: 0.42 0.000 0.00 0.58 0.00 ## class 4: 0.17 0.173 0.00 0.21 0.45 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.48 0.52 0.00 0.00 0.00 ## class 2: 0.68 0.00 0.00 0.00 0.32 ## class 3: 0.00 0.37 0.00 0.37 0.27 ## class 4: 0.32 0.46 0.23 0.00 0.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 1 0.00 0.000 0.000 0.00 ## class 2: 0 0.00 0.000 0.089 0.91 ## class 3: 0 0.00 0.000 0.000 1.00 ## class 4: 0 0.94 0.059 0.000 0.00 ## ## $V4 ## 1 2 4 5 ## class 1: 0.34 0.49 0 0.17 ## class 2: 0.00 0.00 0 1.00 ## class 3: 0.00 0.00 1 0.00 ## class 4: 0.27 0.44 0 0.29 ## ## Estimated class population shares ## 0.18 0.54 0.092 0.19 ## ## Predicted class memberships (by modal posterior prob.) ## 0.19 0.55 0.09 0.17 ## ## ========================================================= ## Fit for 4 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 63 ## residual degrees of freedom: 37 ## maximum log-likelihood: -287 ## ## AIC(4): 699 ## BIC(4): 863 ## G^2(4): 129 (Likelihood ratio/deviance statistic) ## X^2(4): 259 (Chi-square goodness of fit) ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.29 0.132 0.00 0.18 0.4 ## class 2: 0.94 0.059 0.00 0.00 0.0 ## class 3: 0.42 0.000 0.00 0.58 0.0 ## class 4: 0.35 0.000 0.35 0.00 0.3 ## class 5: 0.00 0.000 0.00 0.00 1.0 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.68 0.32 0.00 0.00 0.00 ## class 2: 0.68 0.00 0.00 0.00 0.32 ## class 3: 0.00 0.37 0.00 0.37 0.27 ## class 4: 0.00 1.00 0.00 0.00 0.00 ## class 5: 0.00 0.25 0.75 0.00 0.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.39 0.61 0.00 0.000 0.00 ## class 2: 0.00 0.00 0.00 0.089 0.91 ## class 3: 0.00 0.00 0.00 0.000 1.00 ## class 4: 1.00 0.00 0.00 0.000 0.00 ## class 5: 0.00 0.81 0.19 0.000 0.00 ## ## $V4 ## 1 2 4 5 ## class 1: 0.23 0.77 0 0.00 ## class 2: 0.00 0.00 0 1.00 ## class 3: 0.00 0.00 1 0.00 ## class 4: 0.65 0.00 0 0.35 ## class 5: 0.00 0.00 0 1.00 ## ## Estimated class population shares ## 0.23 0.54 0.092 0.086 0.053 ## ## Predicted class memberships (by modal posterior prob.) ## 0.23 0.55 0.09 0.09 0.04 ## ## ========================================================= ## Fit for 5 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 79 ## residual degrees of freedom: 21 ## maximum log-likelihood: -273 ## ## AIC(5): 704 ## BIC(5): 910 ## G^2(5): 104 (Likelihood ratio/deviance statistic) ## X^2(5): 186 (Chi-square goodness of fit) ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.34 0.11 0.11 0.00 0.44 ## class 2: 1.00 0.00 0.00 0.00 0.00 ## class 3: 0.43 0.00 0.00 0.57 0.00 ## class 4: 0.00 0.00 0.00 0.46 0.54 ## class 5: 0.00 1.00 0.00 0.00 0.00 ## class 6: 1.00 0.00 0.00 0.00 0.00 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.35 0.65 0.00 0.00 0.00 ## class 2: 0.57 0.00 0.00 0.00 0.43 ## class 3: 0.00 0.36 0.00 0.36 0.27 ## class 4: 0.57 0.00 0.43 0.00 0.00 ## class 5: 0.00 0.00 0.00 0.00 1.00 ## class 6: 0.88 0.00 0.00 0.00 0.12 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.66 0.3 0.041 0 0 ## class 2: 0.00 0.0 0.000 0 1 ## class 3: 0.00 0.0 0.000 0 1 ## class 4: 0.00 1.0 0.000 0 0 ## class 5: 0.00 0.0 0.000 1 0 ## class 6: 0.00 0.0 0.000 0 1 ## ## $V4 ## 1 2 4 5 ## class 1: 0.24 0.6 0 0.16 ## class 2: 0.00 0.0 0 1.00 ## class 3: 0.00 0.0 1 0.00 ## class 4: 0.57 0.0 0 0.43 ## class 5: 0.00 0.0 0 1.00 ## class 6: 0.00 0.0 0 1.00 ## ## Estimated class population shares ## 0.28 0.23 0.092 0.087 0.049 0.27 ## ## Predicted class memberships (by modal posterior prob.) ## 0.26 0.13 0.09 0.1 0.05 0.37 ## ## ========================================================= ## Fit for 6 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 95 ## residual degrees of freedom: 5 ## maximum log-likelihood: -270 ## ## AIC(6): 730 ## BIC(6): 977 ## G^2(6): 96 (Likelihood ratio/deviance statistic) ## X^2(6): 136 (Chi-square goodness of fit) ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.42 0.42 0.0 0 0.17 ## class 2: 1.00 0.00 0.0 0 0.00 ## class 3: 0.35 0.00 0.0 0 0.65 ## class 4: 0.30 0.00 0.3 0 0.40 ## class 5: 0.30 0.00 0.0 0 0.70 ## class 6: 0.00 0.00 0.0 1 0.00 ## class 7: 0.00 1.00 0.0 0 0.00 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.00 1.00 0.00 0.00 0.00 ## class 2: 0.74 0.00 0.00 0.00 0.26 ## class 3: 1.00 0.00 0.00 0.00 0.00 ## class 4: 0.00 1.00 0.00 0.00 0.00 ## class 5: 0.00 0.00 0.36 0.36 0.28 ## class 6: 0.57 0.43 0.00 0.00 0.00 ## class 7: 0.00 0.00 0.00 0.00 1.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.0 1.00 0.0 0 0.00 ## class 2: 0.0 0.00 0.0 0 1.00 ## class 3: 1.0 0.00 0.0 0 0.00 ## class 4: 0.9 0.00 0.1 0 0.00 ## class 5: 0.0 0.46 0.0 0 0.54 ## class 6: 0.0 0.57 0.0 0 0.43 ## class 7: 0.0 0.00 0.0 1 0.00 ## ## $V4 ## 1 2 4 5 ## class 1: 0.0 1 0.00 0.00 ## class 2: 0.0 0 0.00 1.00 ## class 3: 0.0 1 0.00 0.00 ## class 4: 0.6 0 0.00 0.40 ## class 5: 0.0 0 0.65 0.35 ## class 6: 1.0 0 0.00 0.00 ## class 7: 0.0 0 0.00 1.00 ## ## Estimated class population shares ## 0.072 0.49 0.1 0.1 0.12 0.07 0.05 ## ## Predicted class memberships (by modal posterior prob.) ## 0.06 0.5 0.1 0.1 0.12 0.07 0.05 ## ## ========================================================= ## Fit for 7 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 111 ## residual degrees of freedom: -11 ## maximum log-likelihood: -252 ## ## AIC(7): 727 ## BIC(7): 1016 ## G^2(7): 56 (Likelihood ratio/deviance statistic) ## X^2(7): 71 (Chi-square goodness of fit) ## ## ALERT: number of parameters estimated ( 111 ) exceeds number of observations ( 100 ) ## ## ALERT: negative degrees of freedom; respecify model ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 1.00 0.00 0.00 0.00 0.00 ## class 2: 0.75 0.00 0.00 0.00 0.25 ## class 3: 0.00 0.00 0.22 0.51 0.27 ## class 4: 0.37 0.00 0.00 0.00 0.63 ## class 5: 0.47 0.47 0.00 0.00 0.06 ## class 6: 1.00 0.00 0.00 0.00 0.00 ## class 7: 0.33 0.00 0.00 0.00 0.67 ## class 8: 0.00 0.37 0.00 0.00 0.63 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.00 0.00 0.00 0.54 0.46 ## class 2: 0.00 1.00 0.00 0.00 0.00 ## class 3: 0.33 0.67 0.00 0.00 0.00 ## class 4: 1.00 0.00 0.00 0.00 0.00 ## class 5: 0.00 1.00 0.00 0.00 0.00 ## class 6: 0.75 0.00 0.00 0.00 0.25 ## class 7: 1.00 0.00 0.00 0.00 0.00 ## class 8: 0.00 0.00 0.43 0.00 0.57 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.00 0.00 0.00 0.00 1.00 ## class 2: 0.75 0.00 0.25 0.00 0.00 ## class 3: 0.44 0.35 0.00 0.00 0.22 ## class 4: 1.00 0.00 0.00 0.00 0.00 ## class 5: 0.00 1.00 0.00 0.00 0.00 ## class 6: 0.00 0.00 0.00 0.00 1.00 ## class 7: 1.00 0.00 0.00 0.00 0.00 ## class 8: 0.00 0.56 0.00 0.44 0.00 ## ## $V4 ## 1 2 4 5 ## class 1: 0 0 1 0 ## class 2: 0 0 0 1 ## class 3: 1 0 0 0 ## class 4: 0 1 0 0 ## class 5: 0 1 0 0 ## class 6: 0 0 0 1 ## class 7: 0 1 0 0 ## class 8: 0 0 0 1 ## ## Estimated class population shares ## 0.066 0.04 0.14 0.018 0.064 0.49 0.085 0.1 ## ## Predicted class memberships (by modal posterior prob.) ## 0.06 0.04 0.13 0.06 0.5 0.1 0.11 ## ## ========================================================= ## Fit for 8 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 127 ## residual degrees of freedom: -27 ## maximum log-likelihood: -252 ## ## AIC(8): 758 ## BIC(8): 1089 ## G^2(8): 57 (Likelihood ratio/deviance statistic) ## X^2(8): 63 (Chi-square goodness of fit) ## ## ALERT: number of parameters estimated ( 127 ) exceeds number of observations ( 100 ) ## ## ALERT: negative degrees of freedom; respecify model ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.50 0.5 0.0 0.00 0.00 ## class 2: 0.00 0.0 0.0 1.00 0.00 ## class 3: 1.00 0.0 0.0 0.00 0.00 ## class 4: 1.00 0.0 0.0 0.00 0.00 ## class 5: 0.30 0.0 0.3 0.00 0.40 ## class 6: 1.00 0.0 0.0 0.00 0.00 ## class 7: 0.43 0.0 0.0 0.57 0.00 ## class 8: 0.21 0.0 0.0 0.00 0.79 ## class 9: 0.00 1.0 0.0 0.00 0.00 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.00 1.00 0.00 0.00 0.00 ## class 2: 1.00 0.00 0.00 0.00 0.00 ## class 3: 0.81 0.00 0.00 0.00 0.19 ## class 4: 0.39 0.00 0.00 0.00 0.61 ## class 5: 0.00 1.00 0.00 0.00 0.00 ## class 6: 0.61 0.00 0.00 0.00 0.39 ## class 7: 0.00 0.36 0.00 0.36 0.27 ## class 8: 0.71 0.00 0.29 0.00 0.00 ## class 9: 0.00 0.00 0.00 0.00 1.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.00 1.00 0.0 0 0 ## class 2: 0.00 1.00 0.0 0 0 ## class 3: 0.00 0.00 0.0 0 1 ## class 4: 0.00 0.00 0.0 0 1 ## class 5: 0.90 0.00 0.1 0 0 ## class 6: 0.00 0.00 0.0 0 1 ## class 7: 0.00 0.00 0.0 0 1 ## class 8: 0.54 0.46 0.0 0 0 ## class 9: 0.00 0.00 0.0 1 0 ## ## $V4 ## 1 2 4 5 ## class 1: 0.0 1.00 0 0.00 ## class 2: 1.0 0.00 0 0.00 ## class 3: 0.0 0.00 0 1.00 ## class 4: 0.0 0.00 0 1.00 ## class 5: 0.6 0.00 0 0.40 ## class 6: 0.0 0.00 0 1.00 ## class 7: 0.0 0.00 1 0.00 ## class 8: 0.0 0.77 0 0.23 ## class 9: 0.0 0.00 0 1.00 ## ## Estimated class population shares ## 0.06 0.04 0.39 0.058 0.1 0.047 0.092 0.16 0.05 ## ## Predicted class memberships (by modal posterior prob.) ## 0.06 0.04 0.5 0.1 0.09 0.16 0.05 ## ## ========================================================= ## Fit for 9 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 143 ## residual degrees of freedom: -43 ## maximum log-likelihood: -248 ## ## AIC(9): 783 ## BIC(9): 1155 ## G^2(9): 59 (Likelihood ratio/deviance statistic) ## X^2(9): 74 (Chi-square goodness of fit) ## ## ALERT: number of parameters estimated ( 143 ) exceeds number of observations ( 100 ) ## ## ALERT: negative degrees of freedom; respecify model ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 1.00 0.00 0.0 0 0.00 ## class 2: 1.00 0.00 0.0 0 0.00 ## class 3: 0.22 0.00 0.2 0 0.57 ## class 4: 1.00 0.00 0.0 0 0.00 ## class 5: 0.00 0.00 0.0 1 0.00 ## class 6: 0.00 0.00 0.0 1 0.00 ## class 7: 0.20 0.22 0.0 0 0.58 ## class 8: 0.00 1.00 0.0 0 0.00 ## class 9: 1.00 0.00 0.0 0 0.00 ## class 10: 1.00 0.00 0.0 0 0.00 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.00 1.00 0.00 0.00 0.00 ## class 2: 0.83 0.00 0.00 0.00 0.17 ## class 3: 0.59 0.41 0.00 0.00 0.00 ## class 4: 0.00 0.00 0.00 0.54 0.46 ## class 5: 0.00 1.00 0.00 0.00 0.00 ## class 6: 1.00 0.00 0.00 0.00 0.00 ## class 7: 0.00 0.67 0.33 0.00 0.00 ## class 8: 0.00 0.00 0.00 0.00 1.00 ## class 9: 0.66 0.00 0.00 0.00 0.34 ## class 10: 0.00 1.00 0.00 0.00 0.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 1 0.00 0.000 0 0 ## class 2: 0 0.00 0.000 0 1 ## class 3: 1 0.00 0.000 0 0 ## class 4: 0 0.00 0.000 0 1 ## class 5: 0 0.00 0.000 0 1 ## class 6: 0 1.00 0.000 0 0 ## class 7: 0 0.92 0.077 0 0 ## class 8: 0 0.00 0.000 1 0 ## class 9: 0 0.00 0.000 0 1 ## class 10: 1 0.00 0.000 0 0 ## ## $V4 ## 1 2 4 5 ## class 1: 0.00 0.00 0 1.00 ## class 2: 0.00 0.00 0 1.00 ## class 3: 0.41 0.59 0 0.00 ## class 4: 0.00 0.00 1 0.00 ## class 5: 0.00 0.00 1 0.00 ## class 6: 1.00 0.00 0 0.00 ## class 7: 0.00 0.61 0 0.39 ## class 8: 0.00 0.00 0 1.00 ## class 9: 0.00 0.00 0 1.00 ## class 10: 0.00 0.00 0 1.00 ## ## Estimated class population shares ## 0.02 0.25 0.15 0.062 0.034 0.04 0.15 0.048 0.24 0.0098 ## ## Predicted class memberships (by modal posterior prob.) ## 0.03 0.37 0.13 0.06 0.03 0.04 0.16 0.05 0.13 ## ## ========================================================= ## Fit for 10 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 159 ## residual degrees of freedom: -59 ## maximum log-likelihood: -247 ## ## AIC(10): 812 ## BIC(10): 1226 ## G^2(10): 60 (Likelihood ratio/deviance statistic) ## X^2(10): 60 (Chi-square goodness of fit) ## ## ALERT: number of parameters estimated ( 159 ) exceeds number of observations ( 100 ) ## ## ALERT: negative degrees of freedom; respecify model ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.95 0.052 0.000 0.00 0.00 ## class 2: 0.24 0.081 0.075 0.18 0.43 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.62 0.00 0.000 0.051 0.33 ## class 2: 0.37 0.54 0.095 0.000 0.00 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.00 0.00 0.000 0.079 0.921 ## class 2: 0.44 0.44 0.028 0.000 0.085 ## ## $V4 ## 1 2 4 5 ## class 1: 0.00 0.00 0.12 0.88 ## class 2: 0.31 0.46 0.00 0.23 ## ## Estimated class population shares ## 0.6 0.4 ## ## Predicted class memberships (by modal posterior prob.) ## 0.61 0.39 ## ## ========================================================= ## Fit for 2 latent classes: ## ========================================================= ## number of observations: 100 ## number of fully observed cases: 68 ## number of estimated parameters: 31 ## residual degrees of freedom: 69 ## maximum log-likelihood: -312 ## ## AIC(2): 686 ## BIC(2): 767 ## G^2(2): 160 (Likelihood ratio/deviance statistic) ## X^2(2): 381 (Chi-square goodness of fit) ## We see that the best model is with 2 segments. For the best model we can look at the posterior probabilities. head(LCA_best_model$posterior) # matrix of posterior class membership probabilities ## [,1] ## [1,] 1.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000 ## [2,] 0.0000000000000000000000000000000000000000000000000000000000000000001719899663792218487 ## [3,] 0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000061 ## [4,] 1.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000 ## [5,] 0.9999999997285484720777049005846492946147918701171875000000000000000000000000000000000 ## [6,] 0.9999999989360808250182799383765086531639099121093750000000000000000000000000000000000 ## [,2] ## [1,] 0.000000000000000000000000000000000000058 ## [2,] 1.000000000000000000000000000000000000000 ## [3,] 1.000000000000000000000000000000000000000 ## [4,] 0.000000000000000000000000000655947965281 ## [5,] 0.000000000271451603944604850642915891786 ## [6,] 0.000000001063919249976284888521404189737 Instead of looking at the probabilities we can also look at the actual class predictions. LCA_best_model$predclass %&gt;% head() # class membership table(LCA_best_model$predclass) LCA_best_model$P # size of each class ## [1] 1 2 2 1 1 1 ## ## 1 2 ## 61 39 ## [1] 0.6 0.4 We see the different predictions, and the table shows the distribution. Lastly we have the proportions for each class. options(scipen = 0) # The estimated class-conditional response probabilities LCA_best_model$probs # Ploting (graph=TRUE) them for better interpretation ch2 &lt;- poLCA(f, data, nclass = 2, graph = TRUE) ## $V1 ## 1 2 3 4 5 ## class 1: 0.95 0.053 7.9e-140 1.7e-37 1.1e-48 ## class 2: 0.24 0.080 7.5e-02 1.8e-01 4.3e-01 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.62 1.6e-37 3.2e-94 5.1e-02 3.3e-01 ## class 2: 0.37 5.4e-01 9.5e-02 1.1e-38 4.6e-10 ## ## $V3 ## 1 2 3 4 5 ## class 1: 6.4e-39 3.2e-42 2.9e-88 7.9e-02 0.921 ## class 2: 4.4e-01 4.4e-01 2.8e-02 6.0e-29 0.085 ## ## $V4 ## 1 2 4 5 ## class 1: 4.7e-85 4.5e-43 1.2e-01 0.88 ## class 2: 3.1e-01 4.6e-01 3.4e-12 0.23 ## ## Conditional item response (column) probabilities, ## by outcome variable, for each class (row) ## ## $V1 ## 1 2 3 4 5 ## class 1: 0.95 0.049 0.000 0.00 0.00 ## class 2: 0.30 0.074 0.074 0.15 0.41 ## ## $V2 ## 1 2 3 4 5 ## class 1: 0.76 0.00 0.00 0.049 0.2 ## class 2: 0.41 0.48 0.11 0.000 0.0 ## ## $V3 ## 1 2 3 4 5 ## class 1: 0.00 0.00 0.000 0.049 0.95 ## class 2: 0.56 0.41 0.037 0.000 0.00 ## ## $V4 ## 1 2 4 5 ## class 1: 0.00 0.00 0.049 0.95 ## class 2: 0.33 0.41 0.000 0.26 ## ## Estimated class population shares ## 0.6 0.4 ## ## Predicted class memberships (by modal posterior prob.) ## 0.6 0.4 ## ## ========================================================= ## Fit for 2 latent classes: ## ========================================================= ## number of observations: 68 ## number of estimated parameters: 31 ## residual degrees of freedom: 37 ## maximum log-likelihood: -212 ## ## AIC(2): 487 ## BIC(2): 555 ## G^2(2): 148 (Likelihood ratio/deviance statistic) ## X^2(2): 508 (Chi-square goodness of fit) ## We see the values for each variable, e.g., in class 1 V2, we primarily have 1’s and 2’s. Where the class 2 vector 2 has only values 1 and 5. # save the class data_copy &lt;- data data_copy$class &lt;- factor(LCA_best_model$predclass) head(data_copy) V1 V2 V3 V4 class 1 4 5 4 1 5 1 1 2 2 4 1 2 1 2 2 5 4 5 1 1 5 5 5 1 1 5 5 NA 1 Now we see that the class that we estimate the observation to be within is added. # learn a BN classifier considering the class as the root node mystructural &lt;- structural.em(data_copy, maximize = &quot;hc&quot;, return.all = TRUE) graphviz.plot(mystructural$dag) We see that the hill climbing method actually sets V3 as an ancestor and not class. This we want to correct. mystructural$dag &lt;- set.arc(mystructural$dag, from = &quot;class&quot;, to = &quot;V3&quot;) graphviz.plot (mystructural$dag) bn.mle &lt;- bn.fit (mystructural$dag, data = data_copy, method = &quot;mle&quot;) Now we have corrected the DAG that is not correct. Hence, we are able to start making inference. Lets say we have a customer who has reported the following: V1 = 1, V2=5, V3=5. Then what would the rating for V4 be? library(gRain) junction &lt;- compile(as.grain(bn.mle)) #We set evidance V1V2V3 &lt;- setEvidence(junction, nodes = c(&quot;V1&quot;, &quot;V2&quot;, &quot;V3&quot;) ,states = c(&quot;1&quot;, &quot;5&quot;, &quot;5&quot;)) querygrain(V1V2V3, nodes = &quot;class&quot;) ## $class ## class ## 1 2 ## 1 0 It is most likely that this person is in class 1. querygrain(V1V2V3, nodes = &quot;V4&quot;) #estimated preference: 0.12*4 + 0.88*5 = 4.88 ## $V4 ## V4 ## 1 2 4 5 ## 0.00 0.00 0.12 0.88 We see that there is 12% probability of rating 4, and 88% probability of rating 5, hence we can aggregate on those. \\(0.12 * 4 + 0.88 * 5 = 4.88\\) 4.5 Latent models for cross selling and acquisition sequence In this section I will introduce two methods. Latent trait model - which is for corss selling, so leans towards the recommender systems as we have previously seen. Latent Markov approach (the hidden Markov chain) - for time-series (dynamic approach). Compared to association rule models, Bayesian Networks collaborative filtering etc. we see that the methods introduced in this chapter is more about the sequence of events and analyzing these. The new method will look at what the customer has bought up until now, or a given date and make recommendations based on these. What data type?, we will see that the two approaches need data in products as column dummies and users as rows, e.g., Notice that the LMM model needs one row pr. user pr. timeperiod. Lets recap, what is a latent trait? This is an udnerlying factor that is unknown hence latent. Therefore, we are going to esimate these. Latent train = underlying characteristics. This is something that cannot be directly measured, that meaning be answered by one question (e.g., as how old are you?), but must be answered through a number of questions. Example of a latent trait. Let’s say that we want to measure intelligence. The procedure would be to ask several questions. So this is basically like an exam, the more correct answers you have, the higher the intelligence. Then one may say, can we assume if a student can answer the most difficult question, will he then be able to answer all the other less difficult questions? If yes, then we in principle only need to know which question was the hardest that he correctly answered, and then we basically just need to look at this item to make recommendations, that could be grading in this example. If not, then we need to know all the other answers as well. Different models needs different solutions. He shows this: Different models for different scenarios Observed variables = manifest variables We see that the latent trait models, which we are going to work with, takes categorical input but will give an continous output. (or more correct, we assume that the underlying scale is continuous). This is also known as Item Response Theory (IRT) He then introduce the item response function (IRF), it looks like the following \\[P(Y = 1 | O_j,a_i,b_i) = \\frac{e^{a_i(O_j-b_i}} {1+e^{a_i(O_j-b_i}}\\] Where \\(a_i\\) = an items location is defined as the amount of latetn trait needed to have a 50% probability of endorsing the item. \\(b\\) = the higher on the trait level a respondent needs to be in order to endorse the item. The Item Response Function Endorsing an item = the probability of buying the product. In the following plot we see two different persons, where the their latent scores are estimated to be different, although they share the same probability of buying the product, hence such curves could look like this: IRF examples We see that they have different latent scores. In a later example we are going to look at bank customers. Where we base the latent score on what financial services that a given customer has. Hence the latent score = financial maturity, this is visualized with the following: IRF example financial services Here we see the relationships between getting the financial services and the expected financial maturity, for instance to have probability of 50% of having a check account, the maturity is low, although to have a loan, you expect the persons to be much much more mature. We see that -1 maturity = 50% probability of having a loan, but that also implies that almost certainty of a person having a check account. Naturally that means that one could also make a list of these models, and we want to know: \\(a_i\\) = the slope parameter, the slope of the sigmoid at 50% \\(b_i\\) = Position parameter, this is the latent score (financial maturity in the example above) at 50% buying probability. See an example below. a_i and b_i scores for each financial service Notice that financial maturity is merely an expression for how far up the ladder (what products a customer have) a given person is and then we can set the B value to plot into the function to assess what products you expect the customer to have. Hence we should be able to calculate the latent score for a given customer based on a set of predictor variables, an example in the following: Example of predictor varaibles + significance and influence on latent score We see that the higher th T-ratio, the higher significance. Then we can also look at the B, e.g., if you are renting, it is a negative effect on financial maturity. Why is this important? We need to know this, to make appropriate recommendations, hence a person with a low latent score (financial maturity) may not get a mortgage loan although he may get a check account. 4.5.1 Latent Trait Model - Applying the LTM Package in R The LSAT data is having 5 questions of increasing difficulty (although not in the actual order) (5 items) and we are going to make an analysis on this. library(ltm) head(LSAT) #We see that itmes are one hot encoded Item 1 Item 2 Item 3 Item 4 Item 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 #Some summary descriptions des &lt;- descript(LSAT) des ## ## Descriptive statistics for the &#39;LSAT&#39; data-set ## ## Sample: ## 5 items and 1000 sample units; 0 missing values ## ## Proportions for each level of response: ## 0 1 logit ## Item 1 0.076 0.92 2.50 ## Item 2 0.291 0.71 0.89 ## Item 3 0.447 0.55 0.21 ## Item 4 0.237 0.76 1.17 ## Item 5 0.130 0.87 1.90 ## ## ## Frequencies of total scores: ## 0 1 2 3 4 5 ## Freq 3 20 85 237 357 298 ## ## ## Point Biserial correlation with Total Score: ## Included Excluded ## Item 1 0.36 0.11 ## Item 2 0.57 0.15 ## Item 3 0.62 0.17 ## Item 4 0.53 0.14 ## Item 5 0.44 0.12 ## ## ## Cronbach&#39;s alpha: ## value ## All Items 0.29 ## Excluding Item 1 0.28 ## Excluding Item 2 0.24 ## Excluding Item 3 0.22 ## Excluding Item 4 0.25 ## Excluding Item 5 0.27 ## ## ## Pairwise Associations: ## Item i Item j p.value ## 1 1 5 0.565 ## 2 1 4 0.208 ## 3 3 5 0.113 ## 4 2 4 0.059 ## 5 1 2 0.028 ## 6 2 5 0.009 ## 7 1 3 0.003 ## 8 4 5 0.002 ## 9 3 4 7e-04 ## 10 2 3 4e-04 We can see the frequency of the different scores etc. The important thing with the descriptive statistics In the proportions for each level of response: We see that item 1 is the simplest and item 3 is the most difficult. That we will be clear from the res output. 0 = proportion of wrong answers, 1 = proportion of correct answers. The other important thing is the pairwise associations: where we see that if there is correlation between the different questions. High p = correlation, low correlation = low p-value. We want to find some high p-values, of not, the model does not make any sense. Now lets fit the model. fit &lt;- ltm(LSAT ~ z1) #z1 for the birnbaum model fit ## ## Call: ## ltm(formula = LSAT ~ z1) ## ## Coefficients: ## Dffclt Dscrmn ## Item 1 -3.36 0.82 ## Item 2 -1.37 0.72 ## Item 3 -0.28 0.89 ## Item 4 -1.87 0.69 ## Item 5 -3.12 0.66 ## ## Log.Lik: -2467 We see: Coefficients: that is where they are placed on the scale, e.g., item 3 (Q3) leads to the highest maturity. Dffclt = location parameters Dscrmn = the slope We can look at the different scores depending on the correct answers, that is for each of the persons. res &lt;- factor.scores(fit) res #Fore respondents ## ## Call: ## ltm(formula = LSAT ~ z1) ## ## Scoring Method: Empirical Bayes ## ## Factor-Scores for observed response patterns: ## Item 1 Item 2 Item 3 Item 4 Item 5 Obs Exp z1 se.z1 ## 1 0 0 0 0 0 3 2.3 -1.895 0.80 ## 2 0 0 0 0 1 6 5.9 -1.479 0.80 ## 3 0 0 0 1 0 2 2.6 -1.460 0.80 ## 4 0 0 0 1 1 11 8.9 -1.041 0.80 ## 5 0 0 1 0 0 1 0.7 -1.331 0.80 ## 6 0 0 1 0 1 1 2.6 -0.911 0.80 ## 7 0 0 1 1 0 3 1.2 -0.891 0.80 ## 8 0 0 1 1 1 4 6.0 -0.463 0.81 ## 9 0 1 0 0 0 1 1.8 -1.438 0.80 ## 10 0 1 0 0 1 8 6.4 -1.019 0.80 ## 11 0 1 0 1 1 16 13.6 -0.573 0.81 ## 12 0 1 1 0 1 3 4.4 -0.441 0.81 ## 13 0 1 1 1 0 2 2.0 -0.420 0.81 ## 14 0 1 1 1 1 15 13.9 0.023 0.83 ## 15 1 0 0 0 0 10 9.5 -1.373 0.80 ## 16 1 0 0 0 1 29 34.6 -0.953 0.80 ## 17 1 0 0 1 0 14 15.6 -0.933 0.80 ## 18 1 0 0 1 1 81 76.6 -0.506 0.81 ## 19 1 0 1 0 0 3 4.7 -0.803 0.80 ## 20 1 0 1 0 1 28 25.0 -0.373 0.81 ## 21 1 0 1 1 0 15 11.5 -0.352 0.81 ## 22 1 0 1 1 1 80 83.5 0.093 0.83 ## 23 1 1 0 0 0 16 11.3 -0.911 0.80 ## 24 1 1 0 0 1 56 56.1 -0.483 0.81 ## 25 1 1 0 1 0 21 25.6 -0.463 0.81 ## 26 1 1 0 1 1 173 173.3 -0.022 0.83 ## 27 1 1 1 0 0 11 8.4 -0.329 0.82 ## 28 1 1 1 0 1 61 62.5 0.117 0.83 ## 29 1 1 1 1 0 28 29.1 0.139 0.83 ## 30 1 1 1 1 1 298 296.7 0.606 0.85 We see that Z1 is where a person is set on a the latent scale. where person no. 30 (or this group of persons as there are 298) has a latent score of 0.606. He is also plotted with the red line in the following plot. Now we can also plot the item response functions. I have added ablines in the plot, to visualize the latent scores of 50% probability of buying. plot(fit) #Lets add plot the coefficient and check if it adds up to 50% prob. abline(h = 0.5,lty = 2,col = &quot;darkgrey&quot;) abline(v = -0.280,lty = 2,col = &quot;darkgrey&quot;) text(x = -0.0280,y = 0.024,&quot;-0.0280 (coef)&quot;,col = &quot;green&quot;,cex = 0.7) text(x = 1.9,y = 0.51,&quot;50% prob.&quot;,col = &quot;green&quot;,cex = 0.7) abline(v = 0.606,lty = 2,col = &quot;darkred&quot;) #Group 30 from the res text(x = 1.0,y = 0.26,&quot;Person 30&quot;,col = &quot;darkred&quot;,cex = 0.7) We see that at the visualize maturity, there is almost certinaty that the person is correctly answering item 1 (Q1). 4.5.2 The latent (hidden) Markov model Let me first summarize the Markov Chain The Markov chain model looks as the following: The Markov Chain model This model has some transition probabilities assigned. Transition Probabilities We see that state 1 is the most recent purchasers and then state 5 is the lost customers. Hence we see one can move from e.g., state 4 to 1. The Markov Chain is that there is a probability of moving upwards in the states (recency states) or they can move down in states. In the end there is a catch all sate, where we assume that you will stay the rest of the time, namely where you dont buy again. But what is the Hidden Markov Chain? (HMC) It is basically what it says, it is hidden. But what does the states then reflect? In the hidden markov chain the states reflect different buying patterns, hence you will try to estimate if a given person has changed his buying patterns and what that means for him. The difference to the Latent Trait Model (LTM): It is that previously we just had one row pr. person (or group with identical person) we are now having one row pr. customer pr. transaction. To explain this, he applied a case with COOP, the data looked the following: COOP data Characteristics of the study (hence also the HMC) Assume that households may be in different states with different purchasing patterns, which is in general also the assumption for the HMC. Across time a household is allowed to switch form one state to another, which is also the assumption for HMC. Then the Markovian assumption, meaning that the latent state membership for a household in a given time period is assumed to only depend on the state membership in the previous time period. Why take a look at the chain above, and see that this is the only thing we need to know, to address which outcomes there may be. The procedure: They calculated the probability of a households buying products within given categories, see the following: Proportions of buying certain items: The example is one household, where each row is the timeperiod and each column is a product category. We see that for instance there is a tendency to buy more vegetables in the periods. We can also show the probability of buying an item in the different states, see the following: State probabilities of buying items Hence we see that in the overall probability of buying something decrease as the state lowers. The same goes for each group. Now we can also look at the transition probabilities. We see: Big values omn the diagonal, indicating that there is a great probability of a person staying in the same state. We see that the probability of moving from state 7 to state 6 is 18.6%, on the other hand if you are in state 6, there is a probability of going to state 7. In general, we see a very low probability of reaching state 7 (those buying the least). Transition probabilities Then the question is, what are we actually adopting, when moving from one state to another. Products that are adopted when moving from one state to another, e.g., when moving from state 7 (all item groups being low) we see that people start adding dairy products and so on. Hence we see that adding dairy indicating that a customer on an upwards journey. Then we can see that if a person starts adding vegetables he will end in state 5 and if he adds eggs and vegetables, he will end up ion state 4. What do we get out of this? In the example he showed one of the COOP private label milk, where they originally marketed a beer on one side of the beers, where they then added ecological eggs, as we see that this will manipulate the customer upwards in the states. This was a great results. Hence again, it about knowing enough about the customer to know what to recommend them, and not just market some random product. "],["customer-behavior.html", "5 Customer behavior 5.1 Characteristics of cliskstream 5.2 Applications with clickstream data 5.3 AB Testing", " 5 Customer behavior 5.1 Characteristics of cliskstream We see that this can be organized in a pyramid. Characteristics of clickstream data We see that the more information we have about the actual user the more valuable is the clickstream data. We saw an example with clickstreams from the google merchendise, here we information on the cookie, timestamps, eventID, sessionID, destination (the actual page they are visiting etc.). Naturally this must be organized so it can be used for analysis. It the slidedeck there is a bunch of steps for preparing data, although I would argue that they are very case sensitive. Challenges with clickstreams: One must choose the right period for evaluation, as we want to have as many full cycles as possible, hence if we start the survey at the 1 of january, then customers starting to visit the page in december will appear as new customers, while they are not. The same applies at the end of the period. Customer use different devices for the same purposes, these are difficult to track. Cookies expires after a certain time period. If a user is having different windows open, it is difficult to know which page the user is actually on. Many patterns in the data is due to how the website is constructed. Where do customers come from?: one could investigate the following: What type of device is used, PC, tableg, mobile etc. What product category is visited. What is the entry channel? advertising, direct access, social media etc. Time of entry: At what time the person visit the page? How does the person behave on the webiste?: it is important to know which panels the customer choose, where they go from a product page, from the front page, from the basket etc. Conversion rates: We need to know the conversion rates of the customers. 5.2 Applications with clickstream data For modeling sequences of clicks: neural networks (NN) and Markov chains are two of the most common type of models. NN are black-box models best for prediction (not covered here). Markov models are transparent models that are used both for predicting and understanding customer behavior. In this application, I focus on the “clickstream” library to model the data with Markov chains models. From time to time I refer to other R packages that offer similar functionality. What is a (discrete state) Markov Chain? (note: we can have Markov chains with discrete and continous states and in discrete and in continuous time). A Markov chain is a stochastic process X that takes state m from a ﬁnite set of states M at each time n. If the state in n only depends on the recent k states, we call X a Markov chain of order k. For example: 0-order Markov chain = the probability to be in any of the m states in the next step is independent of the present state. Hence states are completely independent of each other. 1st-order Markov chain = the probability to be in any of the m states in the next step is independent of the previous states given the present state (one-period memory) 2nd-order Markov chain = two-periods memory 3rd-order Markov chain = three-periods memory …so on Markov chains can be described by transition probability matrices =&gt; Each value in these matrices is a parameter Higher-order Markov chains have \\((m−1)m^k\\) model parameters =&gt; the number of lag parameters increases exponentially Markov Chain It is about sequences of events and we want to use markov chain to analyse the sequence of events, i.e. the clickstream. Hence what we want to get out of it, is the likelyhood of converting. Recall in a Bayesian Network, that variables would only have influence in one direction and hence not create loops etc. So we see that for instance a sequence of events, you would only assess the adjacent click events. With markov chain, we are able to make more relationships e.g., non adjacent events having influence of each other. Recall that HJ showed a hidden markov chain where this is not a hidden markov chain. Why Markov Chain and not a Bayesian Network? We see that a person can be jumping among a set of pages, hence we can create cycles (where one person can start and end at the same page), this is not allowed in a bayesian network, hence we cannot apply this. How we select the Markov Chain order: Usually, a user considering a Product Page might either Add the product to the shopping cart, view Product Reviews, follow a Product Recommendation, or Search for another product. Moe (2003) proposes that the probability for a transition to either of the possible next states depends on the MODE (browsing, searching, or buying) the user is currently in. This MODE (latent state) can be identiﬁed when considering the recent k states. Model order selection is usually based on a criteria like AIC or BIC. The function fitMarkovChain() estimates the parameters of a Markov chain model of order k. How does R interpret the clickstreams? Clickstreams = collection of data sequences - with different sizes! Notice that session = an individual, i.e., their user session. P = page An example: Session 1: P1 P2 P1 P3 P4 Defer Session 2: P3 P4 P1 P3 Defer Session 3: P5 P1 P6 P7 P6 P7 P8 P7 Buy Session 4: P9 P2 P11 P12 P11 P13 P11 Buy Session 5: P4 P6 P11 P6 P1 P3 Defer Session 6: P3 P13 P12 P4 P12 P1 P4 P1 P3 Defer - this is examplified in the DAG Session 7: P10 P5 P10 P8 P8 P5 P1 P7 Buy Session 8: P9 P2 P1 P9 P3 P1 Defer Session 9: P5 P8 P5 P7 P4 P1 P6 P4 Defer There are 13 possible product pages and two absorbing stages, that are stages you can end up in. Hence we see that a clickstream is a sequence of events. This can naturally be shown as a vector in R and we can apply the function readClickstreams(), the input must be a commaseperated file. The following is an example where we start in P3, then go to p13, P12, P4, P12, P1, P4, P1 back to P3 and then leave. The graph may be a biiit difficult to follow, but following the path (directions) one will end up in defer. Notice that we did not do this during the class, although I did it for fun. library(ggdag) #Generating the relationships dag &lt;- dagitty::dagitty(&quot;dag { P3 -&gt; P13 -&gt; P12 -&gt; P4 -&gt; P12 -&gt; P1 -&gt; P4 -&gt; P1 -&gt; P3 -&gt; Defer }&quot; ) tidy_dag &lt;- tidy_dagitty(dag) #Adding information on the color starting_page &lt;- &quot;P3&quot; tidy_dag$data &lt;- dplyr::mutate(tidy_dag$data, colour = ifelse(tidy_dag$data$name == starting_page ,&quot;Starting page&quot;, &quot;Visited page&quot;)) #Making the plot tidy_dag %&gt;% ggplot(aes( x = x, y = y, xend = xend, yend = yend )) + geom_dag_point(aes(colour = colour)) + geom_dag_edges() + geom_dag_text() + theme_dag() 5.2.1 A motivating example library(clickstream) cls &lt;- readClickstreams(file = &quot;Data/Customer behavior/sample.csv&quot; , sep = &quot;,&quot; , header = TRUE) cls #Shows sessions summary(cls) #Summary stats # writeClickstreams(cls, &quot;sample.csv&quot;, header = TRUE, sep = &quot;,&quot;) We see the frequencies of each page, where it is clear that P1 is most visited, hence might also be the front page. Now we can fit the model and s options(scipen = 999,digits = 2) order &lt;- 2 mc &lt;- fitMarkovChain(clickstreamList = cls,order = order #= two-periods memory, i.e. 2 lags ,control = list(optimizer = &quot;quadratic&quot;)) mc The otucome: the two transition probabilities matrices for the two lags start probabilities for the states the corresponding clickstreams started end probabilities for the states the corresponding clickstreams ended We see that with lag = 1, ther is only conversion (buying probability) from P11 and P7, although looking one further step back, we see more pages may also lead to a purchase. I guess P11 and P7 have something to do with a basket or a checkout. Then we can also see the different probabilities of moving from one page to another. Lastly we can see the different start probabilities, that is the probability of a person starting on a given page. Also we can see the end probabilities, that being posterior probabilities of buying or not. Lets now compute the summary of the fitted model and plot the transition probabilities. summary(mc) plot(mc, order = order) We see that fitMarkovChain() computes the log-likelihood, hence we can calculate AIC and BIC. Here we see the flow of typical flow 5.2.1.1 i) Predict either the next click or the ﬁnal click (state) of a customer. Now we can use it to make a prediction. If a customer starts with the clickstream P9 P2, what will do next? #Pattern to evaluate pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;P9&quot;, &quot;P2&quot;)) #using predict resultPattern &lt;- predict(mc , startPattern = pattern , dist = 1 #How far we want to predict, when he has clicked p9 and p2 ) resultPattern #the user will most likely click on P1 next There is a 66% chance of clicking P1. Now we make another prediction where a person has viewed page 9 and 2, with the following: Predict 2 steps into the future. Here we want the likelyhood of purchase pattern &lt;- new(&quot;Pattern&quot; ,sequence = c(&quot;P9&quot;, &quot;P2&quot;) #A sequence for the session ,absorbingProbabilities = data.frame(Buy = 0.333, Defer = 0.667) #provide probs for buying ) resultPattern &lt;- predict(mc, startPattern = pattern, dist = 2) resultPattern The person visit products P1 and then P3, although this pattern only has a 26% probability. Purchasing probability is 5.83% after 2 further clicks, and the person is most likely to defer the purchase. Online stores often have evidence on how many of the visitors convert to a buyer (this info is typically used to formulate initial absorbing probabilities for all users). But, particularly for customers who log on to their account, the online stores can also know how many times a particular user has been only visiting the online store and how often she has bought a product, This information can be used to formulate initial absorbing probabilities for a user. If for example a user has been logged in and ﬁnally bought a product in 50% of her log-ins, we can compute absorbing probabilities (posterior) for a stream of clicks: Now imagine that the probability of buying and defering is 50/50. absorbingProbabilities &lt;- c(0.5, 0.5) sequence &lt;- c(&quot;P9&quot;, &quot;P2&quot;) for (s in sequence) { absorbingProbabilities &lt;- absorbingProbabilities * data.matrix(subset(mc@absorbingProbabilities, state == s, select = c(&quot;Buy&quot;, &quot;Defer&quot;))) } absorbingProbabilities &lt;- absorbingProbabilities /sum(absorbingProbabilities) absorbingProbabilities 22.62% to ﬁnally buy a product after she has visited products P9 and P2. Although we see that it is cumbersome to just hardcode the patterns and the absorbing rates, because the customer journey is not homogenious for all customers. Therefore, we can define cluster and generalize on the clusters instead of on all customers. That is done in the following section 5.2.1.2 ii) Clustering and generalizing upon clickstreams An alternative before running the model is to identify segments of customers by clustering clickstreams and afterwards building a model for each cluster (Huang, Ng, Ching, Ng, and Cheung, 2001, k-means alg. and Euclidean distance) set.seed(12345) clusters &lt;- clusterClickstreams(clickstreamList = cls, order = 1,centers = 3) clusters We see that this creates three clusters. Notice that this is a list element, hence we could also look them up individually. Now we can fit a Markov Chain each of the clusters mc_clu1 &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[1]], order = 2,control = list(optimizer = &quot;quadratic&quot;)) mc_clu1 mc_clu2 &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[2]], order = 2,control = list(optimizer = &quot;quadratic&quot;)) #mc_clu2 mc_clu3 &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[3]], order = 2,control = list(optimizer = &quot;quadratic&quot;)) #mc_clu3 # or write these objects to ﬁle with writeClickstreams(). We see that this construct the same output as we have previously seen, when just analyzing one session. 5.2.2 Second exercise (clickstream) more extensive Full example with simulated data: clickstreams for 100,000 user sessions clicks are either 1 of 7 products or on one of the two ﬁnal states “Buy” and “Defer”. set.seed(123) cls &lt;- randomClickstreams(states = c(&quot;P1&quot;, &quot;P2&quot;, &quot;P3&quot;, &quot;P4&quot;, &quot;P5&quot;, &quot;P6&quot;, &quot;P7&quot;, &quot;Defer&quot;, &quot;Buy&quot;), startProbabilities = c(0.2, 0.25, 0.1, 0.15, 0.1, 0.1, 0.1, 0, 0), transitionMatrix = matrix(c(0.01, 0.09, 0.05, 0.21, 0.12, 0.17, 0.11, 0.2, 0.04, 0.1, 0, 0.29, 0.06, 0.11, 0.13, 0.21, 0.1, 0, 0.07, 0.16, 0.03, 0.25, 0.23, 0.08, 0.03, 0.12, 0.03, 0.16, 0.14, 0.07, 0, 0.05, 0.22, 0.19, 0.1, 0.07, 0.24, 0.27, 0.17, 0.13, 0, 0.03, 0.09, 0.06, 0.01, 0.11, 0.18, 0.04, 0.15, 0.26, 0, 0.1, 0.11, 0.05, 0.21, 0.07, 0.08, 0.2, 0.14, 0.18, 0.02, 0.08, 0.02, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), nrow = 9), meanLength = 50, n = 100000) summary(cls) Here we want to select the right orders. Here we test all the models in order 1 to 5 and then we evaluate BIC for each. maxOrder &lt;- 5 #Max no. of previous steps to evaluate result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = cls, order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result We see that order 2 has the BIC closest to 0, hence we go for order 2. Now we can fit the model with order of 2. mc &lt;- fitMarkovChain(clickstreamList = cls, order = 2,control = list(optimizer = &quot;quadratic&quot;)) mc We see: Transition probabilities between different pages and outcomes both at lag 1 and lag 2 (that is two steps back) Start probabilities: there is the highest chance of starting on page 2. End probabilities: We see the probabilities of ending on different pages or in the absorbing stages, buying or deferring. Now we can start making predictions. We see that for a customer visits product 3 and 4 pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;P3&quot;, &quot;P4&quot;) #If a person visit P3 and then P4 , absorbingProbabilities = data.frame(Buy = 0.22 #mc@end[&quot;Buy&quot;] , Defer = 0.78 #mc@end[&quot;Defer&quot;] ) ) resultPattern &lt;- predict(mc, startPattern = pattern, dist = 1) #Pred one step ahead resultPattern We see that there is a 15% chance of visiting page 1, and 7% chance of buying. Now we do some clustering. Hence, we predict for clusters instead of depicted clickstreams. library(doParallel) CoreCount &lt;- makePSOCKcluster(detectCores()-1) registerDoParallel(CoreCount) clusters_sim &lt;- clusterClickstreams(clickstreamList = cls, order = 1, centers = 3) # takes 5-10 min. to converge stopCluster(CoreCount) registerDoSEQ() Now we could look at each cluster, I choose not to. It is basically what we have seen before with the transition probabilities, start and end probabilities. summary(clusters_sim$clusters[[1]]) summary(clusters_sim$clusters[[2]]) summary(clusters_sim$clusters[[3]]) Now we can loop through the the no. of steps to use for prediction. The following is merely for cluster 1. # mc for clu 1 maxOrder &lt;- 5 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters_sim$clusters[[1]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result We see that the best model is an order one, as BIC is closest to 0. mc_clu1 &lt;- fitMarkovChain(clickstreamList = clusters_sim$clusters[[1]], order = 1) summary(mc_clu1) We see that there are states and also includes absorbing states. Based on this model we can also make predictions pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;P1&quot;, &quot;P4&quot;, &quot;P6&quot;) ,absorbingProbabilities = data.frame(Buy = 0.22, Defer = 0.78)) resultPattern &lt;- predict(mc_clu1, startPattern = pattern, dist = 1) resultPattern Hence a person that cluster of clickstreams will have a 7.7% chance of buying something. 5.2.3 A concrete eaxmple with real data The following sections include four different options. Applying the Markov Chain model. Modeling click streams with higher-order. This is basically what we saw in the latent trait model, where we just needed to know the maturity to estimate how likely a person is to have other financial services. Use frequencies of events instead of sequences. Use neural networks, either with sequence data or not. 5.2.3.1 (1) Markov Chain Example with real data (A Danish company provided us with a file of clickstream data on their e-commerce customers). mydata &lt;- readClickstreams(file = &quot;Data/Customer behavior/StepsDesktop.csv&quot;, sep=&quot;,&quot;,header = T) summary(mydata) We see that there are 37k observations and then we can see the frequencies of the different events. Now we are going to analyze which order to apply. maxOrder &lt;- 3 #One could also use 5 for instance result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = mydata, order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result We see that order of 1, i.e., looking one step back in time is the most appropriate. mc &lt;- fitMarkovChain(clickstreamList = mydata, order = 1,control = list(optimizer = &quot;quadratic&quot;)) mc Since we have names on the rows and columns, it is a bit more confusion to read. Although the principle is the same as we have seen previously. Notice that we are not having any absorbing states. Lets predict what happens one step ahead given the different pages are visited. pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;DeliveryPage&quot;)) resultPattern &lt;- predict(mc, startPattern = pattern, dist = 1) resultPattern # pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;AddCartPage&quot;)) # resultPattern &lt;- predict(mc, startPattern = pattern, dist = 1) # resultPattern # # pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;AddCartPage&quot;, &quot;CartPage&quot;)) # resultPattern &lt;- predict(mc, startPattern = pattern, dist = 1) # resultPattern # # pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;DeliveryPage&quot;)) # resultPattern &lt;- predict(mc, startPattern = pattern, dist = 1) # resultPattern We see that when you have visited the delivery page, then there is a 20% probability that you will go to the channel direct access, whatever that is. Now we can cluster the different click streams and make predictions based on these. # Clustering first library(doParallel) CoreCount &lt;- makePSOCKcluster(detectCores()-1) registerDoParallel(CoreCount) clusters_ex &lt;- clusterClickstreams(clickstreamList = mydata, order = 1, centers = 3) # takes 5-10 min. to converge stopCluster(CoreCount) registerDoSEQ() summary(clusters_ex$clusters[[1]]) # clu 1 We see that the first cluster contain approx. 3.100 observations out of the 37.644 observations. Lets find optimal order based on AIC and BIC maxOrder &lt;- 2 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters_ex$clusters[[1]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result #Lag = 1 is the best. Now we can do the same for the other clusters. Notice that I have commented out the prints, in general we just see that a lag of 1 is the most approapriate. #summary(clusters_ex$clusters[[2]]) # clu 2 # mc for clu 2 maxOrder &lt;- 5 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters_ex$clusters[[2]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) #result #summary(clusters_ex$clusters[[3]]) # clu 3 # mc for clu 3 maxOrder &lt;- 5 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters_ex$clusters[[3]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) #result Result = lag of 1 is the most appropriate. mc_clu1 &lt;- fitMarkovChain(clickstreamList = clusters_ex$clusters[[1]], order = 1) summary(mc_clu1) mc_clu2 &lt;- fitMarkovChain(clickstreamList = clusters_ex$clusters[[2]], order = 1) mc_clu3 &lt;- fitMarkovChain(clickstreamList = clusters_ex$clusters[[3]], order = 1) Plot some graphical representations. Although we see that there are so many outcomes making it difficult to interpret. par(mfrow = c(1,1)) #SEE IF THIS WORKS TO COLLECT THE PLOTS. plot(mc_clu1, order = 1) #plot(mc_clu2, order = 1) #plot(mc_clu3, order = 1) The following is another way of representing the same. # model graphical representation (ii) par(mfrow=c(1,3)) par(mar=c(1, 1, 4, 0)) set.seed(11) plot(mc_clu1, order = 1, digits = 1, minProbability = 0.40, vertex.color=0, vertex.frame.color=0, vertex.shape=&quot;none&quot;, vertex.size=8, vertex.size2=3, vertex.label.dist=0.4, vertex.color=&quot;transparent&quot;, vertex.label.font=2, vertex.label.cex=0.95, vertex.label.degree=1, vertex.label.color=&quot;black&quot;, edge.arrow.size=0.2, edge.label.cex = 0.9, edge.curved=0, edge.label.font=4, margin=c(0,0,0,0.15), main =&quot;Cluster 1&quot;) set.seed(11) plot(mc_clu2, order = 1, digits = 1, minProbability = 0.40, vertex.color=0, vertex.frame.color=0, vertex.shape=&quot;none&quot;, vertex.size=8, vertex.size2=3, vertex.label.dist=0.4, vertex.color=&quot;transparent&quot;, vertex.label.font=2, vertex.label.cex=0.95, vertex.label.degree=1, vertex.label.color=&quot;black&quot;, edge.arrow.size=0.2, edge.label.cex = 0.9, edge.curved=0, edge.label.font=4, margin=c(0,0,0,0.15), main =&quot;Cluster 2&quot;) set.seed(11) plot(mc_clu3, order = 1, digits = 1, minProbability = 0.40, vertex.color=0, vertex.frame.color=0, vertex.shape=&quot;none&quot;, vertex.size=8, vertex.size2=3, vertex.label.dist=0.4, vertex.color=&quot;transparent&quot;, vertex.label.font=2, vertex.label.cex=0.95, vertex.label.degree=1, vertex.label.color=&quot;black&quot;, edge.arrow.size=0.2, edge.label.cex = 0.9, edge.curved=0, edge.label.font=4, margin=c(0,0,0,0.15), main =&quot;Cluster 3&quot;) Characterizing the clusters. Who are the customers underlying the three patterns of movement within the website? cl.2 and cl.3 search a lot through Category page; cl.1 search more through Product page; cl.1 and cl.3 search more varied (use more channels than cl.2) cl.3 are more likely to enter through DirectAccess -&gt; Front page pattern than the other two clusterss. In this application the product page was generalized. However, if a more fine-grained clicktream data were available by product brand it will allow the manager to take informed decisions about the preferences of each cluster. Now we can predict the next step for each cluster. Notice that the sequence for each prediction is the same. # predicting the next steps pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;AddCartPage&quot;, &quot;CartPage&quot;)) resultPattern &lt;- predict(mc_clu1, startPattern = pattern, dist = 1) resultPattern pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;AddCartPage&quot;, &quot;CartPage&quot;)) resultPattern &lt;- predict(mc_clu2, startPattern = pattern, dist = 1) resultPattern pattern &lt;- new(&quot;Pattern&quot;, sequence = c(&quot;AddCartPage&quot;, &quot;CartPage&quot;)) resultPattern &lt;- predict(mc_clu3, startPattern = pattern, dist = 1) resultPattern We see the following: Cluster 1 has 25% probability of going to conversion Cluster 2 has 32% probability of going to the product page Cluster 3 has 22% probability of going to the product page 5.2.3.2 (2) Using association rule mining Here we loose the sequence of events, but we use association rules. So like the maturity example we had with Hans, where if you have been at product 2 then you assume he has seen product 1. That might not always be true. Hence we don’t see how many times a customer has visited a product, as it is just looking at whether we visited a product or not. Summary if the method: we are going to apply arules to visualize association rules. Also we are going to calculate minsupport using the apropri algorithm. Notice that we are going to apply three clusters again. library(&quot;arules&quot;) library(&quot;arulesSequences&quot;) # looking by clustered data trans_clu1 &lt;- as.transactions(clusters$clusters[[1]]) sequences_clu1 &lt;- as(apriori(trans_clu1, parameter = list(support = 0.50)), &quot;data.frame&quot;) sequences_clu1 %&gt;% head() # subrules &lt;- subset(sequences_clu1, support&gt;0.05) We see the different rules in the rows above. The following runs the same for the other clusters. trans_clu2&lt;- as.transactions(clusters$clusters[[2]]) sequences_clu2 &lt;- as(apriori(trans_clu2, parameter = list(support = 0.50)), &quot;data.frame&quot;) #sequences_clu2 trans_clu3&lt;- as.transactions(clusters$clusters[[3]]) sequences_clu3 &lt;- as(apriori(trans_clu3, parameter = list(support = 0.50)), &quot;data.frame&quot;) #sequences_clu3 The corresponding output shows that pattern sequences are supported by at least 50% of the clickstreams in each cluster. cluster 1 of clickstream is the most heterogeneeous. The most common pattern {ChannelAd} =&gt; {ProductPage} has support in 41% of the clicks in clu1. This patterns reflects that this customers are most likely acquired through re-targeting. Visualizing library(&quot;arulesViz&quot;) sequences_clu1 &lt;- apriori(trans_clu1, parameter = list(support = 0.40)) ruleExplorer(sequences_clu1) sequences_clu2 &lt;- apriori(trans_clu2, parameter = list(support = 0.50)) ruleExplorer(sequences_clu2) sequences_clu3 &lt;- apriori(trans_clu3, parameter = list(support = 0.50)) ruleExplorer(sequences_clu3) # this approach is also useful for discovering product/brands that are seen together 5.2.3.3 (3) Use frequencies instead of the sequence If we dont want to work with sequences, we just look at frequencies. This has the advantage as many machine learning methods now can be applied, for instance a random forest. Although we do completely miss out on the sequence. frequencyDF &lt;- frequencies(mydata) frequencyDF Here we just get a table showing each IDs for visitors and then you just get to see how many times the given person has visited the page. 5.2.3.4 (4) Neural networks This is just using Neural Networks # (6) A fourth option is to use Neural Networks (recurrent or not) with sequences # of clickstream data purely for prediction purposes. 5.2.4 Optional exercise # (6) Optional # Consider another real-life data set from Cadez, I., Heckerman, D., Meek, C., # Smyth, P., White, S. (2003) Model-based clustering and visualization # of navigation patterns on a web site, Data Mining and Knowledge Discovery, 399-424. # The dataset msnbc323 (Melnykov 2016a) is available # in data(&quot;msnbc323&quot;, package = &quot;ClickClust&quot;). # There are 323 clickstream sequences that involve 17 different states: # (1) frontpage, (2) news, # (3) tech, (4) local, # (5) opinion, (6) on-air, # (7) miscellaneous, (8) weather, # (9) msn-news, (10) health-on-air, # (11) living, (12) business, # (13) msn-sports, (14) sports, # (15) summary-news, (16) bbs, # and (17) travel. # The length of sequences varies from 35 to 362. # There are 289 possible transitions among the 17 states. library(ClickClust) data(&quot;msnbc323&quot;, package = &quot;ClickClust&quot;) summary(msnbc323) clusters &lt;- clusterClickstreams(clickstreamList = msnbc323, order = 1, centers = 3) # takes 5-10 min. to converge # mc for clu 1 maxOrder &lt;- 5 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[1]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result #1 # mc for clu 2 maxOrder &lt;- 5 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[2]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result #1 # mc for clu 3 maxOrder &lt;- 5 result &lt;- data.frame() for (k in 1:maxOrder) { mc &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[3]], order = k) result &lt;- rbind(result, c(k, summary(mc)$aic, summary(mc)$bic)) } names(result) &lt;- c(&quot;Order&quot;, &quot;AIC&quot;, &quot;BIC&quot;) result #1 mc_clu1 &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[1]], order = 1) summary(mc_clu1) mc_clu2 &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[2]], order = 1) summary(mc_clu2) mc_clu3 &lt;- fitMarkovChain(clickstreamList = clusters$clusters[[3]], order = 1) summary(mc_clu3) # graphically par(mfrow=c(1,3)) par(mar=c(1, 1, 4, 0)) set.seed(11) plot(mc_clu1, order = 1, digits = 1, minProbability = 0.40, vertex.color=0, vertex.frame.color=0, vertex.shape=&quot;none&quot;, vertex.size=8, vertex.size2=3, vertex.label.dist=0.4, vertex.color=&quot;transparent&quot;, vertex.label.font=2, vertex.label.cex=0.95, vertex.label.degree=1, vertex.label.color=&quot;black&quot;, edge.arrow.size=0.2, edge.label.cex = 0.9, edge.curved=0, edge.label.font=4, margin=c(0,0,0,0.15), main =&quot;Cluster 1&quot;) set.seed(11) plot(mc_clu2, order = 1, digits = 1, minProbability = 0.40, vertex.color=0, vertex.frame.color=0, vertex.shape=&quot;none&quot;, vertex.size=8, vertex.size2=3, vertex.label.dist=0.4, vertex.color=&quot;transparent&quot;, vertex.label.font=2, vertex.label.cex=0.95, vertex.label.degree=1, vertex.label.color=&quot;black&quot;, edge.arrow.size=0.2, edge.label.cex = 0.9, edge.curved=0, edge.label.font=4, margin=c(0,0,0,0.15), main =&quot;Cluster 2&quot;) set.seed(11) plot(mc_clu3, order = 1, digits = 1, minProbability = 0.40, vertex.color=0, vertex.frame.color=0, vertex.shape=&quot;none&quot;, vertex.size=8, vertex.size2=3, vertex.label.dist=0.4, vertex.color=&quot;transparent&quot;, vertex.label.font=2, vertex.label.cex=0.95, vertex.label.degree=1, vertex.label.color=&quot;black&quot;, edge.arrow.size=0.2, edge.label.cex = 0.9, edge.curved=0, edge.label.font=4, margin=c(0,0,0,0.15), main =&quot;Cluster 3&quot;) # interpret: # Cluster 2 is entirely driven by transitions within the same categories # Thus, this group represents people who make the majority of transitions within their # preferred category and do not change categories frequently. # Cluster 1 The second cluster is characterized by higher probabilities of transitions # front page–news, news–news, msn-news, and summary–news. # The transition front page–news reﬂects the common pattern for the users starting with # the category front page to proceed directly to the category news. # Once the reader gets to the category news, he or she typically stays within it or proceeds # to summary.Thus, the second cluster consists of people mostly concerned with news. # Cluster 3 is characterized by transitions travel-health-on-air, travel-frontpage, # consisting of the people concerned about travelling issues. # The analysis of this dataset illustrates how click-plots can # be used and interpreted to discover interesting navigation patterns common for # observations within detected clusters. 5.3 AB Testing Basically this is experimenting with for instance a website. For instance changing the location of the basket. The ultimate goal is to be able to find out what works on a website, and what does not. The lecture had the following outline: What is A/B testing? When is the organization ready to use A/B test and why should you use A/B test? How to perform an A/B test? Pitfalls and challenges. A/B tests and bandits. 5.3.1 (1 + 2): What is it and when to apply it Basically you just funnel some visitors into one website layout and the other to another. Then one of the groups is a control group and the other, the treatment group. Then you want to assess if the treatment group is actually buying more stuff. To evaluate this, one can apply statistical tests. You want to apply it if you have operate in different countries, as the one culture may prefer one layout over another. Hence you want to collect that information. Hence instead of just making something that looks good, you want to make something that also works. 5.3.2 (3): How to perform an A/B test The is two steps: Agree on what you want to optimize, e.g., basket size, conversion rates, amount of visitors, the time a person is on the website etc. Then you must select an appropriate metric for this. This must be distributed throughout the organization. This is also known as the Overall Evaluation Criterion (OEC). Report many other metrics and diagnostics to support the OEC. Now you must find out: What sample size do you need? The more variance that that a metric contains, the more samples do you also need. How long will you be experimenting. Often it goes over one or two weeks Making statistical test of different means We want to find n avg_rev_pr_user &lt;- 3.75 #I.e. the control change_rev &lt;- 0.05 delta &lt;- change_rev * avg_rev_pr_user a &lt;- 0.05 beta &lt;- 0.2 power &lt;- 1-beta #Statistical power we want to achieve sigma &lt;- 30 z1 &lt;- abs(qnorm(a/2)) #Because it is double sided z2 &lt;- abs(qnorm(power)) 2*((z1 + z2)^2*sigma^2)/(delta^2) We see that we need 401.863 observations. An R example library(pwr) mde &lt;- 0.1 #minimum detectable effect cr_a &lt;- 0.25 #The expected conversion rate for group A (the control group) alpha &lt;- 0.05 #The false positive rate - significance level power &lt;- 0.8 #Statistical power ptpt &lt;- pwr.2p.test(h = ES.h(p1 = cr_a,p2 = (1+mde)*cr_a) ,sig.level = alpha ,power = power) n_obs &lt;- ceiling(ptpt$n) n_obs #Required pairs of observations (1 pair = 1 control + 1 treatment) We see that we need 4.860 observations, that is for both the control and the treatment. 5.3.3 (4): Pitfalls and challenges. Rampup He talks about slowly scaling up the treatment group to find bugs. Then you should start small and monitor the visitors that enter the website. Common problems Looking at too many metrics - the more measures you look at, the greater is the cahnce of observing random fluctuations. One must be aware of robots, we do not want to count these as a visitor. Many managers don’t let the tests run their course Not keeping it simple. Too many changes on the website will make the conclusions vague, because you don’t know what worked and what no. You must be aware of the sorroundings, like what dates are your comparing, different timezone? timeperiods? What browser? What device is used? To deal with the latter, one could make clusters, for instance we only want to make the test on phone or tablet visitors. 5.3.4 (5): A/B tests and bandits. We have to terms that must be clarified first: Exploration: That is where you are actually making the A/B testing, hence collecting all of your data. Running the test may not cost money, although if one landing page is better than the other, then you might be loosing revenue, hence it is costly in that way, or at least an opportunity cost. Exploitation: We see that what we find from the exploration we can start using and that is basically the exploitation. This is also called the bandit approach, as when we collect information we start using it to minimize the opportunity cost. This bandit approach has the following advnatages: Earn while you learn Automation of selection proces, he presented an example, where the algorithm could automatically select which add to show, based on the given information. Introducing randomness: we give the bandit the ability to explore new information, so it does not only end up confirming itself, as one ad would be used 99% of the time, as it previously has been the best. To run the exploitations, we can apply two different algorithms: Epsilon Greedy Policy Thompson Sampling Policy Simulations - example in R In the following we are going simulate the same example with two different types of simulators: Epsilon Greedy Policy Thompson Sampling Policy Epsilon Greedy An epsilon-greedy policy is a randomized policy that mixes exploitation with a predetermined amount of exploration. For any epsilon \\(\\epsilon\\) \\(\\in\\) (in) [0,1] (meaning epsilons between 0 and 1), the policy randomly allocates \\(\\epsilon\\) of the observations allocated uniformly across the K ads, and allocates 1 - \\(\\epsilon\\) of observations to the ad with the largest observed mean. library(contextual) horizon &lt;- 1000 #1000 timesteps simulations &lt;- 1000 #It means that simulator runs 1.000 simulations for each of the 1000 time steps # Epsilon greedy horizon &lt;- 1000 simulations &lt;- 1000 conversionProbabilities &lt;- c(0.05,0.10,0.15,0.20,0.25) bandit &lt;- BasicBernoulliBandit$new(weights = conversionProbabilities) policy &lt;- EpsilonGreedyPolicy$new(epsilon = 0.10) agent &lt;- Agent$new(policy = policy,bandit = bandit) historyEG &lt;- Simulator$new(agents = agent,horizon = horizon,simulations = simulations)$run() plot(historyEG,type = &quot;arms&quot;,legend_labels = c(&quot;Ad 1&quot;,&quot;Ad 2&quot;,&quot;Ad 3&quot;,&quot;Ad 4&quot;,&quot;Ad 5&quot;) ,legend_title = &quot;Epsilon Greedy&quot;,legend_position = &quot;topright&quot;,smooth = TRUE) We see that we feed the simulator information about the conversion probabilities for the given adds. Then we run 1000 simulations, and we see that there is a tendency towards selecting add 5, naturally because this has the highest conversion probabilities. If we for instance was to set add 5 conversion probability to 0.025 instead of 0.25, then we would see that add 4 would start dominate the add choices. summary(historyEG) we see that we had a reward of 21.81%, which is naturally lower than if we just used ad 5, which had a conversion of 25%. The variance is 71% and the standard deviation 2.66% Thompson Sampling Alogorithm Here we see the add to be presented is based upon probabilities. These are found by using beta distributions (recall the CLV lecture, where with alpha and beta we can basically make all kinds of beta distributions). The following makes a Thompson sampling #Thompson scaling conversionProbabilities &lt;- c(0.05,0.10,0.15,0.20,0.25) bandit &lt;- BasicBernoulliBandit$new(weights = conversionProbabilities) policy &lt;- ThompsonSamplingPolicy$new(alpha = 1,beta = 1) agent &lt;- Agent$new(policy,bandit) historyThompson &lt;- Simulator$new(agent,horizon,simulations)$run() plot(historyThompson,type = &quot;arms&quot;,legend_labels = c(&quot;Ad 1&quot;,&quot;Ad 2&quot;,&quot;Ad 3&quot;,&quot;Ad 4&quot;,&quot;Ad 5&quot;) ,legend_title = &quot;Thompson Sampling&quot;,legend_position = &quot;topright&quot;,smooth = TRUE) Here we practically see the same result. Although as he mentions, the Thompson sampling is better at exploiting the high probability adds, thus we see that it uses add 5 more often than the epsilon greedy function. summary(historyThompson) We see that the reward (conversion) is 21.99% hence slightly higher, while the variance is 32.83% variance and 1.81% standard deviation. Comparison between Greedy Search and Thompson Sampling We see that the Thompson sampling is having slightly better conversion rates, that is because it quicker picks up on ad 5 which is the most successful. Also we see that the variance and standard deviation is half the size of the Epsilon Greedy. "]]
